[{"objectID": "./examples/reproducible-workflow-airbnb.md", "title": "A Reproducible Research Workflow with AirBnB Data", "description": "A platform-independent, reproducible research workflow with AirBnB data, using Stata, Python and R.", "keywords": "tisem, airbnb, template, workflow, example", "code": ["  [js-link](code.js)   ```js // some js code var name = \"Arvind Singh\";  if (name == \"arvind\") { \tconsole.log(\"testing out coding blck\"); } ```  ```bash # some bash code # make-files.txt touch test/john.txt touch test/mike.txt touch test/jenna.txt ```  "], "headers": ["Overview", "How to run it", "Dependencies", "Run it", "Directory structure"], "content": "  Using publicly available data from AirBnB (available via [Kaggle.com](https://www.kaggle.com/airbnb/boston)), we illustrate how a reproducible workflow may look like in practice.  {{% cta-primary-center \"Check out the GitHub Repository\" \"https://github.com/tilburgsciencehub/airbnb-workflow\" %}}  We've crafted this project to run:  - platform-independent (Mac, Linux, Windows) - across a diverse set of software programs (Stata, Python, R) - producing an entire (mock) paper, including modules that \t- download data from Kaggle, \t- prepare data for analysis, \t- run a simple analysis, \t- produce a paper with output tables and figures.   - Install [Python](/get/python/).   - Anaconda is recommended. [Download Anaconda](https://www.anaconda.com/distribution/).   - check availability: type `anaconda --version` in the command line. - Install Kaggle package.   - [Kaggle API](https://github.com/Kaggle/kaggle-api) instruction for installation and setup. - Install [Automation tools](/get/make/).   - GNU make: already installed in Mac and Linux OS. [Download Make](http://gnuwin32.sourceforge.net/packages/make.htm) for Windows OS and install.   - Windows OS users only: make `Make` available via the command line.     - Right Click on `Computer`     - Go to `Property`, and click `Advanced System Settings `     - Choose `Environment Variables`, and choose `Path` under the system variables, click `edit`     - Add the bin of `Make`   - check availability: type `make --version` in the command line. - Install [Stata](/get/stata/).   - making Stata available via the command line. [Instruction](/get/stata/) for adding Stata to path.   - check availability: type `$STATA_BIN --version` in the command line. - Install [Perl](/get/perl/).   - Perl is already installed in Mac and Linux OS. [Download Perl](https://www.perl.org/get.html) for Windows OS.   - Make sure Perl available via the command line.   - check availability: type `perl -v` in the command line. - Install [LyX](/get/latex/).   - LyX is an open source document processor based on the LaTeX. [Download LyX](https://www.lyx.org/Download).   - make sure LyX available via the command line.   - check availability: type `$LYX_BIN` in the command line.   Open your command line tool:  - Check whether your present working directory is `airbnb-workflow` by typing `pwd` in terminal   - if not, type `cd yourpath/airbnb-workflow` to change your directory to `airbnb-workflow` - Type `make` in the command line.     Make sure `makefile` is put in the present working directory. The directory structure for the Airbnb project  is shown below.  ```text \u251c\u2500\u2500 data \u251c\u2500\u2500 gen \u2502\u00a0\u00a0 \u251c\u2500\u2500 analysis \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 input \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 output \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 figure \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 table \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 temp \u2502\u00a0\u00a0 \u251c\u2500\u2500 data_preparation \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 audit \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 figure \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 table \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 input \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 output \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 figure \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 table \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 temp \u2502\u00a0\u00a0 \u2514\u2500\u2500 paper \u2502\u00a0\u00a0     \u251c\u2500\u2500 input \u2502\u00a0\u00a0     \u251c\u2500\u2500 output \u2502\u00a0\u00a0     \u2514\u2500\u2500 temp \u2514\u2500\u2500 src     \u251c\u2500\u2500 analysis     \u251c\u2500\u2500 data_preparation     \u2514\u2500\u2500 paper ```  - **gen**: all generated files such as tables, figures, logs.   - Three parts: **data_preparation**, **analysis**, and **paper**.   - **audit**: put the resulting log/tables/figures of audit program. It has three sub-folders: **figure**, **log**, and **table**.   - **temp** : put the temporary files, such as some intermediate datasets. We may delete these filed in the end.   - **output**: put results, including the generated figures in sub-folder **figure**, log files in sub-folder **log**, and tables in sub-folder **table**.   - **input**: put all temporary input files - **data**: all raw data. - **src**: all source codes.   - Three parts: **data_preparation**, **analysis**, and **paper** (including TeX files).   <!--  --> "}, {"objectID": "./examples/reproducible-workflow-snakemake-r.md", "title": "A Reproducible Workflow Using Snakemake and R", "description": "A template for a reproducible research project that uses Snakemake and the R programming language.", "keywords": "snakemake, r, template, workflow, example", "code": [], "headers": ["Overview", "Motivating Example", "Get The Workflow"], "content": "  This is a template for a reproducible research project that uses `Snakemake` and the `R` programming language.  We use `Snakemake` to construct a set of *rules* to build our workflow from start to finish, starting with some data cleaning, running some regressions, constructung figures and tables, and then finishing with compiling a pdf article and slides.  We believe this mimics an approximate workflow of most empirical research in social sciences.   Our example project involves replicating the main tables and figures of Mankiw, Romer and Weil's classic 1992 QJE article \"[A Contribution to the Empirics of Economic Growth.](https://eml.berkeley.edu/~dromer/papers/MRW_QJE1992.pdf)\" We hope by using an example that is simple in its methods readers focus on how we have chosen to assemble both pure R codes and the Snakemake rules that build our project, rather than getting lost on econometric methodologies.   Check out the GitHub repository by [Lachlan Deer](http://lachlandeer.github.io) to get started.  {{% cta-primary-center \"Go to the GitHub Repository\" \"https://github.com/lachlandeer/snakemake-econ-r\" %}} "}, {"objectID": "./examples/google-covid-shiny-app.md", "title": "An Interactive Shiny App of Google's COVID-19 Data", "description": "Build your own interactive dashboard with an R Shiny app!", "keywords": "google, Shiny, R, covid, data visualization", "code": ["  ```R   library(shiny)   ui <- fluidPage()   server <- function(input, output){}   shinyApp(ui = ui, server = server)  ``` ", " ```R ui <- fluidPage(    sidebarLayout(        sidebarPanel(          h2(\"COVID-19 Mobility Data\"),          selectInput(inputId = \"dv\", label = \"Category\",                      choices = c(\"Retail_Recreation\", \"Grocery_Pharmarcy\", \"Parks\", \"Transit_Stations\", \"Workplaces\", \"Residential\"),                      selected = \"Grocery_Pharmarcy\"),          selectInput(inputId = \"provinces\", \"Province(s)\",                      choices = levels(mobility$Province),                      multiple = TRUE,                      selected = c(\"Utrecht\", \"Friesland\", \"Zeeland\")),          dateRangeInput(inputId = \"date\", label = \"Date range\",                         start = min(mobility$Date),                         end   = max(mobility$Date)),          downloadButton(outputId = \"download_data\", label = \"Download\"),          ),        mainPanel(          plotlyOutput(outputId = \"plot\"),          em(\"Postive and negative percentages indicate an increase and decrease from the baseline period (median value between January 3 and February 6, 2020) respectively.\"),          DT::dataTableOutput(outputId = \"table\")        )    ) ) ``` ", " ```R server <- function(input, output) {     filtered_data <- reactive({         subset(mobility,                Province %in% input$provinces &                Date >= input$date[1] & Date <= input$date[2])})      output$plot <- renderPlotly({         ggplotly({                 p <- ggplot(filtered_data(),                 aes_string(x = \"Date\", y = input$dv, color = \"Province\")) + geom_point(alpha = 0.5) + theme(legend.position = \"none\") + ylab(\"% change from baseline\")             p         })     })      output$table <- DT::renderDataTable({         filtered_data()     })      output$download_data <- downloadHandler(         filename = \"download_data.csv\",         content = function(file) {             data <- filtered_data()             write.csv(data, file, row.names = FALSE)         }     )  } ``` ", " ```R library(shiny) library(plotly) library(DT)  mobility <- read.csv(\"mobility_data.csv\", sep = ';') mobility$Date <- as.Date(mobility$Date) mobility$Province <- as.factor(mobility$Province)   ui <- fluidPage(     sidebarLayout(         sidebarPanel(             h2(\"COVID-19 Mobility Data\"),             selectInput(inputId = \"dv\", label = \"Category\",                         choices = c(\"Retail_Recreation\", \"Grocery_Pharmarcy\", \"Parks\", \"Transit_Stations\", \"Workplaces\", \"Residential\"),                         selected = \"Grocery_Pharmarcy\"),         selectInput(inputId = \"provinces\", \"Province(s)\",                         choices = levels(mobility$Province),                         multiple = TRUE,                         selected = c(\"Utrecht\", \"Friesland\", \"Zeeland\")),             dateRangeInput(inputId = \"date\", \"Date range\",                            start = min(mobility$Date),                            end   = max(mobility$Date)),             downloadButton(outputId = \"download_data\", label = \"Download\"),         ),         mainPanel(             plotlyOutput(outputId = \"plot\"), br(),             em(\"Postive and negative percentages indicate an increase and decrease from the baseline period (median value between January 3 and February 6, 2020) respectively.\"),             br(), br(), br(),             DT::dataTableOutput(outputId = \"table\")         )     ) )  server <- function(input, output) {     filtered_data <- reactive({         subset(mobility,                Province %in% input$provinces &                Date >= input$date[1] & Date <= input$date[2])})      output$plot <- renderPlotly({         ggplotly({                 p <- ggplot(filtered_data(), aes_string(x=\"Date\", y=input$dv, color=\"Province\")) +                 geom_point(alpha=0.5) + theme(legend.position = \"none\") +                     ylab(\"% change from baseline\")              p         })     })      output$table <- DT::renderDataTable({         filtered_data()     })      output$download_data <- downloadHandler(         filename = \"download_data.csv\",         content = function(file) {             data <- filtered_data()             write.csv(data, file, row.names = FALSE)         }     )  }  shinyApp(ui = ui, server = server) ``` "], "headers": ["Overview", "How to create it", "1. Skeleton", "2. User Interface", "3. Server", "4. Source Code"], "content": "  This example illustrates how to create a Shiny app which allows you to interactively explore Google\u2019s COVID-19 Community Mobility Reports of the Netherlands through an intuitive visual interface (see the live version [here](https://royklaassebos.shinyapps.io/dPrep_Demo_Google_Mobility/)).  ![Shiny app](../images/demo_app.png)    The Shiny library helps you turn your analyses into interactive web applications without requiring HTML, CSS, or Javascript knowledge, and provides a powerful web framework for building web applications using R. The skeleton of any Shiny app consists of a user interface (UI) and a server. The UI is where the visual elements are placed such as a scatter plot or dropdown menu. The server is where the logic of the app is implemented, for example, what happens once you click on the download button. And this exactly where Shiny shines: combining inputs with outputs. In the next two sections, we're going to define the inside contents of the `ui` and `server` parts of our app.      In our app, we have a left `sidebarPanel()` with a header, category, province, date range selector, and download button. Shiny apps support a variety of [control widgets](https://shiny.rstudio.com/tutorial/written-tutorial/lesson3/) such as dropdown menus, radio buttons, text fields, number selectors, and sliders. Each of these controls has an input id which is an identifier that the server part of our app recognizes. The label is the text you see above each control. Depending on the widget, you may need to specify additional parameters such as `choices` (list of selections in dropdown), `selected` (the default choice), `multiple` (whether only one or more selections are allowed), or the `start` and `end` values of the date picker.  On the right, there is a `mainPanel()` that shows the plot, figure description, and table. The `plotlyOutput` turns a static plot into an interactive one in which you can select data points, zoom in and out, view tooltips, and download a chart image. Similarly, the `DT::dataTableOutput()` makes the data table interactive so that you can sort by column, search for values, and show a selection of the data.      The server function requires the input and output parameters where input refers to the input ids of the `ui`, for example, `input$provinces` denotes the current selection of provinces. In the same way, `input$date[1]` and `input$date[2]` represent the selected start and end date in the date picker.  First, we create a reactive variable for the filtered data. Any time the user manipulates the province selection or date picker, this variable is re-evaluated. To access the data, you can call the variable name followed by parentheses: `filtered_data()`.  Second, we build up the plot with the [`ggplot`](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) library by defining the dataset, horizontal and vertical axes, and color categories. Next, we add a scatter plot with reduced transparency, remove the default legend, and change the vertical axis label. Note that `output$plot` refers the `outputId` of the `plotlyOutput()` function.  Third, we make sure that the current selection of data is shown in a table and that the download button  becomes functionable by writing `filtered_data()` to a csv-file.     ---  As the last step, we put everything together in a single code snippet that you can re-use for your own projects (just 65 lines of code!). A couple of additional changes we have made include: importing the required packages and the [mobility dataset](./../data/mobility_data.zip), converting the data type of the date end province columns, and adding some white space here and there.  To publish your app online, you can simply hit the \"Publish\" button in the R preview window and follow the steps in the wizard.   "}, {"objectID": "./examples/exploring-regression-results.md", "title": "Exploring Trends in the Cars Dataset with Regression Analysis", "description": "Build linear models to draw inferences from your datasets.", "keywords": "regression analysis, example, cars, lm, glm, statistical model", "code": [" ```R library(ggplot2) library(ggfortify) library(broom) library(dplyr)  data(cars) # import built-in car dataset cars$speed_kmh <- cars$speed * 1.60934  # convert miles per hour to kilometer per hour cars$dist_m <- cars$dist * 0.3048  # convert foot to meters  # estimate linear model mdl_cars <- lm(dist_m ~ speed_kmh, data=cars)  # check linear model assumptions autoplot(   mdl_cars,   which = 1:3,   nrow = 1,   ncol = 3 ) ``` ", "  ```R leverage_influence <- mdl_cars %>%     augment() %>%     select(speed_kmh, dist_m, leverage = .hat, cooks_dist = .cooksd) %>%     arrange(desc(cooks_dist)) %>%     head() ``` ", "  ```R library(stargazer)  stargazer(mdl_cars, mdl_cars_cleaned,           title = \"Figure 1: Car's stop distance increases as speed increases\",           dep.var.caption = \"Stop distance (m)\",             dep.var.labels = \"\",             covariate.labels = c(\"Speed (km/h)\"),             column.labels = c(\"Full model\", \"Outlier excluded\"),           notes.label = \"Significance levels\",             type=\"html\",           out=\"output.html\"             ) ``` ", "  ```R library(ggplot2)  ggplot(cars, aes(speed_kmh, dist_m)) +   geom_point(alpha = 0.5) +     geom_smooth(method = \"lm\", se = FALSE, aes(color=\"Full model\")) +   geom_smooth(method = \"lm\", se = FALSE, data = cars_cleaned,  aes(color=\"Outlier excluded\"))  +   labs(x = \"Speed (km/h)\", y = \"Stop distance (m)\") +     ggtitle(\"Figure 2: Linear trend between speed and stop distance\") +   scale_colour_manual(name=\"Legend\", values=c(\"red\", \"blue\")) ``` ", " ```R library(dplyr) explanatory_data <- data.frame(speed_kmh=c(45, 50, 60))  prediction_data <- explanatory_data %>%   mutate(     dist_m = predict(       mdl_cars, explanatory_data     )   ) ``` "], "headers": ["Overview", "How to create it", "1. Evaluate Model Assumptions", "2. Outlier Screening", "3. Model Reporting", "4. Visualize Linear Relationships", "5. Make Predictions for New Data"], "content": "  In this example, we analyze the relationship between a car's speed and the stop distance. First, we check the model assumptions and screen for potential outliers. Second, we export our regression results into a format that you can easily copy into word processing software. Third, we build a scatter plot with the trend line on top of it. Fourth and last, we make predictions for unseen data points.    We use the built-in `cars` dataset which includes 50 data points of a car's stop distance at a given speed. Since the dimensions are in miles per hour and foot, we first convert it into kilometer per hour and meter, respectively.  Then, we estimate a linear model and evaluate its properties with autoplot function from the `broom` package. Residuals indicate the difference between the predicted and the actual value. The left plot shows the residuals and the data points should center around the horizontal axis (i.e., the mean of the residuals is 0). Although the line goes a little downward around the middle, it does not look worrisome at first sight.  The second requirement is that the residuals are approximately normally distributed. This means that if you would plot the distribution of all residuals it would look like a bell-shaped distribution (also known as Gaussian distribution). This is the case if the data points in the QQ-Plot are close to the diagonal. In the second plot, we find that record 23, 35, and 49 are somewhat higher than expected (the right tail).   Finally, the right chart shows the standardized residuals for all fitted values. The homoskedasticity assumption states that the error term should be the same across all values of the independent variables. Hence, we should check here if there is any pattern that stands out. In our case, this happens to be the case as the blue line stays rather flat. It would, however, be troublesome if the error value increases for higher speed values.    *Output:*  ![model-evaluation](../images/model_evaluation.png)  ---  Two measures used to identify outliers are leverage and influence. High leverage means that the explanatory variable has values that are different from other points in the dataset. In the case of simple linear regression, this typically indicates values with a very high or very low explanatory value. Influence measures how much a model would change if each observation was left out of the model calculations, one at a time. That is, it measures how different the prediction line would look if you run a linear regression on all data points except that point, compared to running a linear regression on the whole dataset. The standard metric for influence is Cook's distance, which calculates the influence based on the size of the residual and the leverage of the point.  We use the `augment` function from the `broom` package to obtain model fit statistics such as the `hat` and `cooksd` columns for the leverage and Cook's distance respectively. Not surprisingly, the data point with the highest influence (record 49) is the one that showed up earlier. To illustrate the impact of this data point, we can exclude it from our analysis and estimate another linear model. As you can see below, our model coefficients are remarkably different - by leaving out just a single data point! In other words, be extremely careful with outliers and in doubt always report your regression estimates with and without them.      *Output:*  | `speed_kmh` | `dist_m` | `leverage` | `cooks_dist` | | :--- | :--- | :--- | :--- | | 38.6  | 36.6  | 0.0740  | 0.340   | | 22.5 | 24.4  | 0.0214 | 0.0856  | | 32.2  | 9.75 | 0.0354 | 0.0681  | | 37.0  | 16.5 | 0.0622 | 0.0532 | | 29.0 | 25.6  | 0.0249  | 0.0526  | | 38.6 | 28.3 | 0.0740 | 0.0479  |  ---   Although the model output from the `summary()` command suffices for your own analysis, it is not exactly in the format you typically find in a journal publication. Fortunately, the `stargazer` package can export multiple model coefficients and fit statistics into a well-formatted HTML file that can be copy-pasted into Word while still being editable.    *Output:*  ![stargazer](../images/stargazer.png)  ---  The most important relationships of a model are usually visualized to help the reader grasp the analysis. In this case, you may want to illustrate that the outlier severely impacts the regression slope, yet the direction of the relationship is consistent for both plots, and therefore the reader can be confident that there is indeed a strong positive relationship between car speed and the stop distance.  The `ggplot` library is based on the notion of visual layers. For example, we first create a scatter plot (`geom_point()`) after which we add a blue and a red trendline (`geom_smooth()`). Thereafter, we add the axis labels (`labs`), plot title (`ggtitle`), and a legend (`scale_colour_manual`).    *Output:*  ![stargazer](../images/trend_plots.png)  ---  Although we should be careful to extrapolate outside the ranges of our data, we may be interested in the stop distance of a car that drives 45, 50 and 60 kilometers per hour. Then, we can create a data frame with these input values and make predictions with our linear model. In essence, this is simply plugging in the input values into the regression equation (`-5.358 + 0.745 * input_value`).    *Output:*  | `speed_kmh` | `dist_m` | | :---- | :---- | | 45  | 28.15682  |   | 50  | 31.88070  | | 60  | 39.32847  | "}, {"objectID": "./examples/simple-reproducible-workflow.md", "title": "A Simple Reproducible Research Workflow", "description": "A simple Make pipeline with R and LaTeX.", "keywords": "simple, R, LaTeX, workflow, example", "code": [], "headers": ["Overview"], "content": "  In this example, you find a simple `make` pipeline, which prepares (`data-preparation`) and analyzes (`analysis`) data using R, and creates some tables for a paper (`paper`) using LaTeX.  This is probably the most simplistic but still complete workflow, and thus highly recommended as a template to kickstart your projects.  You can check out [this repository](https://github.com/rgreminger/example-make-workflow) for all the details. Fork it/use it as a template - contributions appreciated! "}, {"objectID": "./tutorials/more-tutorials/write-an-academic-paper/index.md", "title": "Writing an Academic Paper", "description": "Learn the typical structure of an empirical paper in economics and business.", "keywords": "latex, writing, paper, thesis, structure, academic, abstract", "code": [], "headers": ["Overview", "Structure of your paper", "Abstract", "Introduction", "Literature (review)", "Institutional Background", "Data", "Empirical Approach", "Results", "Robustness", "Summary and Conclusions", "References", "Appendix", "Online Appendix", "Reflections on Academic Writing", "Stick to the outline!", "Curate the paper!", "Writing Tips", "Polish your Writing", "Typesetting is important", "Using LyX?", "A few additional, small things", "Move your paper ahead!"], "content": "  The typical structure of an empirical paper in economics and business (or a master thesis, or a chapter of a Ph.D. thesis) involves the following sections, roughly in this order:   Be as brief as possible and avoid word-by-word duplication with the introduction. State very clearly what the main take-away is for your paper. This can be a qualitative finding, but it can also be a number.  There is no one structure that always works. But the following is often very well-suited as a starting point: Start by providing a general motivation for your paper introducing the reader to the topic, then state which question you answer with your paper, then explain why this question is important and interesting, followed by a short description of the approach you use to answer the question. Then provide an overview over the results. Only then you briefly summarize the relevant literature (not all the literature) and explain in detail how you contribute to it with your paper. There could also be multiple literatures your paper relates and/or contributes to. But don't overdo it with the literature review.  Usually, papers have no separate literature sections anymore. Sometimes it may still be the case, but it's more the exception than the rule.  Be brief. Focus on what is relevant for your study. Provide one or a few references. Don't go too much into the details unless necessary.  Briefly explain which data you use and what you do to construct your estimation sample. Put additional details into an appendix or the online appendix. Normal readers are usually not so interested in those. Provide meaningful summary statistics that are closely related to the analysis you will do. Spend time on thinking what is really relevant for the reader.  Write this section such that a graduate student with general training could run the analysis if you give him the data.  Think hard about how to present and discuss the results. Make a careful selection. For each table and figure, ask yourself what exactly it is meant to convey to the reader.  Try to address all the concerns people may have. Don't be too defensive, but stay honest.  Also describe the main take-away.      You should more or less stick to this outline. Readers will appreciate that they find it easy to find their way, because your paper reads like many others. At the same time, make the structure of the paper work for you, to convey the contents as well as possible. Add subsections or whatever suits the purpose of the reader.  Notice that usually, there is one question one answers per paper, not multiple ones. You should write a paper keeping this in mind. Don't be afraid that your paper will be too short. Usually, it will become long by itself, when you address all the concerns you learn about on the way and explain everything well, even if the analysis that you are doing is actually not too difficult.  Put only tables and figures that are important for what you want to say in your paper. The main body of the paper should not have more than 10 tables and figures, often less. Put extra information in an online appendix, but only if it's referred to and discussed somewhere. Don't use this as a place to dump extra things you did at some point.  A book that we found helpful in this context is called [\"How to Write a Lot: A Practical Guide to Productive Academic Writing\"](https://www.apa.org/pubs/books/4441010), by Paul Silvia. It also gives some more general advice.  As for the writing of the text, keep in mind that each paragraph makes one point or contains one argument or line of thought. Sentences in scientific papers in econ and business should be short and easy to understand and read---one should for instance use active tense as much as possible. Keep in mind that readers are often busy or tired. Help them as much as possible.  **Write. Revise. Revise. Revise. Revise. Revise. Revise. Revise. Revise. Revise. Revise...**  Don't be afraid to delete things. Leave old text behind. Replace it. Make sure your paper contains only what is necessary.  This will take a long time. One rule of thumb is that doing the analysis takes 50% of your time, writing your paper takes the other 50%. Therefore, start early with writing a very first rough draft. Then take it from there in steps. This will give you peace of mind.   When it comes to producing high-quality technical and scientific documentation, researchers and professionals often rely on **[{{< katex >}}\\LaTeX{{< /katex >}}](https://www.latex-project.org)**, an unrivalled document preparation system in many fields.  We prepared [a short guide](/building-blocks/configure-your-computer/statistics-and-computation/latex/) on how to install a $\\LaTeX$ distribution on your system.   Some technical tips when you are using LyX:  - The booktabs package is nice. - It's a good idea to put table and figure notes into minipage environments. - Use a normal size font and the resizebox package to make tables smaller. - Use Roman 12pt, 1.5 line space, and page margins 2.5cm on all margins as a starting point.   - Put only equation numbers when you refer to the equation. - Footnotes are usually placed after the end of a sentence. - Invest time into the formatting of tables and figures. Make sure that they are easily accessible even if readers do not read the entire paper, but only browse through it. Add table and figure notes that describe what is in the figure. - Make sure capitalization is consistent. For instance, you either want all figure titles to be capitalized (\"This is a Figure Title\"), or not (\"This is a figure title\"). Usually the first letter is a capital one. Within tables and figures the same thing holds. One easy way to go is to only have no capitalization inside of tables and figures (except for names etc. of course). - Put tables and figures close to where they are referred to. Don't put them in the very end of the paper. This is less convenient for the reader.   Finally, keep in mind from the very beginning that your paper will go through a messy reviewing process and that it may be rejected many times. Don't be discouraged. Learn from the comments, but don't sit on your paper for too long. Revise it and send it off again. A good motivating read that also provides some background information in this context is [\"Secrets of Economics Editors\"](https://mitpress.mit.edu/books/secrets-economics-editors). "}, {"objectID": "./tutorials/more-tutorials/write-your-first-latex-document/bibliography.md", "title": "Add a Bibliography", "description": "Learn to write your very first professional-looking document with LaTeX.", "keywords": "first, latex, beginner", "code": [" ```latex \\documentclass{article} % Specifies that we're writing an article \\usepackage{amsmath} % Imports amsmath \\title{My First \\LaTeX{} Article} % Defines the title \\author{John Doe} \\date{January 1st, 2000} \\begin{document} % Begins the document environment   \\maketitle % Prints the title   \\newpage % Ensures that the article starts from the next page   \\section{My First Section}   Hello, world! I'm finally writing my first document. I've learned that \\textbf{this is how you make a text bold}. And this is a citation \\cite{surname2020placeholder}.   % This is a comment, not shown in final output.   \\section{A Section for a Very Important Equation}   This is an equation:   \\begin{equation} % Begins an equation environment     E_0 = mc^2   \\end{equation} \\bibliography{bibliography} % Adds a bibliography \\bibliographystyle{apalike} \\end{document} ``` "], "headers": ["Create a .bib File", "Use BibTeX", "Wrap Up"], "content": " Please notice that there are different ways to include a bibliography in {{< katex >}}\\LaTeX{{< /katex >}}, namely `bibtex`, `natbib`, and `biblatex`. In this tutorial, we will deal with `bibtex` only. For more advanced uses, consider researching which system works best for you.   A `.bib` file contains all the bibliographic information for your document. Think of it as a sort of database of all the references you *may* want to include in your article (but you don't have to necessarily).  Your `.bib` file must contain specific information for each entry (i.e., for each article, book, proceeding, etc. that you want to cite.)  The following is an example for an article entry:  ```bib @article{surname2020placeholder, \tAuthor = {Surname, Name}, \tDate-Added = {2020-10-12 11:31:07 +0200}, \tDate-Modified = {2020-10-12 11:31:07 +0200}, \tJournal = {Placeholder Journal}, \tNumber = {1}, \tPages = {100-200}, \tPublisher = {PLACEHOLDER PUBLISHER}, \tTitle = {This is the title of a paper}, \tVolume = {10}, \tYear = {2020}} ```  We **strongly** suggest to use a *bibtex generator* for this task. Some reference management tools can build `.bib` files automatically for you. More on this at [the end of this tutorial](/tips/latex).  {{% cta-primary \"Download a Dummy .bib File for Prototyping\" \"../assets/bibliography.bib\" %}}   Once the `.bib` file is prepared, you will need to include it in the $\\LaTeX$ document.  1. Place this file in the same directory of your $\\LaTeX$ project. 2. Add the following to your code, before closing your `document` environment: ```latex ... \\bibliography{bibliography} \\bibliographystyle{apalike} \\end{document} ```  Notice that our file is saved as `bibliography.bib` but we did not include the extension `.bib` in the `\\bibliography{filename}` command.  You can also specify a bibliography style which will indicate how your references will look like in the bibliography section. `apalike` is just one of the many ones available. Here's [a comprehensive list](http://www.cs.stir.ac.uk/~kjt/software/latex/showbst.html).  3. Lastly, you just need to add citations in your article. You can do so by using the `\\cite{entrytag}` command, where the `entrytag` is a custom-defined tag for each entry that you can find right after the opening brace `{` - in our case, `surname2020placeholder`.  ```latex This is some text. Here a citation will be appended\\cite{surname2020placeholder}. ```  {{% tip %}} Notice that you're not obliged to cite all the entries in your `.bib` file! Only those that you've actually cited will appear in the bibliography section. {{% /tip %}}   Adding a bibliography to our previous article, we should get the following $\\LaTeX$ code and output:    ![Our final LaTeX example.](../img/article-example.png) "}, {"objectID": "./tutorials/more-tutorials/write-your-first-latex-document/your-first-article.md", "title": "Your First Article", "description": "Learn to write your very first professional-looking document with LaTeX.", "keywords": "first, latex, beginner", "code": [" ```latex \\documentclass{article} % Specifies that we're writing an article \\begin{document} Hello, world! \\end{document} ``` ", " ```latex \\documentclass{article} % Specifies that we're writing an article \\usepackage{amsmath} % Imports amsmath \\begin{document} Hello, world! This is an equation:   \\begin{equation}     E_0 = mc^2   \\end{equation} \\end{document} ``` ", " ```latex \\title{My First \\LaTeX{} Article} \\author{John Doe} \\date{January 1st, 2000} ``` ", " ```latex \\begin{document} \\maketitle \\newpage Hello, world! This is an equation:   \\begin{equation}     E_0 = mc^2   \\end{equation} \\end{document} ``` ", " ```latex \\documentclass{article} % Specifies that we're writing an article \\usepackage{amsmath} % Imports amsmath \\title{My First \\LaTeX{} Article} % Defines the title \\author{John Doe} \\date{January 1st, 2000} \\begin{document} % Begins the document environment   \\maketitle % Prints the title   \\newpage % Ensures that the article starts from the next page   \\section{My First Section}   Hello, world! I'm finally writing my first document. I've learned that \\textbf{this is how you make a text bold}.   % This is a comment, hidden in the final output   \\section{A Section for a Very Important Equation}   This is an equation:   \\begin{equation} % Begins an equation environment     E_0 = mc^2   \\end{equation} \\end{document} ``` "], "headers": ["The Preamble", "Document Class", "Packages", "The Body", "Environments", "The Title Page", "Basic Formatting Options", "Sections", "Putting All Together"], "content": "  The preamble is the first section of your `.tex` file. It comes before the the text of the document itself.  The preamble always **starts with the definition of a \"document class\"**. This is where you tell {{< katex >}}\\LaTeX{{< /katex >}} what kind of document you're writing: an article, a book, a letter... More on that later.  You can then continue **specifying additional information and packages** in the preamble.  The preamble is then followed by the document text, the actual \"content\" of your document.  The overall structure resembles this one:  ```latex \\documentclass{class} This is your preamble, where you can specify packages. \\begin{document} This is your document. This is the text that will be shown on the output file. \\end{document} ```  Notice how we specify the document class: beginning with a `\\` backslash. You will find similar statements over and over in your code. These tell $\\LaTeX$ that they're not actual text, but rather some instructions or commands.  All the commands follow the same structure: `\\nameofcommand{option}`. Commands can also appear within the document body, not only in the preamble. You can also include some additional parameters within square brackets.   Let's start your new document. First, we need to tell $\\LaTeX$ that we're going to write an article. Then, we write \"Hello, world!\" as our document text.    If you now compile the document, you should see \"Hello, world!\" together with a page number at the bottom, which $\\LaTeX$ automatically added for us since we used the *article* document class.  That's because $\\LaTeX$ **uses the document class to infer the page and document layout** that it should use.  {{% tip %}} If you're using [Atom to compile your code](/setup/latex), you can quickly \"build\" your PDF output with the shortcut: `ctrl + alt + B`. {{% /tip %}}  There are many document classes available out there. Here's [a comprehensive list](https://ctan.org/topic/class). In this tutorial, we will only use the `article` class.  In case you wanted to slightly modify an existing class, you may use any of the available parameters for that class. For instance, you can specify a different font size (the default is 10pt) or a different paper format (the default is `letterpaper`) for the `article` class: `\\documentclass[12pt,a4paper]{article}`.  As you can see, you can specify these **additional parameters within square brackets**.   You can extend the functionalities of $\\LaTeX$ by importing some packages in the preamble, similar to what you'd do with Python or R packages. However, contrary to those, most packages will be already installed by default - no need to download them manually[^1]. So, if you want to include a package, you only need to tell $\\LaTeX$ that you want to use it.  Import packages in the preamble with the command: `\\usepackage[options]{packagename}`.  For instance, if we wanted to add an equation to our document, we would need the `amsmath` package. This is a package from the American Mathematical Society designed to add features, facilitate writing formulas and improve the typographical quality of math equations.  So, our new code would look like this:    There are *many* packages for a great variety of purposes, like adding pictures, a bibliography, flowcharts, links, and so on. Most of the time, you will simply need to look up which one is the one you need for the specific purpose.  {{% tip %}} We provide a short list of the most important packages at the [end of this tutorial](/tips/latex). {{% /tip %}}    You may have noticed some commands with `\\begin` and `\\end` statements in our code above. Actually, these are not commands. Instead, they define the so-called environments.  An environment is a section of your document where particular typesetting rules apply. Usually, you will have multiple environments in the same document. For instance, in our previous example, we've added the `equation` environment within our `document` environment.  {{% tip %}} Try adding an `abstract` environment to your article on your own, if you feel you've mastered $\\LaTeX$ environments so far. {{% /tip %}}  {{% warning %}} You may specify as many environments as you wish, nested into each other or in any sequential order. However, it is imperative to **specify the `document` environment as the very first one**. In other words, you cannot have any environment outside the document environment. {{% /warning %}}   Let's now add a title page to our article. This page will show the article title, the author and a date.  Add these to your preamble:    Great! Now let's build again our document and... Oh, wait. Why isn't the title showing up?  That's because we've only specified the information in the preamble. But we did not tell $\\LaTeX$ where to show that information inside our document.  Therefore, we'll also need to add the `\\maketitle` command at the place you want the title to be printed, which is right after we begin the `document` environment in our case. If we wanted to make the title fill the entire first page, we may also add the command `\\newpage` so that the article content starts from the second page.    {{% tip %}} Try building your document with and without the `\\newpage` command to see how it effects the output. {{% /tip %}}   A few formatting options that you'll need to remember:  | $\\LaTeX$ | Output | |-----------------------|-----------------------| | `\\textbf{This text is bold}` | **This text is bold** | | `\\textit{This text is italicised}` | *This text is italicised* |  ---  These backslashes`\\\\`will break the line.  These backslashes  will break the line.  ---  ```latex \\begin{itemize}   \\item First bullet point.   \\item Second one. \\end{itemize} ```  - First bullet point. - Second one.   It's often important to give a logical structure to your document, especially when writing a paper. You can split up your article in different sections and subsections[^2]:  ```latex \\section{This is the section title} \\subsection{These titles will be printed in your document} \\subsubsection{These will also show up in your table of content} ```   Congrats! You've just learned the basics of $\\LaTeX$! Now, putting all together, your document code may look something like this:    {{% tip %}} You've just scratched the surface. For more formatting options, you can check out this [great guide](https://www.overleaf.com/learn). {{% /tip %}}  [^1]:     Especially if you're working on a $\\TeX$ Live distribution (on MacOS and Linux), where most packages are included in the \"required\" collection of packages.  [^2]:     For other document classes, please refer to their documentation. For instance, in the `book` class you cannot have sections. Instead, you have chapters. "}, {"objectID": "./tutorials/more-tutorials/write-your-first-latex-document/what-is-latex.md", "title": "The Fine Art of Typesetting", "description": "Learn to write your very first professional-looking document with LaTeX.", "keywords": "first, latex, beginner, typesetting", "code": [], "headers": ["The Design Philosophy", "Why $\\LaTeX$?", "Is $\\LaTeX$ Difficult to Learn?", "When to Use It"], "content": "  In this tutorial, you'll learn to write your very first document using {{< katex >}}\\LaTeX{{< /katex >}} within 15 minutes.  [$\\LaTeX$](https://www.latex-project.org) is a professional typesetting system widely used in academia because of its high-quality output and the relatively ease of use in setting up complex documents, like journal articles, books, technical reports, and similar.  One of its main features is that **it separates the writing and the formatting stages** - or the content and its presentation. With most word processors, the writer can see the result of their edits in real-time. With $\\LaTeX$, the writer can focus solely on writing the content in plain text, and then use some markup tags and commands to stylize the text and define the document structure.  The typesetting is asynchronous. The author can only see the results afterwards. This allows them to focus only on **what** the document should be, while it's up to the engine to deal with the **how**. Therefore, if the end result is unsatisfactory, the author cannot directly modify what's on-screen, but they have to correct the source code.  This is mainly what sets $\\LaTeX$ apart from WYSIWYG (an acronym for \"What You See Is What You Get\") editors, like Microsoft Word. Experienced users can **produce complex documents very quickly**, but at the expense of losing a Graphical User Interface.   Because:  ![Why LaTeX?](../img/why-latex.png)  Jokes aside, there are a few advantages in using $\\LaTeX$:  - It can yield the highest-quality and professional-looking documents - It deals with the page layout, so you don't have to - It can generate complex structures with thousands of cross-references, notes, citations that stay always up-to-date when you edit your document - It's free - It can produce gorgeous mathematical equations:  $$\\int_1^\\infty\\frac{1}{x^2}dx=\\left[-\\frac{1}{x}\\right]_1^\\infty=1$$   You'll probably find yourself Googling a lot even for the most elementary commands at the beginning. However, with a bit of practice on simple documents, you can quickly master its functioning and all your time and effort will be rewarded by high-quality results.  In the end, if you're a researcher dealing with complex econometrics models on a daily basis, if you can read a financial statement, if you can prove the transcendence of $\\pi$... we are pretty sure you can also learn to typeset documents professionally.  ![Is LaTeX difficult?](../img/latex-comparison.jpg)   To quickly sum up, it's useful to use $\\LaTeX$ in any of the following scenarios:  - Your document needs a clear and solid logical structure - Your document is very long - You need to manage a lot of cross-references and a long bibliography - Your document involves a lot of maths, tables, figures, graphic elements, chemical figures, flowcharts, diagrams, etc. - You aim for the highest-quality output but you don't want to spend time dealing with the page layout - A $\\LaTeX$ template is given to you, because that's the standard in your industry or domain  {{% tip %}} Because $\\LaTeX$ is such widely used among academics, there are many packages for researchers that can speed up their workflow.  For instance, `stargazer` is a R package that can automatically create gorgeous summary tables in $\\LaTeX$ from your R code (regression tables, summary statistics, etc...), saving you the trouble of manually filling in the table. {{% /tip %}} "}, {"objectID": "./tutorials/more-tutorials/write-your-first-latex-document/setup-latex.md", "title": "Setup LaTeX", "description": "Learn to setup your computer to produce LaTeX documents.", "keywords": "setup, latex, atom, editor", "code": [], "headers": ["Installation", "Setup"], "content": "  The first step is to setup a {{< katex >}}\\TeX{{< /katex >}} distribution on your machine.  1. If you haven't done it yet, you can read **[our guide](/get/latex)** on how to install $\\LaTeX$:  {{% cta-primary-center \"How to Install $\\LaTeX$\" \"/get/latex\" %}}  2. Once you've installed it, you'll need a text editor to work with. We recommend **[Atom](https://atom.io)**, a free and cross-platform editors with great functions and extra packages. You can download Atom from its official website or read the instructions from [our Building Block](/get/atom). Another great solution is [Visual Studio Code](https://code.visualstudio.com).  3. The last piece of the puzzle: let's setup Atom so that it can run $\\LaTeX$. By default, Atom can only work as a general text editor with your `.tex` files. Luckily, there are a number of (free) Atom packages that we can install to boost its functionalities and transform it into a proper $\\LaTeX$ editor:     - `latex`, a package that allows Atom to compile $\\LaTeX$ code: [download here](https://atom.io/packages/latex)     - `language-latex`, a package for syntax highlighting: [download here](https://atom.io/packages/language-latex)     - (Optional) `pdf-view`, a package for viewing PDF files side-by-side while you work: [download here](https://atom.io/packages/pdf-view)   Before starting to write anything, let's setup a directory for this project.  Make a new folder wherever you prefer on your local disk. Then launch Atom, create a new file, and save it with a `.tex` file extension in said folder.  You're finally ready to start writing your first $\\LaTeX$ document! "}, {"objectID": "./tutorials/more-tutorials/write-your-first-latex-document/latex-tips.md", "title": "Useful Tips", "description": "Learn to write your very first professional-looking document with LaTeX.", "keywords": "first, latex, beginner", "code": [], "headers": ["Specify The Encoding", "Use Labels", "Here's how it works", "Use a Reference Management Tool", "Useful Packages", "See Also"], "content": " Congratulations! You've completed this tutorial. Here are some useful tips when writing with {{< katex >}}\\LaTeX{{< /katex >}}.   It's always a good idea to specify the encoding of your document to allow characters beyond ASCII to be visible. It's recommended to use UTF-8. Unless you know what you're doing, simply add this in your preamble:  ```latex \\usepackage[utf8]{inputenc} ```   This will change your life. One of the greatest features and reasons why you should use $\\LaTeX$ for complex documents.  It's a good practice to **always add a label** to virtually anything, so that you can then reference to it in your paper.  Are you creating a new section? Label it. Are you adding a footnote? Label it. Are you adding a figure? Label it. A table? Label it. You get the sense of it.   1. Create a label with `\\label{marker}`. Place it next to the element you want to label. 2. Reference to it anywhere in your document with `\\ref{marker}` or `\\pageref{marker}` to print the page number where the element is positioned.  ```latex \\section{Dogs}\\label{sec:dogs} Some text. \\section{Cats} Some more text. A reference to a previous section \\ref{sec:dogs}. ```  If, for instance, at the 3rd revision of your paper you decided to swap the two sections, all your cross-reference would still work! The references to your sections, tables, figures, etc. will be automatically updated to follow the new ordering and the new page numbering. Pretty neat, isn't it?   Nobody writes their `.bib` files manually. It would be an insane amount of work.  You should use, instead, any kind of program that can generate these files for you. Most of the time, these are great tools that can also work as a \"vault\" for all your papers. You can use them to manage your library of references, categorize them, tag them, add notes, and so on. The following are the most famous ones that we recommend:  - [Zotero](https://www.zotero.org) - [Mendeley](https://www.mendeley.com) - The old and trusty [BibDesk](https://bibdesk.sourceforge.io)   Here's a list of the most used packages.  - `amsmath`, `amsfonts`, `amssymb` for awesome math typesetting - `tikz` to draw almost any vector graphic you'll ever need - `geometry` to resize elements, control margins, text areas, etc. - `hyperref` to have hyperlinks within the text - `lipsum` to generate random text, useful for prototyping   - A very [comprehensive wiki](https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes) about $\\LaTeX$ - A [good guide](https://www.learnlatex.org) to learn $\\LaTeX$ "}, {"objectID": "./tutorials/more-tutorials/contribute-to-tilburg-science-hub/mode-1.md", "title": "Mode 1: Just Shoot an Email", "description": "This is the easiest possible way to contribute to Tilburg Science Hub: send us your work!", "keywords": "contribute, method, email, easy, guide", "code": [], "headers": ["The Easiest Possible Way"], "content": "  Maybe you don't want to have to deal with GitHub - or you don't have the time for that. Or maybe you hate formatting Markdown files, or you don't know on what category on our website your new article should fit in.  No worries, you can still contribute to Tilburg Science Hub! It's as easy as sending an email.  You can send us your corrections or new material at **[tsh [at] tilburguniversity [dot] edu](mailto:tsh@tilburguniversity.edu)**. "}, {"objectID": "./tutorials/more-tutorials/contribute-to-tilburg-science-hub/code-of-conduct.md", "title": "Contributor's Code of Conduct", "description": "When contributing content to our platform, please abide by our code of conduct.", "keywords": "code of conduct, conduct, contribute, guideline", "code": [], "headers": ["Code of Conduct", "Disclaimer"], "content": "  As contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities.  We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion.  Examples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.  Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. Project maintainers who do not follow the Code of Conduct may be removed from the project team.  Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by opening an issue or contacting one or more of the project maintainers.  This Code of Conduct is adapted from [the Contributor Covenant](http://contributor-covenant.org/version/1/0/0/), version 1.0.0.   {{% warning %}}  When writing for Tilburg Science Hub, please follow our [communication style and writing guideline](../style-guide).  By contributing, you agree that we may redistribute your work under [our license](/about/#license). In exchange, we will address your issues and/or assess your change proposal as promptly as we can, and help you become a member of our community.  Everyone involved agrees to abide by our [code of conduct](#code-of-conduct).  {{% /warning %}} "}, {"objectID": "./tutorials/more-tutorials/contribute-to-tilburg-science-hub/contribute.md", "title": "Three Ways to Contribute", "description": "Learn how to contribute to Tilburg Science Hub in three easy ways.", "keywords": "contribute, contribution, tsh", "code": [], "headers": ["How to contribute to Tilburg Science Hub", "What is a contribution?", "How is the content organized?"], "content": "  We're glad for your interest in contributing to our project. Tilburg Science Hub is open-source, and as such, **anyone** can contribute! The process is really simple.  You are welcome to contribute to our platform in three different ways. Keep reading to learn which one is the best method for you or quickly jump to any of them:  1. [Just Shoot an Email](../mode-1) 2. [Use GitHub Issues](../mode-2) 3. [The Nerdy Way (Issue a Pull Request)](../mode-3)  {{% cta-primary \"Check out our GitHub repository\" \"https://github.com/tilburgsciencehub/website\" %}}   Anything from simple changes, grammar checks and reporting a bug to more elaborate changes and writing entirely new content!  We basically welcome **any** kind of feedback and contribution, including but not limited to:  - general fixes (typos, small sentences) - elaborate changes (modifying, adding or removing an entire paragraph or chapter) - asking for new content to be added - submitting an article without worrying about formatting (even a Word file!) - reporting a bug - requesting a new feature on the website - general tips or questions on the project    We provide content in three forms:  - **Building Blocks** are concepts in small doses, the DNA of our platform. They are small code snippets that users can independently \"mix\" to create something unique, like LEGO bricks. These allow us to explain the theory while also providing some practical examples and code snippets for a variety of programming languages or operating systems. Information is explained in a way that it is easy to clone or implement in an existing project. While everybody can follow our Building Blocks, they are generally more appealing to advanced users \u2013 or those who already know what to look for.  - **Tutorials** explain a broader argument compared to Building Blocks, and follow a sequential order. They teach our users how to combine individual building blocks into a \"final product\". These are particularly useful for novices or anyone new to a certain topic because of their comprehensive nature and step-by-step guidance. We support and encourage the use of videos, exercises, and quizzes in a tutorial.  - **Examples** are real-life cases, publications, templates, or research projects that put into practice the concepts explained on this website. "}, {"objectID": "./tutorials/more-tutorials/contribute-to-tilburg-science-hub/mode-2.md", "title": "Mode 2: Use GitHub Issues", "description": "Learn how to use GitHub issues to requests new content, features or report a bug.", "keywords": "contribute, requests, issues, github, guide", "code": [], "headers": ["There's a Better Way", "How to create an issue on GitHub"], "content": "  Like in [the first method](../mode-1), you can reach out to us with any request by opening an issue on [our GitHub repository](https://github.com/tilburgsciencehub/website). We prefer this method because it allow us to keep track of bugs and requests more easily, to merge duplicates, assign roles, etc...   It's very easy!  1. First, navigate to **[our GitHub repository](https://github.com/tilburgsciencehub/website)**.  2. Click \"**Issues**\" under the repository name.  ![Click on Issues under the repository name.](../git-issues-1.png)  3. Click \"**New issue**\".  ![Click New issue](../git-issues-2.png)  4. Click \"**Get started**\" next to the type of issue that you'd like to open.  ![Click Get started next to the type of issue that you'd like to open.](../git-issues-3.png)  5. Type a title and fill in the description for your request.  ![Type a title and fill in the description.;](../git-issues-4.png)  6. When you're done, click \"**Submit new issue**\".  You can learn more about GitHub issues [here](https://docs.github.com/en/github/managing-your-work-on-github/creating-an-issue). "}, {"objectID": "./tutorials/more-tutorials/contribute-to-tilburg-science-hub/style-guide.md", "title": "Style and Writing Guide", "description": "When contributing content to our platform, please follow our style and writing guidelines.", "keywords": "style, styling, guideline, contribute, writing", "code": [" ```python # some Python code here print(\"Hello, world!\") ```  ```R # some R code here cat('Hello, world!') ``` "], "headers": ["Style Guidelines", "Adopt Our Style of Communication", "Language", "Start Your Content With Our Templates", "Contribute via Git Pull Requests", "Style Your Content", "Code Highlighting", "LaTeX Integration & Math Formulas", "Highlighting Boxes", "Writing Instructions", "Develop content for our target audience", "Use cases for experienced users and novices:"], "content": "   We are glossy and nerdy. We are welcoming, competent and helpful. We seek to cheerfully inform our users. We communicate concisely.  Will you blend in?   Please develop your content in English.   Please use [our templates](../mode-3/#contribution-templates) when developing new building blocks or tutorials.   Please fork our site, and develop your content in a new branch. When you're done, make a pull request, explain briefly what you've done (and why), and we're going to review your code and add it to the site. -->   Your content is written in [Markdown](https://guides.github.com/features/mastering-markdown/), and is automatically rendered in our house style.  You can also make use of code highlighting and admonitions.   In addition to the standard way of formatting code in Markdown, code snippets can be displayed in special boxes that highlight the code based on the programming language.  Provide your code in all the relevant languages and/or operating systems and specify them after the three back ticks. Wrap all of your snippets (in different languages) inside a codeblock shortcode (see our templates for more info on this).     You can include mathematical notation via our KaTeX integration. You can learn more on how to use it [here](https://themes.gohugo.io/theme/hugo-book/docs/shortcodes/katex/).  If you use KaTeX more than once within the same Markdown file, there's no need to write the shortcode again. Simply place your math formulas: - within single dollar signs for inline math: `$f(x)=x^2$` yields: {{< katex >}}f(x)=x^2{{< /katex >}} - within double dollar signs for display: `$$f(x)=x^2$$` yields: $$f(x)=x^2$$   We support four kind of highlighting boxes.  {{% tip %}}  This is a tip  {{% /tip %}}   {{% warning %}}  This is a warning  {{% /warning %}}  {{% summary %}}  This is a summary  {{% /summary %}}  {{% example %}}  This is an example  {{% /example %}}   - Use the second person (\"you\") when speaking to our or talking about our users - Content creators may refer to themselves in the first person (\"I\" in single-author articles, or \"we\" in multiple-author articles). Do remember to keep the focus on our users. - Avoid personal references: e.g., in my career, on a project I run. - We encourage the use of contractions: e.g., it\u2019s, you\u2019ll, you\u2019re, we\u2019re, let\u2019s. - We prefer shorter words over longer alternatives.   - e.g., \"helps\" is better than \"facilitates\"   - e.g., \"uses\" is better than \"utilizes\" - Use an active voice wherever possible. - Write conversationally: prepositions are okay to end sentences with. And another thing. You can even start sentences with \"And\" or \"But.\" - Be informal, but being clear is more important than being entertaining! - Get to the point fast. - Frame sentences positively: use positive language (e.g., be more efficient, write better code...), rather than negative language (e.g., avoid mistakes...). - Avoid slang. - Avoid jargon. We know it's difficult for you academics out there. - Capitalize Your Headings Like This. But don't do that for articles and prepositions. - Avoid headings to start with articles and \"How to...\". Of course we're telling you how to do it. - Use `##`, `###`, `####` for headers, not a standard line on **bold**. Use `**` only for bolded words or short sentences when you want to stress a concept. Learn more about markdown syntax [here](https://www.markdownguide.org/basic-syntax/). - Do **NOT** use `#` for headers. Instead, use the appropriate `title` flag for your main header. - Suggest at least one short link for your article! Combine an action verb and a noun like this: /VERB/NOUN (/do/this, /learn/that, /get/me). Ex. https://tilburgsciencehub.com/get/python. You will find these instructions in the contribution template. - Be consistent and double-check your file before sending!  {{% tip %}} If you want to be credited for your work, don't forget to fill in the `author` and `authorlink` fields at the top of our templates with your name and a link to your personal webpage, respectively. {{% /tip %}}  {{% warning %}} **The naming of your file matters!**  We use the filename, which is different from the actual article's title, to extract information and display the current path within the navigation menu and on the breadcrumb trails. Please, choose your filename carefully!  Generally speaking, it's a good idea to name your file exactly like your main title, but using all lowercase letters and separating words with a `-`.  **Example**:      Title: Automation with GNU Make     Filename: automation-with-gnu-make.md {{% /warning %}}   When contributing content to our platform, please address at least one of our target groups.  1. __Students and novice researchers__ who look for material developed, curated, and rigorously tested by professors and experienced users, ensuring them about quality and usability. Users may not know yet what (and why) they should learn the Tilburg Science Hub way of working. These users need to be onboarded first and guided through our content.  2. __(Advanced) researchers and professors__ who look for useful material developed by their peers. These users have identified a problem, but need code snippets to use in their projects. Researchers seek to avoid mistakes that others have made before them, and look to get inspired by their colleagues. They may also use our platform to share their solutions to problems (e.g., in the form of building blocks or tutorials).  3. __Teams and small businesses__ who wish to get inspired by how scientists work on on data-intensive projects. Tilburg Science Hub offers tools and knowledge to help teams work together better, more efficiently. Our content can be adopted by individual team members (i.e., adoption doesn't disturb the way of working of other team members), or jointly by a team (i.e., the entire team commits to the Tilburg Science Hub way of working). <!--Businesses can also request a paid custom consultancy to implement reproducible workflows in their own processes.-->  4. __Data science enthusiasts__ who encounter our site when working on their projects. We strive to become a key learning resource in the data science community.  {{% warning %}}  **Keep in mind our key goals**  Our platform focuses on __usability__ (i.e., ability to quickly copy-paste code or download templates), __speed__ (i.e., loading time), and __attractiveness__ (i.e., visual appeal, \u201cstate-of-the-art\u201d look, good writing).  Our platform is a (classical) two-sided market.  1. We attract __novice and expert users__ that *use* our content in their work, and  2. We build a __community of content creators__ that have an outspoken vision on how to conduct science efficiently. {{% /warning %}}  {{% warning %}} **License**  Text and content are licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/). In other words, our content can be reused by anybody for non-commercial purposes, but needs to be properly cited.  [![Creative Commons Licence](https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-nc-sa/4.0)  {{% /warning %}}  <!-- TSH communicates content expertise and content depth that is appealing to advanced users. At the same time, it appeals to novices who use TSH to (i) understand what is potentially wrong in their current workflow; (ii) understand the advantages of our proposed one; (iii) understand what is needed to get started and what they need to learn; (iv) learn the concepts and put them into practice with templates, examples, and exercises.  Students and professors: TSH seeks to build a community of professors and scholars who use reproducible science in their daily work and want to contribute (with their own code). These professors build content that is useful for their students. Also, they share content that they can use to kick-start their own projects without \"reinventing the wheel\".  New users and returning ones: Ideally, TSH is ranked high on search engines to attract new users. It also becomes part of the curriculum at various schools. For returning users, it becomes the essential bookmark tab they often resort to, knowing they will find the information they need to run their projects. --> "}, {"objectID": "./tutorials/more-tutorials/contribute-to-tilburg-science-hub/mode-3.md", "title": "Mode 3: The Nerdy Way", "description": "Tutorials teach our users how to combine individual building blocks into a final product. Learn how to write one.", "keywords": "template, pull, requests, contribute, github", "code": [], "headers": ["The Ideal Way: Issue a Pull Request", "If you want to revise, modify, add, or remove content from existing pages, or report a bug", "Simple changes", "Elaborate changes", "If you want to write new content", "Place new sections in a logical order.", "Contribution Templates"], "content": "  On this page, we provide all the necessary resources and information for you to be able to write content autonomously.  {{% tip %}}  Please, notice that Tilburg Science Hub Building Blocks, Tutorials and Examples are written as [Markdown](https://guides.github.com/features/mastering-markdown/) files. We encourage you to familiarize with the Markdown formatting before starting to write - it's very easy!  {{% /tip %}}    The easiest method to make straightforward updates to Markdown files is to use [GitHub's web-based file editor](https://help.github.com/en/articles/editing-files-in-your-repository).  1. Click on the \"**Edit this page**\" button at the bottom of the page you want to edit on our website.  ![Edit this page button](../tsh-edit-this-page-button.png)  2. You will be redirected to our GitHub repository where that file is hosted. You will need to **fork** our repository in order to propose changes.  ![Click on the pencil icon to open the text editor.](../github-fork.png)  3. Edit the file and then **submit a new pull request**. That's it.   For more complex updates or editing more than a file, it's better to use a local Git workflow to create a pull request.  After having [set up your GitHub account](/building-blocks/configure-your-computer/statistics-and-computation/git/), follow the following steps (required only the first time you set up a local project):  1. Fork the originating repository to your GitHub profile (press the \"fork\" button on GitHub, which creates a copy of this project in your own GitHub account).  2. Within your version of the forked repository, move to the `tilburg-update` branch and create a **new branch for each significant change being made**.  3. Navigate to the file(s) you wish to change within the new branches and make revisions as required.  4. Commit all changed files within the appropriate branches.  5. Create individual pull requests from each of your changed branches to the `tilburg-update` branch within the originating repository.  6. Maintainers and other contributors will review your pull request. When your pull request is approved, it will be merged into the upstream Tilburg Science Hub repo. If you receive feedback, make changes using your issue-specific branches of the forked repository and the pull requests will update automatically.  7. Repeat as needed until all feedback has been addressed.    1. First, you'll need to prepare the content as Markdown file(s). Follow our templates [below](#contribution-templates) to get started. In case you want to feature your project in the Examples section, please [contact us](/about/#who-maintains-tsh).  2. Fork the originating repository to your GitHub profile (press the \"fork\" button on GitHub, which creates a copy of this project in your own GitHub account).  3. Within your version of the forked repository, move to the `tilburg-update` branch and create a **new branch for each new topic you are writing about**.  4. Commit all the new Markdown files within the appropriate branches.  5. Create individual pull requests from each of your changed branches to the `tilburg-update` branch within the originating repository.  6. Maintainers and other contributors will review your pull request. When your pull request is approved, it will be merged into the upstream Tilburg Science Hub repo. If you receive feedback, make changes using your issue-specific branches of the forked repository and the pull requests will update automatically.  7. Repeat as needed until all feedback has been addressed.  {{% tip %}}  Don't know how to do this? You can follow a great tutorial about [contributing on GitHub](https://github.com/firstcontributions/first-contributions).  {{% /tip %}}   If you have written a new section which you wish to add to the website, make sure they are where they suppose to be. {{% example %}}   You wish to add a Building Block on \"Advanced Git Commands\". Now, imagine that once your page is completed, you notice that your new section appears in the first position:    ![](../advanced-git-misplaced.PNG)    Does it make sense to have the \"Advanced Git Commands\" section before the \"Get Started with Git and Github\"? It does not.    - Ideally you would place it for instance, just after \"The Most Important Git Commands You Should Know\". This way, when going through the tutorials, the new content would come in a more natural order. {{% /example%}}     **How to choose the order of sections?**  Well, using our Markdown Templates (which can be found below) it is quite a simple task. You only need to change the value of the parameter **\"weight\"** in the markdown file:    ![](../weights.PNG)  {{% tip %}} A weight set equal to 1 will place your section in the first place, likewise the prior example. If you wish to make it appear after other sections, make sure to give it a higher weight. {{% /tip %}}   {{% warning %}}  First, make sure to read our [code of conduct](../code-of-conduct) as well as our [writing guidelines](../style-guide).  {{% /warning %}}  {{% cta-primary \"Start with our Building Block Markdown template\" \"https://raw.githubusercontent.com/tilburgsciencehub/tsh-website/master/content/tutorials/more-tutorials/contribute-to-tilburg-science-hub/building-block-shell.md\" %}}  {{% cta-primary \"Start with our Tutorial Markdown template\" \"https://raw.githubusercontent.com/tilburgsciencehub/tsh-website/master/content/tutorials/more-tutorials/contribute-to-tilburg-science-hub/tutorial-shell.md\" %}}   <!-- The design should always accommodate all users' knowledge levels and avoid confusion. For instance, on a tutorial page, there should be a quick and concise explanation (a sort of TL;DR), as well as a more in-depth exposition for those who need to educate themselves first.  The design should be attractive and easy to use for all our target groups and should strike a balance between glossiness and nerdiness. We want to avoid unnecessary clutter and stock photos. Let's keep it simple. --> "}, {"objectID": "./tutorials/more-tutorials/running-computations-remotely/connect-instance.md", "title": "Connect Instance", "description": "Learn how to run computations remotely", "keywords": "cloud computing, azure, amazon, google, virtual machine", "code": [], "headers": ["Linux instance", "Windows instance"], "content": " # Connect Instance  Depending on your operating system and whether you chose to launch a Linux or Windows instance, follow one of the tutorials below.   **Mac** 1. Click on your running EC2-instance in the console and copy the Public IPv4 DNS address (e.g., `ec2-XX-XXX-XXX-XXX.compute-1.amazonaws.com`).  2. Open the terminal and type `export HOST=<YOUR-PUBLIC-IPV4-DNS>` (see step 1) and `export KEY=<PATH_TO_KEY_PAIR_FILE>` (e.g., `/Users/Roy/Downloads/ec2-key-pair.pem`). This creates two variables, `HOST` and `KEY`, that we can reference in our terminal (e.g., if you type `$KEY` it returns the aforementioned file path).  3. Type `chmod 700 $KEY` to change the permissions of the private key file.  4. Type `SSH -i $KEY ec2-user@$HOST` to connect with the EC2-instance.  5. Type `yes` if it asks you to continue connecting and hit Enter. If done successfully, it prints the text \"EC2\" to the terminal followed by the AMI that you selected (e.g., Amazon Linux 2 AMI). So, in short, we connected to an EC2-instance that we can control through our terminal, however, there are no scripts or data on it yet so we first need to copy files to our instance before we can put it to work.   **Windows**  1. Download [PuTTY](https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html) MSI Windows Installer and follow the installation wizard. If it doesn't come with the PuTTY Key Generator pre-installed, also make sure to download `puttygen.exe` from the same page.   2. Open PuTTy Key Generator, click on \"Load\", and select the private key file you downloaded earlier.  3. Click on \"Save private key\" to convert the `*.pem` file into a `*.ppk` file.  4. Click on your running EC2-instance in the console and copy the Public IPv4 DNS address (e.g., `ec2-XX-XXX-XXX-XXX.compute-1.amazonaws.com`).  5. Open PuTTY (so not the Key Generator!) and enter the following as the host name: `ec2-user@<YOUR-PUBLIC-IPV4-DNS>`  6. In the left panel, click on Connection > SSH > Auth and click on \"Browse\" to add your `*.ppk` private key.   ![Connect Windows Instance](../img/connect-windows-instance.gif)  1. Install Windows Remote Desktop ([Windows](https://www.microsoft.com/en-us/p/microsoft-remote-desktop/9wzdncrfj3ps?activetab=pivot:regionofsystemrequirementstab) & [Mac](https://apps.apple.com/us/app/microsoft-remote-desktop/id1295203466?mt=12).   2. Head over to your Windows Instance in the AWS console and click on \"Connect\" and then \"RDP Client\".  3. Click on the \"Download remote desktop file\" button to download a rdp file.  4. Before you open the file, click on \"Get password\", upload the keypair (`*.pem` file), click on \"Decrypt password\" and copy the password.  5. Now, open the rdp file and login with the credentials (Username = \"Administrator\", Password = <see step 4>). "}, {"objectID": "./tutorials/more-tutorials/running-computations-remotely/launch-instance.md", "title": "Launch Instance", "description": "Learn how to run computations remotely", "keywords": "cloud computing, azure, amazon, google, virtual machine", "code": [], "headers": [], "content": " # Launch Instance  Although you can also launch instances via the command line, here we show you how to get started with the [AWS console](https://console.aws.amazon.com/console) in your browser.  ![Launch Instance](../img/launch-instance.gif)  1. After you have signed in, type \u201cEC2\u201d in the search bar.  2. Click on the orange button that says \"Launch instance\".  3. It asks you to choose an Amazon Machine Image (AMI) which is the operating system that will be launched on the system. If you're just getting started, we recommend to pick one that has a free tier eligible (e.g., Amazon Linux 2 AMI) and click on \"Select\".  4. Now it asks you to choose an instance type that fits your needs (i.e., number of CPUs, RAM Memory, network performance, etc.). In other words, how powerful do you want your machine to be? This depends on the computations that you plan on running on the VM. For example, the baseline `t2.micro` instance type (1 CPU and 1 GB RAM) is eligible for a free tier and is therefore recommended to get started. Other instance types can cost less than a penny per hour for the most basic offering up to 32 dollars per hour for a machine with 1152GB of RAM and 96 CPUs!  5. After you have selected an instance type, click on \"Review and Launch\" and then on \"Launch\".  6. Now you need to create a new key pair which is basically like a password to authenticate yourself and connect with the instance. Give the key pair a name (can be anything) and click on \"Download Key Pair\". Store this private key file (`*.pem`) in a secure and accessible location because you'll not be able to download the file again after it's created.  7. Finally, click on \"Launch Instances\". If you now return to the instances dashboard page, you'll see that Amazon is spinning up your configured instance. After a minute or so, the instance state should say \"Running\" which means we are ready to connect to it! "}, {"objectID": "./tutorials/more-tutorials/running-computations-remotely/run-scripts.md", "title": "Run Scripts", "description": "Learn how to run computations remotely", "keywords": "cloud computing, azure, amazon, google, virtual machine", "code": [" ```bash # install Python sudo yum install python3  # install a single package sudo python3 -m pip install <PACKAGE_NAME>  # install multiple packages from txt file sudo python3 -m pip install -r requirements.txt ``` "], "headers": [], "content": " # Run Scripts  If you opted for the default Amazon Linux 2 AMI, you probably need to install some additional software to be able to execute your code files. To install Python 3 and any other packages on your EC2 instance, run the following commands:    Next, you can run your scripts from the command line like you're used to with `python3 <SCRIPT_NAME>.py`.  {{% tip %}} One of the key advantages of using a VM is that you can leave it running indefinitely (even if you shut down your laptop!). Take a look at the [task scheduling](https://tilburgsciencehub.com/building-blocks/automate-and-execute-your-work/automate-your-workflow/task-scheduling/) building block to learn how to use crontab to schedule recurring tasks (e.g., run a Python script that scrapes a website every day at 3 AM). Keep in mind that the time at your VM may differ from your local time because the data center that hosts the VM is located in another time zone (tip: run `date` to find out the UTC time). {{% /tip %}} "}, {"objectID": "./tutorials/more-tutorials/running-computations-remotely/terminate-instances.md", "title": "Terminate Instance", "description": "Learn how to run computations remotely", "keywords": "cloud computing, azure, amazon, google, virtual machine", "code": [], "headers": [], "content": "# Terminate Instance  Even though the very basic VMs can run for free forever, it is good practice to shut down inactive VMs. After all, you don't want to find out at the end of the month - once you receive your bill from AWS - that you accidentally left an instance running in the background. To terminate a VM follow the steps below:  ![Terminate instance](../img/terminate-instance.gif)   1. Visit your \"Instances\" dashboard in the console 2. Tick the checkboxes of those instances you want to stop. 3. Click on the \"Instant state\" button. Here you can either stop or terminate the instance. Stop means that you can re-activate the instance at a later point in time, whereas terminate means that you remove it permanently (including all files stored on it). "}, {"objectID": "./tutorials/more-tutorials/running-computations-remotely/move-files.md", "title": "Move Files", "description": "Learn how to run computations remotely", "keywords": "cloud computing, azure, amazon, google, virtual machine", "code": [], "headers": ["Terminal (Mac)", "FileZilla (Mac & Windows)"], "content": " # Move Files   1. `cd` into the directory that contains the files that you want to transfer.  2. Type `scp -i $KEY -r $(pwd) ec2-user@$HOST:/home/ec2-user` to copy your entire working directory to the home directory of your VM. (tip: this method assumes you're not connected yet. If you are though, press Ctrl + D to disconnect).   ![Move files](../img/move-files.gif)  1. Alternatively, install File Transfer Protocol (FTP) client [FileZilla](https://filezilla-project.org). 2. Click on File > Site Manager > New Site. 3. Change the protocol to \"SFTP\". 4. Use the public IPV4 DNS address (see AWS console) as the host name. 5. Choose \"Key file\" as the logon type, set the username to `ec2-user`, and add your `*.pem` file as the key file. 6. Click on \"Connect\". On the right side, all files on the EC2 Instance are listed, and on the left side you can find the files on your local machine. To copy files from one place to another, you can simply drag and drop them between both windows. "}, {"objectID": "./tutorials/more-tutorials/running-computations-remotely/cloud-computing.md", "title": "Overview", "description": "Learn how to run computations remotely", "keywords": "cloud computing, azure, amazon, google, virtual machine", "code": [], "headers": ["Why Cloud Computing?", "Comparison Cloud Providers"], "content": " With increasing demands in computations (i.e., high number of CPUs) or data (i.e., high memory), it can be worthwhile to move your computations and data preparation pipelines to the cloud. Cloud providers offer virtual machines (VM) for rent that you configure to your own needs. For example, if you ran out of RAM on your local machine due to complex computational tasks, you can launch a VM instance with a high amount of memory to execute the computations in the cloud. This not only speeds up computations, but also minimizes errors (e.g., avoid computer restart) and interference from others.  In this tutorial we outline how to launch, monitor, and terminate virtual instances on Amazon Web Services (AWS).   The three biggest cloud VM infrastructure providers are: Amazon Elastic Cloud 2 (EC2), Google Compute Engine (EC), and Microsoft Azure Virtual Machines (VM). VM instances can be perceived as real computers with CPU, memory, network, storage configurations, and an operation system (also known as image). The table below indicates the differences between the three providers.  | | [Amazon EC2](https://portal.aws.amazon.com/billing/signup#/start) | [Google CE](https://accounts.google.com/signin/v2/identifier?service=cloudconsole) | [Azure VM](https://azure.microsoft.com/en-us/free/) |  | ---: | :--- | :--- | :--- | | CPUs | 1-40  | 1-32 | 1-32  | | RAM memory | 0.5 - 244 GB | 0.6 - 208 GB | 0.75 - 448 GB |  | Temporary storage limit | 48 TB | 3 TB | 4 TB |  | Number of image templates  | 39  | 18 | 40 | | Free credits students | $200  | $300 | $100 | | Creditcard required  | No | Yes | No |  We choose for AWS EC2 here because of its \"always free\" tier of the most basic VMs, ease of use, and the educational programme they have in place ([AWS Educate](https://aws.amazon.com/education/awseducate/)) that allows educators to create classes, allocate credits to students, and invite them to sign up without a credit card.   "}, {"objectID": "./tutorials/more-tutorials/airbnb-workflow/airbnb-workflow-overview.md", "title": "Overview", "description": "Set up an end-to-end workflow to investigate the impact of COVID-19 on AirBnB bookings", "keywords": "airbnb, make, R, workflow, inside airbnb, walkthrough", "code": [], "headers": ["Inside AirBnB", "Research Question  "], "content": " # Overview  [Inside Airbnb](http://insideairbnb.com/index.html) is an independent open-source data tool developed by community activist Murray Cox who aims to shed light on how Airbnb is being used and affecting neighborhoods in large cities. The tool provides a visual overview of the amount. availability, and spread of rooms across a city, an approximation of the number of bookings and occupancy rate, and the number of listings per host.  For example, [here](http://insideairbnb.com/amsterdam/) is the dashboard for the city of Amsterdam which shows us that 79% of the 19,619 listings are entire homes of which about a quarter is available all year round. Moreover, the animation below illustrates how the number of listings have been growing rapidly throughout the years:  ![title](../images/airbnb_expansion.gif)     The overarching research question that we aim to investigate is:     **How did the Airbnb market in Amsterdam respond to the COVID-19 pandemic in terms of bookings per neighborhood?**  To get to an answer, we need a multitude of input, transformation, and output steps. We'll use `make` to automate our workflows and make them reproducible. As data on Inside Airbnb is updated monthly, our program should still work once new data becomes available or if we change the city from Amsterdam to, for example, New York. "}, {"objectID": "./tutorials/project-management/principles-of-project-setup-and-workflow-management/collaboration.md", "title": "Collaborating using GitHub", "description": "While you can use Git exclusively to keep track of your files' version history, the real power of Git lies in collaborating on projects with others.", "keywords": "git, github, collaboration, sharing, share, repository, gist, contributing", "code": [], "headers": ["Overview", "Repositories versus Gists", "Contributing to a project", "Contribution Guide"], "content": "  While you can use Git exclusively to keep track of your files' version history, the real power of Git lies in **collaborating on coding projects with others**. Below, we outline how you can contribute to existing (open-source) projects that are hosted on GitHub.   GitHub knows two types of \"projects\". Full-fledged projects (e.g., for software development or research) are hosted in so-called \"repositories\", which offer you the full range of development features available. Configuring a GitHub repository to work well may take a while.  Gists, in turn, are only code snippets - e.g., a small piece of R code to clean some data, or a bit of Python code with some data to demonstrate a feature that you have developed. Gists are *ideal* to very quickly share stuff with the community to receive feedback. At the same time, you can hope for members of your wider network to comment on your code and improve it!  Below, we refer to repositories and Gists as \"projects\".   GitHub hosts thousands of open-source Gists and repositories that you can integrate in your own work, or - alternatively - that you can contribute to. Further, Git knows \"private\" Gists and repositories that you can only share with selected team members.  In following our guide, you have to ask yourself two questions:  - **Are you already a team member of the GitHub repository or Gist you would like to contribute to?** Probably not, at least that would be the default case for open source projects you     would like to contribute to. In that case, you have to make **\"fork\"** (i.e., a copy) of the     original repository or Gist in your own GitHub profile so that you actually     can edit files.  - **Would you like to contribute to the project using your browser, or would you instead like to \"build\" the entire project on your computer?** Editing a project in the browser is way easier, but you cannot test whether     the project still works. Building the entire project on your computer is a bit more complex, as you     will not only have to *clone* the repository or Gist to your computer,     but you also have to install necessary software to actually build the project     for testing.   Here's our contribution guide (these instructions     also apply to [contributing to this site](/contribute)).  1. To get started... (you need to only do this once per repository)     1. Forking required?         - If you would like to contribute to a project you are NOT officially a member of (such as a public open source project), you first **need to fork** the originating Gist or repository, which creates a copy of the original repository in your own GitHub profile (press the \"fork\" button on GitHub). You can now edit the project without breaking the original version.         - If you are a member of the project (i.e., your user name has been added to the list of contributors in the repository's settings), then you do **not need to fork** the repository, as you can directly work in the original repository.     2. Cloning to your local computer required?         - If you would like to contribute to the project using the GitHub web interface (recommended for novices), it's not required to clone the repository to your computer.         - If you would like to make changes to the project on your local computer, and also test whether the project still runs well (e.g., such as to engage into prototyping), you have to **clone** a copy of your Gist/repository to your computer via Git Bash.             - Open Git Bash             - Type `git clone https://github.com/[user-name]/repository-name.git`, followed by Enter.             - If you forked the repository in the previous step, use the URL of the fork here. Otherwise, use the originating repository's URL.             - Last, and finally, **get the Gist/repository working** on your computer (e.g., install the necessary software tools for it to run). Check the project's readme file, which will point you to the repository-specific installation instructions.  2. Make changes     1. Making changes directly on the website of GitHub (i.e., if you have not cloned the repository)         - Within your version of the repository, create a branch for each significant change you are planning to make. To do so, click on the [branch selector menu](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-and-deleting-branches-within-your-repository) and enter a name for your branch. Gists do not know branches - so no need to trying to create one.         - Navigate to the file(s) you wish to change (within the new branch), and make revisions as required.         - Commit all changed files within the appropriate branches.     2. Making changes on your local computer (i.e., if you have cloned the repository)         - For *repositories* (which know the concept of branching):             - Within your version of the forked repository, create a branch for each significant change you are planning to make. To do so, use Git Bash and use the command `git branch branch-name`. Create a branch for each feature you would like to implement.             - Change to the newly created branch (instead of making changes to the master branch), by using `git checkout branch-name` in Git Bash.             - Navigate to the file(s) you wish to change within the new branch, and make revisions as required (e.g., using your favorite software tools, like R or Python - depending on the type of source code, obviously).             - Commit all changed files within the appropriate branches (`git status`, `git add`, `git commit -m \"your description here\"`). Finally, push your changes to GitHub (`git push`). Need a refresher? You can review the [necessary steps versioning files with Git/GitHub here](../versioning).         - For *gists*:             - Navigate to the file(s) you wish to change, and make revisions as required (e.g., using your favorite software tools, like R or Python - depending on the type of source code, obviously).             - Commit all changed files (`git status`, `git add`, `git commit -m \"your description here\"`). Finally, push your changes to GitHub (`git push`). Need a refresher? You can review the [necessary steps on working with Git/GitHub here](../versioning).  3. Contribute your changes to the originating repository     - If you are a team member and have worked on the original repository or Gist, your changes     are already fully \"saved\".     - If you are not a team member and have worked on a forked repository or Gist, do as follows         - For *repositories*:             - It's time to shine! Share your changes with the world, by creating individual pull requests from each of your changed branches to the `master` branch within the originating repository.             - To do so, navigate to the forked copy of the repository, available at your personal GitHub page. Then, click on \"pull request\" to notify the owner of the original repository about the new features.             - You will likely receive feedback and may be requested to make changes using issue-specific branches of the forked repository. Repeat as needed until all feedback has been addressed.         - For *Gists*:             - Navigate to the original Gist, and scroll down to the comment section. Write about the changes you have made in your Fork, and include a URL to your fork! The project owner will then get in touch with you to discuss how the changes can be implemented. (Unfortunately, GitHub does not know the concepts of pull requests for Gists.)  {{< youtube pOhl932vbTI iframe-video-margins >}}  {{% warning %}} - Before starting to make any changes in your forked repository, make sure the fork is up-to-date with its originating repository. - Please only work from your newly-created branch(es) in a forked repository, and *not* in a clone of the originating master branch. {{% /warning %}}  {{% tip %}} More info? Follow this [fantastic tutorial on contributing to an open-source project on GitHub](https://akrabat.com/the-beginners-guide-to-contributing-to-a-github-project/). {{% /tip %}} "}, {"objectID": "./tutorials/project-management/principles-of-project-setup-and-workflow-management/versioning.md", "title": "Versioning using Git and GitHub", "description": "Git is an open-source version control system that allows you to keep track of your source files, and the changes you make to them", "keywords": "git, versioning, github, configuration, repository", "code": [" ```bash # set the author name for your commits git config --global user.name \"[name]\"  # set the author email for your commits git config --global user.email \"[email address]\" ``` ", " ```bash # initialize a new repository git init ``` ", " ```bash # clone an existing repository from Github to local PC git clone \"[url]\" ``` ", " ```bash # show files or directories that were changed (or that still need to be added) # files that are already in the staging area are green; files that are not # in the staging area are red. git status  # add a directory or file to the staging area git add directory-name-or-file-name # for example, git add sourcecode.R, or dir-name/sourcecode.R.  # run git status again, to see that the files have \"turned green\" git status  # added a wrong file to the staging area, and want it to turn \"red\" again? git reset directory-name-or-file-name  # are you happy with what is green in your staging area? Then it's # time to finalize your save operation by initiating a so-called \"commit\". git commit -m \"message\" # (give it a clear message, so that you can easily roll back to this version # of your repository.  # optionally, use git log to see the versioning history git log ``` ", " ```bash # (1) DOWNLOAD AND MERGE CHANGES  # download and merge any changes from the remote/GitHub git pull  # alternatively, you can do the same in two steps: git fetch # fetches changes git merge # merges changes with your local files  # (2) UPLOAD CHANGES  # upload all local branch commits to GitHub git push ``` ", " ```bash # create a branch [branch-name] git branch [branch-name]  # switch to s specific branch [branch-name] git checkout [branch-name]  # merge a specific branch [branch] to the working branch; #, e.g., if you're in the master branch, you can integrate # any changes done in the feature [branch]. git merge [branch]  # delete a specific branch [branch-name] git branch -d [branch-name] ``` "], "headers": ["Overview", "What is Git?", "What is GitHub?", "Let's Use Git!", "1. Configuring Git on a new computer", "2a. Create a repository for a new project", "2b. Create a repository for an existing project", "2c. Clone an existing repository to your local computer", "3. Work on your project", "3a. Track your changes", "3b. Synchronize your changes with a server", "4. Branching", "Summary"], "content": "   **Git** is an [open-source version control system](/building-blocks/configure-your-computer/statistics-and-computation/git/), which allows you to keep track of your source files, and the changes you make to them. Using Git, you can roll back to any previous version of a file, and easily collaborate with team members.  Git \"lives\" on your local computer, and allows you to configure so-called \"repositories\" that track files and directories.  {{% tip %}} **Storing sensitive data**  Git will eventually become the long-term memory of your project, and you may decide to make the repository public so others can learn from or use your work. Therefore, it is crucial that you __do not store any sensitive information__ in your source code (e.g., API credentials, passwords). {{% /tip %}}   You can optionally use Git in combination with an online hosting service such as [**GitHub**](https://www.github.com), which allows you to backup your code, synchronize your work across multiple computers, or collaborate with others on joint projects. There are several alternatives to GitHub available, and maybe your institution even offers its own \"Git\" services.  On top of the basic features of Git (like versioning), GitHub offers a range of tools that allow you to collaborate with each other more efficiently. A few examples: - *Issues* are sort of to-do lists, flavored with a discussion board - *Projects* are a sort of [*Scrum board*](https://www.visual-paradigm.com/scrum/how-to-use-scrum-board-for-agile-development/).   {{% tip %}} **Download the GitHub cheatsheet!**  There is no better summary than [this official GitHub cheatsheet](https://education.github.com/git-cheat-sheet-education.pdf). Download it, print it out or put it on your desktop. Consider it as your best friend for a while. {{% /tip %}}  **(Check [cheatsheet](https://education.github.com/git-cheat-sheet-education.pdf) &rarr; setup)**  To use Git locally, tell Git your name and email address, so that any work you do (and potentially sync later) can be linked to your name.  Open `Git bash`, and type the following commands:     If you would like to start a new project, it's easiest to start your repository from one of our [example projects](/examples). Just navigate to a project's GitHub page, and select *Use this template* (a green button). Choose a name for your new repository, and proceed with the standard options.  Note down the URL for your repository (e.g., `https://github.com/tilburgsciencehub/test-repository`), and proceed to step 2c.  **(Check [cheatsheet](https://education.github.com/git-cheat-sheet-education.pdf) &rarr; setup & init)**  Chance is you are already working on a project, and you'd like to adopt Git to start versioning your files from now onwards.  Just navigate to your main project directory, open `Git bash`, and type    Then, proceed to step 3.  **(Check [cheatsheet](https://education.github.com/git-cheat-sheet-education.pdf) &rarr; setup & init)**  Your code is already hosted on GitHub, or you'd like to take a sneak preview of one of the thousands of projects hosted on GitHub? Just note down the URL of the repository (e.g., `https://github.com/[username]/[project-name]`), and clone the repository to your local computer.    For example, type `git clone \"https://github.com/rgreminger/example-make-workflow\"` to clone a copy of the example workflow using R and make (see also the [example projects](/examples)). You can also change the default directory name to which the repository will be cloned. For example, typing `git clone \"https://github.com/rgreminger/example-make-workflow my-project` will clone the repository to the folder `my-project`.  Proceed to step 3.  {{% tip %}} Ideally, you use a main project folder on your computer, pretty high-up in your folder hierarchy (e.g., 'D:/projects/`). While technically feasible, you should avoid storing your Git repositories on Dropbox as this may lead to synchronization conflicts. {{% /tip %}}   When working on your project, you do the following two things: - track changes to your project (e.g., such as adding, removing, or changing source files), - synchronize your repository with GitHub, so that (a) you make a backup of your changes, and (b) you allow other team members to see your changes, and (c) you see changes that team members (may) have done.  ![Git workflow.](../git_workflow.png)  **(Check [cheatsheet](https://education.github.com/git-cheat-sheet-education.pdf) &rarr; stage & snapshot)**  Now it's time to start working on your project. \"Working\" means making changes to files or directories (such as adding, changing, or deleting files or directories).  Typically, you execute this workflow multiple times a day when working on a project. Every time you execute this workflow, you \"save\" a snapshot of your project that you can roll back to later.  Git separates a \"save\" operation in two stages: first, files (or directories) can be gathered on a so-called \"staging area\" using the command `git add`. You can use multiple of these commands after each other. Then, in a second step, the changes are saved - or, in Git terminology - \"committed\". The command for this is `git commit`. See below for an example, which also adds a few other useful commands.    {{% tip %}} **Exclude files from tracking**  Experience shows that you want to avoid tracking specific files and folders. For example, if you recall the [directory structure](../directories) for your project, there is no point in tracking *generated files* in `/gen`, as these files are purely created based on source code in `/src` (which, in turn, you *do* would like to track).  You can exclude files and directories from tracking by putting a textfile called `.gitignore` in your project's root directory.  Check out a few example `.gitignore` files for [inspiration](https://github.com/rgreminger/example-make-workflow/blob/master/.gitignore), or copy-paste the following to your own `.gitignore`:      **/rbin/     **/raw/     *RData     *pdf     **/audit     **/input     **/output     **/temp     **/zip     *csv     *xlsx     *~*     *log     *.Rhistory     **/exports     **.ipynb_checkpoints     **__pycache__     *.log     slides/*.gz     slides/*.snm     slides/*.toc     slides/*.nav     slides/*.out     slides/*.aux     .RProfile  {{% /tip %}}  **(Check [cheatsheet](https://education.github.com/git-cheat-sheet-education.pdf) &rarr; share & update)**  Everyone can sync their local changes with the remote repository on GitHub. You can also \"download\" changes to your local repository from the remote repository.    **(Check [cheatsheet](https://education.github.com/git-cheat-sheet-education.pdf) &rarr; branch & merge)**  Branches are key tools for version control. In essence, branches separate the main version of your project (the \"master\" branch), from any experimental code in which you develop new features.  One team member is in charge of the \"master\" branch, while everybody else implements new features in feature branches (give them any name you like, but names should be easily understood by anybody in your team).    After integrating changes from a particular branch, you ideally synchronize changes with the remote repository on GitHub.   Want to know more about how to use Git? Check out the lessons at [software carpentry](https://software-carpentry.org/lessons/), and make use of the [cheatsheet](https://education.github.com/git-cheat-sheet-education.pdf) throughout your work. Also, our friends at the University of Zurich have a [fantastic tutorial on using Git](https://github.com/pp4rs/2020-uzh-course-material/blob/master/11-version-control/slides/git-local.pdf).  {{% summary %}}  ![Git workflows.](../git.png)  1. We distinguish between **local** and **remote** repositories.     - Each project can consist of multiple local repositories, which are stored on one or many computers (e.g., your desktop PC, your laptop, a computer in a cloud, or computers by team members).     - Each project typically has one remote repository (e.g., hosted on GitHub), which is used to backup and synchronize changes between multiple computers.  2. Each project has a \"working tree\" (your project's main directory) - by default, all files in that working directory can be tracked.  3. Workflow to version your files     - Run `git status` to see which files are staged (green), and which ones are not (red)     - Add files to the 'staging area', using the command `git add`     - Run `git status` to verify you have tracked everything you want.     - See files and/or directories that you never want to track? Add those to a `.gitignore` file (typically data files, or generated output files)     - Finalize your \"save\" by running `git commit -m \"give yourself a clear message \"`, which will commit any changes to your project's history. 4. Workflow to synchronize changes with a remote repository     - Run `git pull`; alternatively, first run `git fetch` and then `git merge`     - Push (`git push`) your own local changes to the repository (so that others see your changes) 5. Other useful commands     - Use `git checkout` to switch branches     - Use `git clone` to clone repositories from GitHub  {{% /summary %}} "}, {"objectID": "./tutorials/project-management/principles-of-project-setup-and-workflow-management/pipeline.md", "title": "Pipelines and Project Components", "description": "Let's break down a project into its most basic parts, which we call pipeline and components.", "keywords": "pipeline, project, components, stages, steps", "code": [], "headers": ["Overview", "Project Pipelines", "Project Components", "Putting it all together..."], "content": "  It is useful to break down a project into its most basic parts, which we call **pipeline** and **components**.  - A pipeline refers to the **steps that are necessary to build a project** (e.g., prepare dataset, run model, produce tables and figures), and  - Components refer to **a project's most nuclear building blocks** (e.g., data, source code, and generated temporary and/or output files).  Later on, you will see that such a structure enables you to work on your project using multiple computers (e.g., your workstation, your laptop, a computer in the cloud), or with various collaborators/co-authors.   In a research project, one typically has several tasks to accomplish, such as preparing the data, analyzing the data, writing the paper and producing a set of slides.  **This is what we call a \"project pipeline.\"**  A typical pipeline for an academic paper may look like this:  {{% example %}} **Typical pipeline for an academic paper** - Prepare dataset for analysis - Run model on a dataset - Produce tables and figures for the paper {{% /example %}}  Over time, your pipeline will grow increasingly complex. For example, the pipeline above recently \"matured\" into this one:  {{% example %}} **More complex pipeline for an academic paper** - Download public datasets from the U.N. (to be used for some control variables) - Have an RA code some auxiliary variables - files to be delivered in Excel - Merge your primary data set with control variables from previous steps; generate to \"derivative\" data sets - one at the monthly level, and one at the weekly level. - Estimate models on both data sets - Systematically compare both models - Choose the best model and produce tables and figures for a paper. {{% /example %}}  Of course, you can even come up with pipelines that are even more complex - not because we love complexity, but because we love clarity.  The benefits of conceiving your project like a pipeline are straightforward:  - **Write clearer source code:** separate your thousand-line source code into multiple smaller, more accessible components (as defined in your pipeline, like preparing dataset, then running a model, then producing tables and figures).  - **Obtain results faster:** Because your project is separated into different pipeline stages and each of these stages is *self-contained*, you can easily run \"later\" stages of your project (we call this \"downstream\"), based on *different* input files defined earlier in your project (we call this \"upstream\"). You can run later stages in your project (e.g., \"analysis\") using different inputs (e.g., a data set at the monthly level, and at the weekly level) - that way, you can rapidly understand how robust your findings are with regard to alternative specifications.  - **Increase transparency and foster collaboration:** With more transparent source code, you allow others to more easily understand your code. Also, why not make a co-author responsible for prepping that data for you (\"upstream\" in your pipeline), while you concentrate on developing a prototype model (more \"downstream\" in your pipeline)?  - **Use multiple software packages:** In a pipeline, the \"outputs\" of one module serve as the \"inputs\" of another. Hence, you can use multiple software packages to accomplish different tasks in your pipeline. For example, we use R's powerful *data.table* package to clean data, and pass on the cleaned dataset to a clustering algorithm in Python's *scikit-learn*. In other words, we use whatever software package we deem most appropriate for each step in our pipeline.   Now that we've covered what a pipeline is, let's draw our attention to **project components**, which are the most nuclear building blocks of a project. It's useful to think of these components as *separate entities of your project* because their nature allows you to apply different data management policies.  * For example, we probably all agree it's desirable to *roll back to previous versions of a project* (e.g., an earlier version of a prepped dataset). But - if you work on large datasets - it may probably be too burdensome to store each version of your **generated files** (e.g., in one of the projects we've been working on, the generated (cleaned) data sets were a 500 GB, and we've created probably close to 50 versions = 25 TB).  * If you think about this a bit more, you may discover that storing these different data sets is completely inefficient - as the combination of raw data and *versioned source code* will be able to \"re-cast\" any data set version you have ever worked on.   Suppose you needed to bring structure to a chaotic and poorly organized project, then probably you may end up with the following project \"components\":  1. **raw data**      These are the data sets used in the projects. Typically, you will never *delete* that data,     but only *add* new data sets throughout the duration of the project.  2. **source code**      This is all the source code written to *execute your pipeline* (defined earlier; e.g., preparing data, analyzing it, and producing some tables or figures).      - Each source code file typically...         - loads some input files or accepts some arguments,         - does some operations on data (e.g., clean it, estimate a model), and         - then saves *generated files* (output, audit, or temporary files).      - It's important to version your code, so that you can always roll back to different incarnations of your project.      3. **generated files (temporary, output, and audit)**      These files are *the result of executing your source code*. Think about cleaned     data sets, results of your analysis, tables and figures, or some log files to audit whether everything went well (e.g., counting the number of observations, and writing those in `audit.txt`). In most instances, it may be enough to store     those only temporarily on your local computer.  4. **file exchange**      File exchanges are used to easily \"ping-pong\" (upload or download)     generated temporary or output files between different stages of the pipeline.  {{% example %}} **Examples of file exchanges**  - A file exchange to **collaborate with others**:   - A co-author builds a prototype model on his laptop (see stage 2 of the pipeline).   He/she can work on a dataset that you have prepped using a high-performance workstation (this part of the pipeline), without having to actually build the dataset him/herself from scratch.  - A file exchange **without any collaboration**:   - You have built a data set on a high-performance workstation,   and would like to work with a sample dataset on your laptop, without   having to actually build the dataset from scratch.  - Situations in which a file exchange **is not necessary**:   - You are working exclusively on one computer, without any   coauthors.  - Curious on how to set up a file exchange? We'll explain that later. {{% /example %}}    5. **notes**        You can keep your meeting notes, PDFs of the literature that you've read, etc. in a separate       folder (with, potentially, subfolders).   It's time to finally build your project! See our overview chart below, which illustrates how different stages of your pipeline use different project components.  ![Workflow overview](../workflow.png)  {{% summary %}} **Written explanation of the workflow**  1. **Stage 1 - Prepare raw data**: In the first stage of the pipeline, we use a workstation with relatively large memory (e.g., 64 GB) to download the raw data from our secure server, and run some code to prepare different versions of the dataset (e.g., the final data, and dataset only 1% of the size of the full one). We version our code using Git, and upload the two final datasets to the temporary file exchange.  2. **Stage 2 - build prototype model**: In the second stage of the pipeline, we have switched to our laptop, and pull in only the sample data set from the file exchange. That one is only a few megabytes big, so it's perfect to develop a prototype model and extensively test it. We pass the model results back to the file exchange so that we can already generate some tables and figures for stage 4 of the project. Of course, we're also committing our changes to the model code to Git so that later pipeline stages can use our updated model code.  3. **Stage 3 - estimate final model**: It's time to estimate our model on the entire dataset! We've launched a powerful EC2 instance in the Amazon Cloud, cloned our source code using Git, pulled in that big final dataset from the file exchange, and estimate our model. We're relieved the model is running in the cloud, as that computer likely will not restart or crash.  4. **Stage 4 - prepare tables and figures & write paper**: Time to shine! We pull in our final data set from stage 1, and either the prototype model results (from stage 2) or the final model results (from stage 3) to produce tables (e.g., descriptive stats, model results) and figures (e.g., plotting some data) that will directly feed in our paper our slide deck. {{% /summary %}}  {{% summary %}} You've just learnt about two essential ways to look at a project.  1. The **pipeline** defines the logical steps in which your project is built (such as prepping data, analyzing it, and creating tables and figures). 2. The **components** refer to the most nuclear units in a project - consisting of raw data, source code, generated temporary/output files, a file exchange, and lastly a collection of notes/other documents.  The power of setting up your project that way lies in:  - **full portability**   - Because of the modular nature of the project, each pipeline   can essentially be executed on different computing systems. That's super   handy if you have a powerful workstation in your office (e.g., high RAM to prep   some large data sets), but would like to work on your laptop while traveling to   make some progress on your model prototype.   - Your project may need large computational resources to estimate your model on your final data set -   so you can easily \"port\" your entire infrastructure to a high-performance cluster   (e.g., surfsara in Amsterdam, EC2 at Amazon Web Services, etc.). - **reproducibility and transparency**   - nothing in this project is \"manually edited\" (e.g., deleting some observations     in an Excel file) - every bit of your project is documented     in source code.   - anybody interested in running your analysis can run your code   (but tell them to visit our website first!) ;)   - even *you* will be able to understand your project better,   e.g., when you continue working on it after a while, like for a revision. {{% /summary %}} "}, {"objectID": "./tutorials/project-management/principles-of-project-setup-and-workflow-management/overview.md", "title": "Overview", "description": "When working on a project, most of us spend time thinking about what to create (a cleaned data set, a new algorithm, an analysis, a paper and corresponding slides), but not about how to manage its creation.", "keywords": "principles, workflow, setup, project, configure", "code": [], "headers": ["Motivation", "Guiding Principles", "Gradual Implementation", "Configure your Computer"], "content": "  When working on a project, most of us spend time thinking about *what* to create (e.g., a cleaned data set, a new algorithm, an analysis, and/or a paper and corresponding slides), but not about how to *manage* its creation. This results in two major issues, as outlined next.  **Major issues in managing data-intensive projects**  - we may lose sight of the project (\"directory and file chaos\")    - Gradually, we write code and edit data sets, and put those files somewhere on our computer.   - When we update files, we either overwrite them, or save new versions under different file names (e.g., including dates like `20191126_mycode.R` or version numbers like `mycode_v2.R`).   - Even with *best intentions to keep everything tidy*, months or years of working on a project will very likely result in chaos.  - we may find it difficult to (re)execute the project (\"lack of automation\")   - The way you have set up your project may make it cumbersome to execute the project. Which code files to run, which not? How long does it take for a code file to complete running?   - For example, you wish to re-do your analysis on a small subset of the data (either for prototyping, or as a robustness check), or you would like to try out new variables or test whether a new package provides speed gains...   - However, re-running your project takes a lot of time - if at all you remember how to \"run\" the various code files you have written.  The primary mission of **managing data- and computation-intensive projects** is to build a transparent project infrastructure, that allows for easily (re)executing your code potentially many times.   The objectives of this tutorial are:  - learn how to organize and track the evolution of your projects (e.g., by means of a proper [directory structure](../directories), and [code versioning](../versioning)) - learn how to automize your workflows, and make them reproducible (e.g., by using [automation](../automation)) - learn how to [work on projects with others](../collaboration) (e.g., by means of Git/GitHub) - learn how to [document datasets](../documenting-data) and [workflows](../documenting-code) - learn how to write clean code (e.g., see our [Building Blocks](/building-blocks))   {{% tip %}} **Gradually implement our suggestions.**  * We may sometimes sound a bit dogmatic (e.g., you must do this or that). Some of our instructions will only make sense to you after a while. So, please stick with our recommendations during the course of your first few projects. Later on, take the liberty to re-design the workflows to your needs. * Consider adopting our suggestions **gradually**.   1. Start with a proper [directory structure on your local computer](../directories/#data-management-for-each-of-the-projects-components), which   may already be sufficient to start collaborating. For example,   do you need feedback from your advisor? Just zip (the relevant parts of) your pipeline   and use [SURF's filesending service (for researchers affiliated with Dutch institutions)](https://filesender.surf.nl/) to send it!   2. Start [automating](../automation) (parts of) your pipeline   3. Document your [project](../documenting-code) and [raw data](../documenting-data)   4. Start to [track changes to your source code](../versioning), and [clean up your source/\"do your housekeeping\"](../checklist) regularly {{% /tip %}} {{% warning %}} **Uhh, you just suggested to send an email, really?!** - Indeed, email is not what we want to advocate. - But then again, we want you to get started with managing your workflows right away, and adhering to the directory structure outlined above already increases your efficiency. - So, before you proceed to the future chapters of this guide, sit back, and relax, and keep on using good old email. {{% /warning %}}  Think your machine is already configured well? Then proceed directly to the next page.  {{% tip %}} **Configure your computer**. - Note that to implement our workflow suggestions, your computer needs to be configured properly - so we [suggest you to do that first](/building-blocks/configure-your-computer). - Of course, you need not to install all software tools - but pick *at least* your statistical software package (e.g., [we use R](/get/r/), but others prefer [Stata](/get/stata/)), [Python](/get/python/), and [`make`](/get/make/). {{% /tip %}}  Of course, there are many ways to set up a machine, and we do not mean to consider our way of doing to be perfect. In fact, the setup instructions sometimes resemble a compromise, so that later instructions - given to both Windows, Mac and Linux users - can be followed as closely as possible.  **If you are writing your Master thesis at Tilburg University, please attempt to install necessary software and packages prior to your first meeting.**  {{% summary %}} * If everything goes smoothly, you should be able to complete the installation in one sitting within 60-120 minutes. * Please follow the steps one-by-one in the order they appear on the side bar and do not deviate from them, unless you really know what you are doing. * Where necessary, we have provided instructions for Mac, Windows and Linux machines. {{% /summary %}}  {{% warning %}} * We will use the terms command prompt (Windows) and terminal (Linux, Mac) interchangeably. {{% /warning %}} <!---* You should be able to complete this subchapter in  sitting within 90-150 minutes.-->  <!-- !!! warning \tThis site is under development, and will be updated continuously. Please check back frequently. --!>  <!--#* Please follow the steps one-by-one in the order they appear on the side bar and do not deviate from them, unless you really know what you are doing. #* Where necessary, we have provided instructions for Mac, Windows and Linux machines. --!> <!-- [^1]:  As you will quickly realize, the folder structure is a mess, and it is close to impossible to find the code that prepared the datasets, or the code that was used to estimate the econometric model that eventually got published (if you do find these files, please let us know). ;-)  --> "}, {"objectID": "./tutorials/project-management/principles-of-project-setup-and-workflow-management/documenting-code.md", "title": "Documenting Source Code and Pipeline Workflows", "description": "Documenting your project's workflow, not only for others, but also for your future self is absolutely crucial. Learn how.", "keywords": "document, code, workflow, reference, comment, documentation", "code": ["  ```txt =================================================================== PROJECT NAME ===================================================================  DESCRIPTION: ------------ Put project description here. You can use multiple lines, but keep the width of the text limited to the header.  AUTHORS: -------- Hannes Datta, h.datta@tilburguniversity.edu (maintainer)  LAST UPDATED: ------------- 29 NOVEMBER 2019   BUILD INSTRUCTIONS ==================  1) Dependencies  Please follow the installation guide on https://www.tilburgsciencehub.com/ for  - R and RStudio (3.6.x)   Install the following R packages:  \tpackages <- c(\"data.table\", \"ggplot2\")  \tinstall.packages(packages)  - Gnu Make   Put GnuMake and R to path so that you can run it   from anywhere on your system. See http://www.tilburgsciencehub.com/  - Obtain raw data files and put them into /data/  2) Directory structure  The project pipeline consists of the following stages:  /src/collect                Code required to collect/download raw data /src/data-preparation       Data preparation /src/analysis               Data analysis /src/paper                  Stores literature reference, paper, and slides  Each directory has a makefile, with running descriptions for each stage of the pipeline.  For each pipeline stage, the /gen directory contains files generated on the basis of the /data and source code stored in /src.  Each directory contains subdirectories, \t/input (for input files) \t/output (for final output files) \t/temp (for any temporary files) \t/audit (for any auditing files)  3) How to run the project  Navigate to the project's root directory, open a terminal, and run  > make  ``` ", "  ```txt OVERVIEW ==================================================== - Provide a two or three sentence overview of the directory.  DESCRIPTION ========================================================== - If you are using a makefile (strongly recommended!),   please refer to the content of that file for running instructions.  - If you do not make use of a makefile, please briefly describe   the contents of the subdirectory and its files.   Also provide instructions how to run the files, and in which order.  ``` "], "headers": ["Overview", "Main Project Documentation", "Documentation for each stage of the pipeline"], "content": "  Documenting your project's *workflow*, not only for others, but also for your future self (i.e., if you plan to continue working on the project after a while) is **absolutely crucial** to the long-term success of you as a researcher or analyst.  Typically, you would like to  - include a main project documentation, and - one documentation each for each [stage of your pipeline](../pipeline).   You should place a main project documentation in the root directory of your project (`/my_project`), and call it `readme.txt`. Keep the document *brief and simple*, but include at least the following information:  * Project name * Details about the project \t* Project description (\"what does the project do?\") \t* Authors and email addresses \t* Date of last update * Build instructions \t* Dependencies (\"what software is needed to replicate the project?\") \t* Explaining the [directory structure](../directories) (\"where to find what?\") \t* How to run/build the project  Here is an example documentation you can use as a template:     Ideally, a `makefile` lists all the necessary steps to run your pipeline. If you do not have a `makefile` yet, include a `readme.txt` instead.  Here is a `readme.txt` template to start from:   "}, {"objectID": "./tutorials/project-management/principles-of-project-setup-and-workflow-management/checklist.md", "title": "Checklist to Audit Data- and Computation-intensive Projects", "description": "Here is a checklist to audit your progress in making your workflows efficient, reproducible, and well-structured.", "keywords": "checklist, workflow, efficient, reproducible", "code": [], "headers": [], "content": " There is quite some material to cover to make sure your workflows become efficient, reproducible, and well-structured.  Here's a checklist you can use to audit your progress.  {{% wide-table %}} <!-- | Makefile available at the root of the project (tying together individual makefiles) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | -->  |                                                                         | data-preparation | analysis    | paper       | ...     | | ------------------------------------------------------------------------|:--------------:|:-----------:|:-----------:|:-------:| | **At the project level** | Implement a consistent [directory structure](../directories/#working-example): data/src/gen | Include [readme with project description](../documenting-code/#main-project-documentation) and technical instruction how to run/build the project | Store any authentication credentials outside of the repository (e.g., in a JSON file), NOT clear-text in source code | Mirror your `/data` folder to a secure backup location; alternatively, store all raw data on a secure server and download relevant files to `/data` | | **At the level of each stage of your pipeline** | *File/directory structure* | Create [subdirectory for source code](../directories/#working-example): `/src/[pipeline-stage-name]/` | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Create [subdirectories for generated files](../directories/#working-example) in `/gen/[pipeline-stage-name]/`: `temp`, `output`, and `audit`. | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Make all file names relative, and not absolute (i.e., never refer to C:/mydata/myproject, but only use relative paths, e.g., ../output) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Create directory structure from within your source code, or use .gitkeep | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | *Automation and Documentation* | Have a [`makefile`](../automation) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Alternatively, include a [readme with running instructions](../automation/#are-there-alternatives-to-make) | &#9744;        | &#9744;     | | Make dependencies between source code and files-to-be-built explicit, so that `make` automatically recognizes when a rule does not need to be run [(properly define targets and source files)](../automation) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Include function to delete temp, output files, and audit files in makefile | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | *Versioning* | Version all source code stored in `/src` (i.e., add to Git/GitHub) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Do not version any files in `/data` and `/gen` (i.e., do NOT add them to Git/GitHub) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Want to exclude additional files (e.g., files that (unintentionally) get written to `/src`? Use .gitignore for files/directories that need not to be versioned | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | *Housekeeping* | Have short and accessible variable names | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Loop what can be looped | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Break down \"long\" source code in subprograms/functions, or split script in multiple smaller scripts | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Delete what can be deleted (including unnecessary comments, legacy calls to packages/libraries, variables) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Use of asserts (i.e., make your program crash if it encounters an error which is not recognized as an error)| &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | *Testing for portability* | Tested on own computer (entirely wipe `/gen`, re-build the entire project using `make`) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Tested on own computer (first clone to new directory, then re-build the entire project using `make`) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Tested on different computer (Windows) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Tested on different computer (Mac) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | | Tested on different computer (Linux) | &#9744;        | &#9744;     | &#9744;     | &#9744; |    | {{% /wide-table %}}  {{% warning %}} **Versioned any sensitive data?**  Before making a GitHub repository public, we recommend you check that you have not stored any sensitive information in it (such as any passwords). This tool has worked great for us: [GitHub credentials scanner](https://geekflare.com/github-credentials-scanner/). {{% /warning %}} "}, {"objectID": "./tutorials/project-management/principles-of-project-setup-and-workflow-management/directories.md", "title": "Data Management and Directory Structure", "description": "A guide on how to manage project components, by casting a", "keywords": "data, directory, directories, data management, structure, raw, generated, source", "code": [], "headers": ["Overview", "Data Management for Each of the Project's Components", "1. Raw data", "Which data format to use", "Which directory structure to adhere to", "Where to store the data", "2. Source code", "Which files are source code", "Where to store the source code", "3. Generated files", "Which directory structure to adhere to", "Where to store the data", "4. File exchange", "5. Managing notes and other documents", "Summary", "Complete Project Directory Structure", "Download Example Directory Structure", "Most Important Data Management Policies by Project Component"], "content": "  From the [previous section in this guide](../pipeline), recall that a project can be logically structured in the following **components**: raw data (e.g., data sets), source code (e.g., code to prepare your data and to analyze it), generated files (e.g., some diagnostic plots or the result of your analysis), a file exchange, and notes (e.g., literature, meeting summaries).  Below, we give you some more guidance on how to *manage these components*, by casting a prototypical directory structure that you need to set up on your computer.  {{% tip %}} **Principles**  We adhere to the following principles in managing our project's data and code: - others should understand the project and its pipeline, merely by looking at your directory and file structure,  - each step in the pipeline is self-contained, i.e., it can be executed without actually having to run previous stages in the pipeline (\"portability\"), and  - the project is versioned (source code), and backed up (raw data, source code, notes) {{% /tip %}} <!---!!! hint \t\tRemember that next to the components (i.e., the most basic building blocks of the projects), \t\tyour project has a **pipeline** which defines the different tasks to accomplish (e.g., \t\tsuch as preparing and analyzing the data, writing the paper, etc.). --->  {{% tip %}} **Obtain approval for using external services**  Below, we're mentioning the use of several external services, such as Amazon Web Services (AWS), Dropbox, or Google Drive. Please make sure to obtain the approval of your organization to make use of these services. Typically, organizations also have a range of alternatives available in-house (e.g,. such as SurfDrive as a replacement for Dropbox for Dutch research institutions). {{% /tip %}}   Here's our advice on how to manage and store each component of your project. Of course, check with your own institution and their rules on data storage and management.  To start, let's assume we're working on a project, called `my_project`. Let us create that directory somewhere on our computer, preferably not in the cloud (i.e., not in a Dropbox or Google Drive folder).   Raw data gets downloaded into the `data` folder of your project (`my_project/data`) from either a network drive, or a remote file storage that is securely backed up.   - Raw data needs to be stored in text-based data files, such as CSV or JSON files. This ensures highest portability across different platforms, and also stability should certain software packages be discontinued or data formats being substantially changed.  - Do you use databases to handle data storage (e.g., such as a MySQL or MongoDB server)?  Then still, take CSV/JSON extracts from that server from time to time. The reason is that, somewhere down the road, you are going to shut down these database servers, but you still need access to the \"last version\" of your data.   - Please structure data in logically consistent \"subfolders\". For example, by data provider (e.g., website A, company B, data provider C).  - If you receive multiple versions of the same data (e.g., a dump in 2012, a dump in 2015), then also create subfolders for these particular years. It is not necessary to use file versioning for your raw data, as this may greatly exceed the capacity of services such as GitHub.  See below for an example directory structure for your raw data:      /data/website_A/...     /data/company_B/...     /data/data_provider_C/2019-11-04/...     /data/data_provider_C/2020-03-01/...  - We also recommend you to use self-explanatory file names for your data, [document the data yourself with a `readme`](../documenting-data), or include an overview about how the data was collected from your data provider.  - Last, it may happen that you code up some data yourself, and that you still wish to store multiple versions of that file. In that case, please only keep *the head copy* (i.e., the most recent version) in your folder, and move all outdated files to a subfolder (which you can call `/legacy` or `/outdated`).   - Ideally, the folder with your raw data is stored on a secure server that (a) project members have access to, and (b) gets backed up frequently. Good locations are:     - a secure network folder in your organization     - file hosting services such as Amazon S3     - public cloud services such as Dropbox or Google Drive  <!-- just click on the links to get instruction on how to do that**-->  - When working on the project, download copies of your raw data to your `my_project/data/` folder.  - No time to think about remote storage locations right now? Then just store your data in `my_project/data/` on your PC, and set yourself a reminder to move it to a more secure location soon. Do make backups of this folder though!   Source code is made available in the `src` folder of your main project: `my_project/src/`. Create subdirectories for each stage of your pipeline.   - Source code are all files that are required to execute your project's pipeline.  - In addition, source code consists of a [`makefile`](/get/make/) which makes explicit how the source code needs to be run, and in which order, and  - Scripts which put the `/gen/[pipeline-stage]/output` files from the current pipeline stage to the file exchange (`put_output`), so that other pipeline stages can pull in that data to its `/gen/[pipeline-stage]/input` folder (`get_input`). More about this [here](#4-file-exchange).  **Version your source code**  - Source code must be [*versioned*](../versioning) so that you can roll back to previous versions of the same files, and engage into \"housekeeping\" exercises to improve the readability of your code.  - Of course, versioning also is a requirement when you work with multiple team members on a project.  **Which directory structure to adhere to**  - Create subfolders for each stage of your [project's pipeline](../pipeline), and store source code pertaining to these stages in their respective directories.  - For example, let's assume your pipeline consists of three stages (ordered from \"upstream\" to \"downstream\" stages):     - the pipeline stage `data-preparation` is used to prepare/clean a data set     - the pipeline stage `analysis` is used to analyze the data cleaned in the previous step, and     - the pipeline stage `paper` produces tables and figures for the final paper.  Your directory structure for the `/src` directory would then become:      /src/data-preparation/     /src/analysis/     /src/paper/   - First of all, the source code is available on your local computer.  - Second, the source code is versioned using Git, and synchronized with [GitHub](http://github.com) (so, automatic backups are guaranteed).   - Generated files are all files that are generated by running your source code (`/src`) on your raw data (`/data`).  - Generated files are stored in the `gen` folder of your main project: `my_project/gen/`.  - You can use subdirectories that match your pipeline stages to further bring structure to your project.   Each subdirectory contains the following subdirectories:  - `input`: This subdirectory contains any required input files to run this step of the pipeline. Think of this as a directory that holds files from preceding modules (e.g., the analysis uses the *file exchange* to pull in the dataset from its preceding stage in the pipeline, `/data-preparation`).  - `temp`: These are temporary files, like an Excel dataset that needed to be converted to a CSV data set before reading it in your statistical software package.  - `output`: This subdirectory stores the *final* result of the module. For example, in the case of a data preparation module, you would expect this subdirectory to hold the final dataset. In the case of the analysis module, you would expect this directory to contain a document with the results of your analysis (e.g., some tables, or some figures).If necessary, pass these to a file exchange\tfor use in other stages of your pipeline.  - `audit`: Checking data and model quality is important. So use this directory to generate diagnostic information on the performance of each step in your pipeline. For example, for a data preparation, save a txt file with information on missing observations in your final data set. For an analysis, write down a txt file with some fit measures, etc.  Your directory structure for the generated data is:      /gen/data-preparation/input     /gen/data-preparation/temp     /gen/data-preparation/output     /gen/data-preparation/audit     /gen/analysis/input     /gen/analysis/temp     /gen/analysis/output     /gen/analysis/audit     /gen/paper/input     /gen/paper/temp     /gen/paper/output     /gen/paper/audit   - Since generated files are purely the result of running source code (which is versioned) on your raw data (which is backed up), it's sufficient to store generated files locally only.  - If you are working with other team members that may need access to the `/output` of preceding pipeline stages, you can make use of the file exchange (see next section).   - File exchanges are used to easily \"ping-pong\" (upload or download) generated temporary or output files between different stages of the pipeline. Review the use situations for file exchanges [here](../pipeline/#project-components).  - In simple terms, a file exchange \"mirrors\" (parts of) your generated files in `/gen`, so that you or your team members can download the outputs of previous pipeline stages.  {{% example %}} **Use cases for file exchanges**  - A file exchange to **collaborate with others**:   - A co-author builds a prototype model on his laptop.   He/she can work on a dataset that you have prepped using a high-performance workstation (this part of the pipeline), without having to actually build the dataset him/herself from scratch.  - A file exchange **without any collaboration**:   - You have built a data set on a high-performance workstation,   and would like to work with a sample dataset on your laptop, without   having to actually build the dataset from scratch. {{% /example %}}  - For hosting, you have the same options as for storing your raw data:     - a secure network folder in your organization,     - file hosting services such as Amazon S3, or     - public cloud services such as Dropbox or Google Drive.  - To upload or download data from your file exchange, put scripts in the `src` folder of the relevant pipeline stage:     - have a script in `src/data-preparation` that uploads the output of this pipeline stage from `gen/data-preparation/output` to the file exchange (`put_output`)     - have a script in `src/analysis` which downloads the data from the file exchange to     `gen/analysis/input` (`get_input`)  - For details on setting up and using a file exchange, follow our [Building Block here](/setup/file-exchanges).   - Notes and other documents are NOT part of your `my_project` directory, but are kept on a conveniently accessible cloud provider. The conditions are: files are accessible for all team members, and files are automatically backed up. Services that meet these requirements typically are Dropbox, Google Drive, or - if you're located in The Netherlands - Surfdrive.  - Do create subdirectories for each type of files (e.g., notes, literature, etc.)    {{% summary %}}  - Below, we reproduce the resulting directory structure for your entire project, called `my_project` (of course, feel free to relabel that project when you implement this!). - This example project has three pipeline stages:   - the pipeline stage `data-preparation` is used to prepare/clean a data set   - the pipeline stage `analysis` is used to analyze the data cleaned in the previous step, and   - the pipeline stage `paper` produces tables and figures for the final paper.  ``` Contents of folder my_project ============================= \u251c\u2500\u2500\u2500data \u2502   \u251c\u2500\u2500\u2500company_B \u2502   \u2502       coding.csv \u2502   \u2502       readme.txt \u2502   \u2502        \u2502   \u251c\u2500\u2500\u2500data_provider_C \u2502   \u2502   \u251c\u2500\u2500\u25002019-11-04 \u2502   \u2502   \u2502       coding.csv \u2502   \u2502   \u2502       readme.txt \u2502   \u2502   \u2502        \u2502   \u2502   \u2514\u2500\u2500\u25002020-03-01 \u2502   \u2502           coding.csv \u2502   \u2502           readme.txt \u2502   \u2502            \u2502   \u2514\u2500\u2500\u2500website_A \u2502           file1.csv \u2502           file2.csv \u2502           readme.txt \u2502            \u251c\u2500\u2500\u2500gen \u2502   \u251c\u2500\u2500\u2500analysis \u2502   \u2502   \u251c\u2500\u2500\u2500audit \u2502   \u2502   \u2502       audit.txt \u2502   \u2502   \u2502        \u2502   \u2502   \u251c\u2500\u2500\u2500input \u2502   \u2502   \u2502       cleaned_data.csv \u2502   \u2502   \u2502        \u2502   \u2502   \u251c\u2500\u2500\u2500output \u2502   \u2502   \u2502       model_results.RData \u2502   \u2502   \u2502        \u2502   \u2502   \u2514\u2500\u2500\u2500temp \u2502   \u2502           imported_data.csv \u2502   \u2502            \u2502   \u251c\u2500\u2500\u2500data-preparation \u2502   \u2502   \u251c\u2500\u2500\u2500audit \u2502   \u2502   \u2502       checks.txt \u2502   \u2502   \u2502        \u2502   \u2502   \u251c\u2500\u2500\u2500input \u2502   \u2502   \u2502       dataset1.csv \u2502   \u2502   \u2502       dataset2.csv \u2502   \u2502   \u2502        \u2502   \u2502   \u251c\u2500\u2500\u2500output \u2502   \u2502   \u2502       cleaned_data.csv \u2502   \u2502   \u2502        \u2502   \u2502   \u2514\u2500\u2500\u2500temp \u2502   \u2502           tmpfile1.csv \u2502   \u2502           tmpfile2.RData \u2502   \u2502            \u2502   \u2514\u2500\u2500\u2500paper \u2502       \u251c\u2500\u2500\u2500audit \u2502       \u2502       audit.txt \u2502       \u2502        \u2502       \u251c\u2500\u2500\u2500input \u2502       \u2502       model_results.RData \u2502       \u2502        \u2502       \u251c\u2500\u2500\u2500output \u2502       \u2502       figure1.png \u2502       \u2502       figure2.png \u2502       \u2502       tables.html \u2502       \u2502        \u2502       \u2514\u2500\u2500\u2500temp \u2502               table1.tex \u2502               table2.tex \u2502                \u2514\u2500\u2500\u2500src     \u251c\u2500\u2500\u2500analysis     \u2502       analyze.R     \u2502       get_input.txt     \u2502       put_output.txt     \u2502            \u251c\u2500\u2500\u2500data-preparation     \u2502       clean_data.R     \u2502       get_input.txt     \u2502       load_data.R     \u2502       put_output.txt     \u2502            \u2514\u2500\u2500\u2500paper             figures.R             get_input.txt             paper.tex             tables.R   ``` {{% /summary %}} {{% summary %}} **Description of the workflow**  1. **Raw data** is stored in `my_project/data`, logically     structured into data sources and backed up securely. 2. **Source code** is written in the `src` folder, with each step of     your pipeline getting its own subdirectory. 3. **Generated files** are written to the `gen` folder, again with subdirectories     for each step in your pipeline. Further, they have up to four subdirectories:     `/input` for input files, `/temp` for any temporary files, `/output` for any     output files, and `audit` for any auditing/checking files. 4. **Notes** are kept on an easily accessible cloud provider, like Dropbox, Google Drive, or - if you're located in The Netherlands - Surfdrive. {{% /summary %}}  {{% tip %}} - [Download our example directory structure here](../dir_structure.zip), so you can get started right away. - Remember that horrible [directory and file structure](../structure_phd_2013.html)? Check out how this tidier project on [how streaming services change music consumption](https://pubsonline.informs.org/doi/pdf/10.1287/mksc.2017.1051) looked [like](../structure_spotify_2018.html#spotify). - You've seen those readme.txt's?! These are super helpful to include to [describe your project](../documenting-code), and to [describe raw data](../documenting-data). {{% /tip %}}   {{% tip %}} **Data management for your project's components**  1. Raw data   - Store where: On a secure server that project members have access to   (e.g., could also be a network folder; later, we show to you how to use   a variety of systems like Amazon S3, Dropbox, or Google Drive to   \"host\" your raw data). No time to think about this much? Well, then   just have your data available locally on your PC, and set yourself a   reminder to move it to a more secure environment soon.   - Backup policy: Regular backups (e.g., weekly)   - Versioning (i.e., being able to roll back to prior versions): not necessary, although   you need to store different versions of the same data (e.g., a data set delivered in 2012, an updated dataset delivered in 2015) in   different folders. 2. Source code   - Store where: on Git/GitHub - which will allow you to collaborate efficiently on code   - Backup policy: Inherent (every change can be rolled back - good if you     want to roll back to previous versions of a model, for example)   - Versioning: complete versioning 3. Generated temp (temporary and output) files   - Store where: only on your local computer.   - Backup policy: None. These files are entirely produced on the basis of raw data and source code,   and hence, can always be \"reproduced\" from 1. and 2. if they get wiped. 4. Notes   - Store where: anywhere where it's convenient for you! Think about tools like   Dropbox or Google Drive, which also offer great features that you may   enjoy when you work in a team.   - Backup policy: Automatic backups (standard for these services)   - Versioning: not necessary (but typically offered for free for 1 month) {{% /tip %}} <!-- !!! summary \tEach submodule contains five subdirectories:  \t- `/code` for the code \t- `/temp` for any temp files that are generated on the fly \t- `/input` for any input files required to run your code (e.g., datasets) \t- `/output` for the final \"output\" of your module (e.g., dataset, analysis report) \t- `/audit` for some auditing files such as plots or text files, which will you allow to check whether the module was executed well.  --> "}, {"objectID": "./tutorials/project-management/principles-of-project-setup-and-workflow-management/automation.md", "title": "Automating your Pipeline", "description": "We can automate our pipeline using so-called build tools to execute sequential code files.", "keywords": "make, data, automation, build, recipe, pipeline", "code": [], "headers": ["Overview", "Translating the Pipeline into \"Recipes\" for `make`", "Running `make`", "Building your pipeline / running `make`", "Preview your pipeline, \"dry run\"", "Consider Source Code or Targets as Up-to-Date", "Advanced use cases", "Miscellaneous", "Why is `make` useful?", "Are there alternatives to `make`?", "Other build tools", "Readme.txt", "make.bat - A bash Script"], "content": "  Remember the [different stages of a project's pipeline](../pipeline/#project-pipelines)? Let's suppose we're in the process of preparing our data set for analysis. For example:  1. You wish to convert three raw data sets from Excel to CSV files. 2. You want to merge these three CSV files and apply some cleaning steps. 3. Finally, you want to save the final data set, so that it can be used in other stages of your project pipeline (e.g., such as the analysis).  This workflow for your specific pipeline can be visualized as follows:  ![Workflow](../make_flowchart.png)  Using so-called \"build tools\" such as [`make`](/building-blocks/configure-your-computer/automation-and-workflows/make/), we can specify:  - what code runs and when, and - what inputs (e.g., data, parameters) a given source code file needs to run.  Specifically, `make` introduces \"recipes\" that are used to tell `make` how to build certain `targets`, using a set of `source files` and `execution command(s)`.  - A *target* refers to **what** needs to be built (e.g., a file), - *source(s)* specify what is **required** to execute the build, and - the *execution command* specifies **how** to execute the build.  In `make` code, this becomes:  ```make target: source(s)     execution command ```  In \"`make` code,\" the workflow above - saved in a *makefile* (a file called `makefile`, without a file type ending) - becomes:  ```make  ../../gen/data-preparation/temp/cleaned_data1.csv ../../gen/data-preparation/temp/cleaned_data2.csv ../../gen/data-preparation/temp/cleaned_data3.csv: ../../data/dataset1/raw_data1.xlsx ../../data/dataset2/raw_data2.xlsx ../../data/dataset3/raw_data3.xlsx python to_csv.py    \tpython to_csv.py  ../../gen/data-preparation/temp/merged.csv: ../../gen/data-preparation/temp/cleaned_data1.csv ../../gen/data-preparation/temp/cleaned_data2.csv ../../gen/data-preparation/temp/cleaned_data3.csv merge.R    \tR --vanilla --args \"\" < \"merge.R\" ```  <!-- ../temp/analysis.RData: ../temp/merged.csv analyze.R    \tR --vanilla --args \"\" < \"analyze.R\"  ../temp/plot.png: ../temp/merged.csv plot.R    \tR --vanilla --args \"\" < \"plot.R\"  ../output/report.pdf: ../temp/plot.png ../temp/analysis.RData \tR -e \"rmarkdown::render('make_report.Rmd', output_file = '../output/report.pdf')\" -->   {{% tip %}} Pay attention to the subdirectory structure used here: the rules refer to files in different folders (src, gen, data, etc.), which are explained [earlier in this guide](../directories). {{% /tip %}}    You run the entire workflow by typing `make` in the directory where the `makefile` is located.   If you type `make -n`, you are entering a sort-of \"preview\" mode: `make` will provide you a list of commands it would execute - but it does not actually execute them. Great to preview how a workflow would be executed!   By default, `make` runs each step in a workflow that *needs* to be updated. However, sometimes you wish to only rebuild *some* but not all parts of your project. For example, consider the case where you have only added some comments to some R scripts, but re-running that part of the project would not change any of the resulting output files (e.g., let's say a dataset).  There are two ways in which you can \"skip\" the re-builds, depending on whether you want to consider **file(s)**, or **targets** as up-to-date. Recall that *targets* are higher-order recipes, whereas files are, well, merely files.  **Considering a *target* as up-to-date:**  Pass the parameter `-t targetname` to `make`, and press enter. For example, ``` make -t targetname ```  The `targetname` is now \"up-to-date\". When you then run `make`, it will only run those files necessary to build the remainder of the workflow.  **Considering *source code* as up-to-date:**  Pass the parameter `-o filename1 filename2` to `make`. In other words, `filename1` and `filename2` will be considered \"infinitely old\", and when rebuilding, that part of the project will not be executed.  {{% warning %}} Of course, using `-t` and `-o` should only be used for *prototyping* your code. When you're done editing (e.g., at the end of the day), make your temporary and output files, and re-run `make` to see whether everything works (and reproduces). {{% /warning %}}   This [book by O'Reilly Media](https://www.oreilly.com/openbook/make3/book/index.csp) explains all the bells and whistles about using `make`. Definitely recommended!    - You may have a script that takes a very long time to build a dataset (let's say a couple of hours), and another script that runs an analysis on it. You only would like to produce a new dataset if actually code to make that dataset has changed. Using `make`, your computer will figure out what code to execute to get you your final analysis.  - You may want to run a robustness check on a larger sample, using a virtual computer you have rented in the cloud. To run your analysis, you would have to spend hours of executing script after script to make sure the project runs the way you want. Using `make `, you can simply ship your entire code off to the cluster, change the sample size, and wait for the job to be done.    There are dozens of build tools in the market, many of which are open source. For more information, please check [Awesome Pipeline](https://github.com/pditommaso/awesome-pipeline) and [Awesome Workflow](https://github.com/meirwah/awesome-workflow-engines).  - [Dagster](https://github.com/dagster-io/dagster)/[Dask](https://github.com/dask/dask)/[Kedro](https://github.com/quantumblacklabs/kedro)/[Pachyderm](https://github.com/pachyderm/pachyderm)/[Reflow](https://github.com/grailbio/reflow): for data analysts  - [Airflow](https://github.com/apache/airflow) by Airbnb/[Azkaban](https://github.com/azkaban/azkaban) by Linkedin: for programmers - [Cmake](https://cmake.org/)/[Scons](https://scons.org/): general make tools - [Bazel](https://bazel.build): Google's next generation build system   Don't have time to set up a reproducible workflow using `make`? A `readme.txt` - or, in other words, a plain text file with some documentation - is great alternative. They are very useful to provide an overview about what the project is about, and many researchers also use them to explain in which order to run scripts. But then again, you would have to execute that code manually.   What you see with other researchers is that they put the running instructions into a bash script, for example a `.bat` file on Windows. Such a file is helpful because it makes the order of execution *explicit*. The downside is that such a file will always build *everything* - even those targets that are already up-to-date. Especially in data- and computation-intensive projects, though, you would exactly want to avoid that to make quick progress.  To sum up, we prefer `make` over a `readme.txt` or a `make.bat`. But better have one of those than no documentation at all.  {{% summary %}} **What is `make`, and how can we use it to automate pipelines?**  With `make`, we:  - explicitly document the workflow, making communication with colleagues (and especially our future selves) more efficient,  - can reproduce the entire workflow with one command,  - keep track of complicated file dependencies, and  - are kept from *repeating* typos or mistakes - if we stick to using `make` everytime we want to run our project, then we *must* correct each mistake before we can continue. {{% /summary %}} "}, {"objectID": "./tutorials/project-management/principles-of-project-setup-and-workflow-management/documenting-data.md", "title": "Documenting Datasets", "description": "If your project contains data that has been newly created, you are required to include a documentation of that data in your project.", "keywords": "document, data, dataset, derived, readme, describe", "code": [], "headers": ["Documenting Datasets"], "content": "  If your project contains data that has been newly created (i.e., which is not otherwise (publicly) available yet; including *derived* data sets), you are required to include a documentation of that data in your project.  Instances of \"new data\" may included, but are not restricted to be:  * data scraped from websites * data gathered via APIs * manually labeled data \t* e.g., to assign GDP per capita to a list of countries \t* e.g., to classify a music label as a major versus independent label \t* ... * data *derived* from secondary data (e.g., a cleaned data set; making explicit how you cleaned the data is important for future use of that data)  {{% tip %}}  Think of \"new data\" as *any* data that feeds into one of the pipeline stages in your project; it really needs not to be \"big\" data, but can simply consist of a `.csv` file with names and associated labels (e.g., as in the case of countries --> GDP per capita).  {{% /tip %}}  Check out [our building block for documenting data](/building-blocks/store-and-document-your-data/document-data/documenting-new-data/)! "}, {"objectID": "./tutorials/project-management/software-setup/index.md", "title": "Software Setup Overview", "description": "Here is a guide to help you get started setting up the computing environment on your machine ready.", "keywords": "software, setup, guide, configure, configuration", "code": [], "headers": ["Configure your computer"], "content": " These pages should help you to get started setting up the computing environment on your machine ready.  Think your machine is already configured well? Then [directly proceed to our tutorial on setting up efficient workflows to manage data- and computation-intensive projects](/tutorials/reproducible-research/implement-an-efficient-and-reproducible-workflow/implement-an-efficient-and-reproducible-workflow-overview/). If necessary, you can come back to this section to make changes to your computer's configuration.  Of course, there are many ways to set up a machine, and we do not mean to consider our way of doing to be perfect. In fact, the setup instructions sometimes resemble a compromise, so that later instructions - given to both Windows, Mac and Linux users - can be followed as closely as possible.  **If you are writing your Master thesis at Tilburg University, please attempt to install necessary software and packages prior to your first meeting.**  {{% summary %}} * If everything goes smoothly, you should be able to complete the installation in one sitting within 60-120 minutes. * Please follow the steps one-by-one in the order they appear on the side bar and do not deviate from them, unless you really know what you are doing. * Where necessary, we have provided instructions for Mac, Windows and Linux machines. {{% /summary %}}  {{% warning %}} * We will use the terms command prompt (Windows) and terminal (Linux, Mac) interchangeably. {{% /warning %}} <!-- \"Installation Help\" --> <!--     Please try and install all the software before the course begins. --> <!--     If you are struggling we are able to help - but we expect you have tried to work through the guide yourself. --> <!--     Details of the Installation help session are found below: -->  <!--     * When: Friday, August 25th, 9.30am - 12.30am --> <!--     * Where: SOF-E-09 --> "}, {"objectID": "./tutorials/project-management/scrum-for-researchers/index.md", "title": "Scrum for Researchers", "description": "Discover Scrum to improve your project management when working on empirical research projects", "keywords": "scrum, project, sprint, team, management", "code": [], "headers": ["How we learned to use Scrum", "Scrum in a nutshell", "Team members have different roles", "Meeting types and planning", "Why Scrum is useful and how to make it a success", "See also"], "content": "  We use [Scrum](https://www.scrum.org) - a simple framework for effective team collaboration - to manage the development of Tilburg Science Hub. By sharing our experiences with (an admittedly customized version of) Scrum, we hope to inspire and possibly help other teams to make their collaborations run smoothly.  Importantly, Scrum works for any type of project, including __academic research projects__. Keep on reading to find out how to use Scrum as a framework for collective research!    Scrum defines three main roles for members of the team: the product owner, the Scrum master and development team members.  - The **product owner** is accountable for maximizing the value of the product and for defining a clear \"task list\" (called product backlog), including:     - Developing and explicitly communicating the Product Goal;     - Creating and clearly communicating product backlog items;     - Ordering product backlog items; and,     - Ensuring that the product backlog is transparent, visible and understood.  - The **Scrum master** is accountable for the team\u2019s effectiveness by coaching and helping the team members to focus, removing obstacles for the team and ensuring that tasks are completed in a positive, productive and timely manner.  - The **development team members** are responsible for completing the tasks in the Sprint.   - During separately scheduled __strategic discussions__, the product owners (in collaboration with the team members) decide upon the needs for further development or improvement of the product. These needs form the *product backlog* and are broken down in smaller items/tasks that can be completed in a reasonable amount of time (for example a few hours). Items can be allocated to a specific team member.  - During our team\u2019s __sprint planning meeting__, which kicks off each new sprint, the product owners inspect the work from the backlog that\u2019s most valuable to be done next and move these items to the sprint backlog. We also review tasks that have been completed in the previous sprint.  - __During the sprint__ (in our case a two-week period), team members complete the tasks selected by the product owners from the product backlog.  - In addition to regular sprint meetings, every week we meet for a brief __\"WOW meeting\"__ (max. 15 minutes) to discuss the past week\u2019s successes: basically, anything we have achieved and are very proud of sharing with the team. This helps us stay connected and positive!  {{% tip %}}  To get started with using Scrum... - determine the roles of team members, - pick a sprint duration (e.g., two weeks), - plan a strategic discussion so that you converge on the backlog items, - schedule your first sprint planning meeting, - schedule the \"weekly wows\", and - reserve time in your agenda to work on tasks assigned to you.  To keep track of the product and sprint backlog and the items/tasks that\u2019s being worked on, you can use **[Trello](https://trello.com)** or the project tab on your project's [GitHub repository](https://github.com). Start by creating these \"tabs\"/columns now: - backlog, - sprint (to do), - sprint (in progress), - sprint (done), and - notes to keep (in which to pin important links so you don't lose them).  {{% /tip %}}   {{% example %}} **Using Scrum for a research project**  Think about the following situation. There are multiple co-authors who want to use publicly available data. They will analyze them with R. They have just come together to discuss how to proceed.  The goal is to develop a workflow using [`make`](/building-blocks/configure-your-computer/automation-and-workflows/make/). The workflow consists of downloading the data, putting them in the right place, preparing them for analysis, producing summary statistics, and finally producing a PDF with those summary statistics.  The **starting point of the first meeting is creating a backlog** that consists of bite-size items that take (max.) a few hours to complete.  So far, the team members have come up with the following items: - Create directory structure, - Create new Git repository, - Write first raw version of code that downloads the data and saves them in the right place, - Write first raw version of code that prepares the data for analysis; - Write first simple analysis file that produces summary statistics and writes them into a LaTeX file; - Set up a new tex file that produces a document with summary statistics table; - Set up makefile so that make can be used to run the project.  Next, the team members have **the first sprint planning meeting** and discuss which items they want to **move to the sprint**. They are optimistic and move all of them to the next sprint. Importantly, particularly in times in which we have a lot of long online meetings, this is all they have to do, so the meeting can end.  During the sprint, **they all know what things need to be done**, so they can flexibly discuss with one another who does what. One can use Trello or the project board on GitHub to **keep track** of this. Each entry above is a little card (think of it as a post-it note). Whenever someone starts working on something, the card is moved to the \u201cin progress\u201d section or column. And once something is finished, it ends up in the \u201cdone\u201d column.  When people work very intensely on projects, they sometimes start the day with a brief meeting (a \"stand-up\" meeting) where the Scrum master manages the board and everyone coordinates who does what. In less intense times, one can instead have a weekly \"WOW\" meeting like we do.  In any case, after two weeks, the sprint ends with a review and after that, it\u2019s time to look ahead. The **backlog has some new items** that the product owner has added, and the discussion is then again about which items should end up in the next sprint. {{% /example %}}   We find Scrum helpful because it provides us with structure. That in turn leads to commitment and motivation. After all, it\u2019s really satisfying to see many cards in the \u201cdone\u201d column.  But Scrum is not the full answer. For instance, one needs to find a way to determine new items for the product backlog. This should not be done in the sprint planning meeting, because then one ends up having another long meeting every two weeks. Communication and discipline in the meetings are important.  In short, Scrum can be seen as a structured way of working with meetings that are shorter and more productive, and cooperating in a flexible way in-between meetings.   - Learn more about Scrum [here](https://www.scrum.org) - Read a more comprehensive [Scrum Guide](https://www.scrumguides.org/scrum-guide.html) - Learn about how other researchers are using Scrum [here](http://crosstalk.cell.com/blog/scrum-for-science-a-framework-for-collective-research) - [Good book on project management (in Dutch)](https://gripboek.nl) "}, {"objectID": "./tutorials/reproducible-research/practicing-pipeline-automation-make/overview.md", "title": "Overview", "description": "Follow our tutorial to implement a fully automated workflow to conduct sentiment analysis on tweets.", "keywords": "tutorial, tweets, sentiment, analysis, workflow", "code": [], "headers": ["Objectives of this tutorial", "Prerequisites", "Disclaimer"], "content": " Longing to put your knowledge from our [workflow guide](/tutorials/project-management/principles-of-project-setup-and-workflow-management/overview/) into practice? Then follow this tutorial to implement a fully automated workflow to conduct sentiment analysis on tweets, using our [GitHub workflow template](https://github.com/hannesdatta/textmining-workflow).   -\tFamiliarize yourself with a [robust directory structure](/tutorials/project-management/principles-of-project-setup-and-workflow-management/directories/) for data-intensive projects -\tExperience the benefits of [automating workflows with makefiles/GNU make](/tutorials/project-management/principles-of-project-setup-and-workflow-management/automation/) -\tLearn to use Git templates for your own research projects -\tAdjust the workflow template to     -\t...download different datasets from the web     - ...unzip data automatically     -\t...parse JSON objects and select relevant attributes     - ...add new text mining metrics to the final data set using Python's `textblob`     - ...modify the analysis in an RMarkdown/html document   -\tComputer setup following our [setup instructions](/building-blocks/configure-your-computer/).     - [Python](/building-blocks/configure-your-computer/statistics-and-computation/python/) and the `textblob` package          ```         pip install -U textblob         ```          Then, open Python (`python`) and type          ```         import nltk         nltk.download('punkt')         ```          If you receive an error message, please verify you are typing this command in python (opened on the terminal by typing `python`), and not *directly* in the terminal/Anaconda prompt.      -\t[R, RStudio](/building-blocks/configure-your-computer/statistics-and-computation/r/) and the following packages:          ```         install.packages(c(\"data.table\", \"knitr\", \"Rcpp\", \"ggplot2\", \"rmarkdown\"))         ```          When installing the packages, R may ask you to select a \"CRAN-Mirror\". This is the location of the package repository from which R seeks to download the packages. Either pick `0-Cloud`, or manually choose any of the location nearest to your current geographical location.  {{% warning %}}   **R 4.0**.   Newer versions of R (>=R 4.0) may require you to download additional packages.    ```   install.packages(c(\"rlang\", \"pillar\"))   ```    - If you're being asked whether to build these packages from source or not [options: yes/no], select NO.    - If you're being asked to install RTools, please do follow these installation instructions. {{% /warning %}}    -\t[GNU Make](/building-blocks/configure-your-computer/automation-and-workflows/make/)  - Familiarity with our [workflows](/tutorials/project-management/principles-of-project-setup-and-workflow-management/overview/), in particular on [pipelines and project components](/tutorials/project-management/principles-of-project-setup-and-workflow-management/pipeline/), [directory structure](/tutorials/project-management/principles-of-project-setup-and-workflow-management/directories/) and [pipeline automation](/tutorials/project-management/principles-of-project-setup-and-workflow-management/automation/).  -\tNice-to-haves:     - Basic experience with Python and R     -\tFamiliarity with common data operations using `data.table` in R     -\tFamiliarity with text mining using Python and TextBlob     - If you want to learn Git on the way...         - Have Git installed on your computer (see here)     \t  - Have GitHub login credentials   To keep this tutorial as accessible as possible, it will mention Git/GitHub a few times, but assume you will acquire details on these skills elsewhere. In other words, versioning and contributing to Git repositories is not part of this tutorial.  <!-- to do:  add note what to put in make, and what to put in R--> "}, {"objectID": "./tutorials/reproducible-research/start-new-project/index.md", "title": "Start a new project", "description": "Start working on a new project, using the principles of workflow management and reproducible science.", "keywords": "new project", "code": [], "headers": ["1. Assess project requirements", "2. Set up computing environment", "3. Setup the repository", "4. Setup the data environment", "5. Automate your pipeline", "Next steps"], "content": " When working on a new project, it's efficient to kick-start your work with a well-documented \"repository template\" that sets up your workflow, and provides you with some minimal documentation so that team members can quickly contribute to your work.  In this tutorial, we walk you through the steps of setting up a project on your computer. Throughout, you'll use one of our project templates on GitHub.   Before we dive right into the nitty gritty details, here are a couple of things to consider when you set up new projects.  * Will you collaborate with others on writing code?    If you do, using a version control system like Git is a must-have, as you need to be able to work on a project simultaneously, without running the risk of overwriting each other's work.  * What's the technical proficiency of your team members?    It's not uncommon that your team members may be unfamiliar with tools like Git, make, or the terminal. First, that's not a problem at all. You can stay in charge of the main workflow, and integrate others' work as it is being updated (e.g., on Dropbox). However, it's way better to help team members develop the skills to use Git and automation, just to name a few.  <!--This workflow assumes some experience with    * *Technical proficiency*     - Are you co-workers familiar with tools like Git, make, and the terminal? In other words, can they independently set-up their machine and install required dependencies? And what high-level programming languages do they know (Python / R)?     integrate those somehow -->  * What minimum security levels do you have to ensure? Can you make your code public?    Are you working on a data consultancy project for a Fortune 500 client? Have you signed a NDA for the data that requires you to treat it with a great sense of responsibility? If so, then you better make sure that you have configured your systems securely (e.g., private Github repositories, 2-factor authentication, etc.).  * How will you manage your data?    Can the raw data be managed in a cloud storage service like Dropbox or Google Drive, or does the sheer amount of data requires us to look for alternatives (e.g., database or object storage)?  * How long will it take to run the workflow?    While importing a dataset and running a bunch of regression models typically happens with a matter of seconds, you may encounter circumstances in which you need to factor in the run time. For example, if you throttle API calls, experiment with a variety of hyperparameters, run a process repeatedly (e.g., web scraping). In these cases, the hardware of your machine may not suffice nor do you want to keep your machine running all day long. Creating a virtual instance (e.g., EC2) adjusted to your specific needs can overcome these hurdles.  <!--    * *Public Availability*     - If you advocate for open science and strive for reproducibility, open sourcing your data and code online is almost a given. This in turn means you need to put in the extra effort to write comprehensive documentation and running instructions so that others - who may lack some prior knowledge - can still make sense of your repository.  --> Together, these considerations can guide your decision making in terms of (a) code versioning, (b) raw data storage, and (c) computation (local vs remote).   Configure your software environment. The minimum requirements typically are - programming languages (e.g., Python, R) - version control system (e.g., Git/GitHub) - automation tools (e.g., make).  Head over to the [software section ](../../../building-blocks/configure-your-computer) section Tilburg Science Hub to view the installation guides.   Never worked with Git/GitHub before? Then follow our [onboarding for Git/GitHub first](../../../building-blocks/collaborate-and-share-your-work/use-github/versioning-using-git).  1. Initialize a new Github repository and clone it to your local machine. Based on your project requirements, consider whether you need a public or private repository.  2. Create a directory structure that suits your project's goals. For example this one.  ```txt \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u251c\u2500\u2500 gen \u2502   \u251c\u2500\u2500 analysis |   \u251c\u2500\u2500 data-preparation |   \u2514\u2500\u2500 paper \u2514\u2500\u2500 src     \u251c\u2500\u2500 analysis     \u251c\u2500\u2500 data-preparation     \u2514\u2500\u2500 paper ```  3. Update the README.md file (e.g., add a project description and title) 4. Commit your changes and push them to Github. 5. Create a gitignore file (`touch .gitignore`) that skips the `data` and `gen` folder (after all, your code in the `src` folder should download the data and store the intermediate results in `gen`). 6. Visit your repository on Github and check whether it works as expected. Please keep in mind that only folders that contain files will be visible (i.e., empty folders are ignored by Git). Once you start working on your project and create new files these folders will automatically become visible. 7. Invite your team members to contribute to the repository!   1. For small files and projects with limited duration, put your data file in Google Drive or Dropbox and generate a public sharing link. For more long-term projects, it's better to use object storage (e.g., such as the one at S3).  2. Create a script in the `src` folder that downloads the data from your cloud storage (or external website) and stores it in the `data` folder.  - Create a makefile that handles the end-to-end process (e.g., download data, preprocess data, estimate linear model, generate regression table and plot). - Start automating your project early on - even if it's just downloading the data and producing a little overview of the data. Expand your `makefile` while you're working on the project.    Think you're done? No way! This is just the start of your reproducible research project. So take some time to go through our suggestions on how to continue your work on the project.  - Inspect raw data after data delivery - Prepare data for analysis - Pull in changes from GitHub (and push your own changes to the remote) - Create issues, and assign team members to these issues - Work on your repository's readme - the *first* thing users of your repository will view when visiting it on GitHub  <!--   - [Document your data sets]   - [Document your source code](https://tilburgsciencehub.com/tutorials/project-setup/principles-of-project-setup-and-workflow-management/documenting-code/) --> "}, {"objectID": "./tutorials/reproducible-research/share-data/index.md", "title": "Data Sharing", "description": "Learn how to document and share data for internal and external use.", "keywords": "data sharing", "code": [], "headers": ["Overview", "What's a data workflow?", "How to get started in 6 steps", "Step 1: Start a new GitHub repository", "Step 2: Upload your raw data to a secure location", "Step 3: Clone your repository", "Step 4: Download your data and store authentication credentials", "Step 5: Work on your data and documentation", "Step 6: Share your data and workflow", "Advanced use cases", "Create", "Share"], "content": "  When conducting your empirical research, you typically spend a lot of time cleaning and documenting your data so that team members can use it.  Several open science tools can help you manage the process of cleaning, documenting, and - importantly - maintaining your dataset more efficiently. For this, we require a \"data workflow.\"   A data workflow consists of a code repository (e.g., on GitHub), your raw data (e.g., on a server), and a place to share your data with team members (e.g., via public clouds like Dropbox, or data repositories such as Zenodo or Dataverse).  We adhere to the following principles:  - Your code used for data cleaning and documentation is maintained with a versioning tool. We use GitHub. - Your raw data is always stored *outside* of your repository, ideally on an external server. We assume your raw data is confidential. It will never be shared. - The workflow creates \"public\" versions of your dataset. Let's call them your \"releases.\" Of course, the data does not need to be shared immediately with the public. You can use the workflow to share your data with your team members as a first step.    Don't start from scratch, but use our data workflow template, which you can find here:  Just click on \"use template\"!  - https://github.com/hannesdatta/data-spotify-releases-2015-2018 https://github.com/hannesdatta/data-spotify-playlist-ecosystem  You can make the repository publicly available without worry - your raw data will be on a secure server, and others won't be able to run the workflow without it at this moment.  <!--@ BBlocK: develop a \"stripped\" version of this workflow with dummy data for release -->   Raw data ideally is stored on a server that is frequently backed-up. For small-scale projects, it may be enough to keep the data on your institutional network drives or even on your computer's hard disk.  The most important aspect is that you need to be able to *automatically retrieve your data* from that location later. Don't put it on a USB stick or on a laptop that you can lose during your commute!  <!-- BBlocK: add bblock on storing possibilities -->   Now let's move the repository from GitHub to your local computer by cloning it.  ``` git clone https://github.com/your-username/your-repository-name ```  Navigate to the directory structure - do you recognize the components we've talked about earlier?   Did you notice that the raw data folder is empty? It's supposed to be like this! Recall that raw data should never be versioned. To still work with the data, you need to write a little code snippet to *download* the data from its secure location to your local repository. <!-- add building blocks -->  Probably you need to use some passwords to download the data. *Never* store them in your source code (after all, it will become public on GitHub).  <!-- add building blocks-->   You're set up to document and maintain your data set!  - Write code to clean your data, and make sensible decisions about what will remain \"secure and private,\" what part of the data you can disclose to coauthors, and what part of the data can be used publicly. - You can also automate your workflow using `makefiles`. - Document your data using our handy templates. You find them [here](https://tilburgsciencehub.com/document/new-data).  Write your final output data - the one you or others will be using for your project - to the `release` folder.   Write a little code snippet that pushes your final data set to your team members, for example, to Google Drive, Dropbox, or S3.  Whenever you are making updates to the data or documentation, you can \"re-push\"/upload the data and notify your coauthors about the update.  It's good practice to supply coauthors with a little code snippet to download the most recent data set. Ideally, you store that snippet in the (separate) GitHub repository of your empirical research project. As long as your team members' repository is up-to-date, they will always use the most recent version of your data.   - Launch a website that tells potential users of your data about it. Check out [this excellent page](https://nijianmo.github.io/amazon/index.html) by Julian McAuley.  - Push your code, documentation, and final data set to Dataverse or Zenodo for long-term storage. Both offer DOI's, so your work becomes citable!  - Engage with your community of data users on GitHub (e.g., via issues or using the discussion board). It's good practice to maintain an FAQ, so you don't need to answer questions over and over again.  Above all, don't perceive the process as a burden. Instead, enjoy the experience of seeing how the *quality of your data* becomes better with each iteration of this process.   <!--  - Create GitHub repository from template - Store raw data on secure server (institution) - Create data prep code to create derived version - Create initial documentation from template - Prototype workflow by running it with `make`   - Publish internally   - Store on Dropbox   - Store on S3, make available to coauthors (code snippet)  - Publish externally   - Dataverse     1. Create empty data verse     2. Get Dataverse API credential     3. Run push.sh or push.bat in repository to push data to server "}, {"objectID": "./tutorials/open-education/hugo-website/get-hugo.md", "title": "Get Hugo", "description": "Learn how to install Hugo, a famous static site generator.", "keywords": "hugo, static, website, generator, class, academic", "code": [], "headers": ["Install Hugo", "MacOS & Linux", "Windows", "Create a New Local Website"], "content": "  First off, make sure to have [Git](/get/git) properly installed and set up.   Make sure you have [Brew](/building-blocks/configure-your-computer/automation-and-workflows/commandline/#mac-users) installed. Then, to install Hugo:  ``` brew install hugo ```   If you use [Chocolatey](https://chocolatey.org):  ``` choco install hugo -confirm ```  Alternatively, see other installing options [here](https://gohugo.io/getting-started/installing).   You're now ready to start.  To create a new Hugo website, move to a directory of your choice and run: ``` hugo new site yourwebsitename ``` Where `yourwebsitename` is the name of the folder you've just created. Your entire website will live inside this folder on your computer.  Congrats! In the next sections, you will start tweaking your new website.  {{% tip %}} Check out the Hugo documentation for the official [quick start guide](https://gohugo.io/getting-started/quick-start/). {{% /tip %}} "}, {"objectID": "./tutorials/open-education/hugo-website/go-live.md", "title": "Go Live!", "description": "Learn how to publish your static website with GitHub and Netlify.", "keywords": "hugo, netlify, domain, dns, github", "code": [" ```toml [build]   command = \"hugo --gc --minify -b $URL\"   publish = \"public\"  [build.environment]   HUGO_VERSION = \"0.81.0\"   HUGO_ENABLEGITINFO = \"true\"  [context.production.environment]   HUGO_ENV = \"production\"  [context.deploy-preview]   command = \"hugo --gc --minify --buildFuture -b $DEPLOY_PRIME_URL\"  [context.branch-deploy]   command = \"hugo --gc --minify -b $DEPLOY_PRIME_URL\" ``` "], "headers": ["How to Host Your Website", "The Last Piece of the Puzzle", "See Also"], "content": "  Because your website is static, it can be hosted virtually anywhere using any web server.  However, we highly suggest you to deploy your website using **[Netlify](https://www.netlify.com)**.  Netlify can host your website for free, it's very easy to use since it's linked to your GitHub profile, and it provides some great perks, like a global CDN, free SSL certificates and continuous deployment.   1. First, if you haven't done it already, create a new GitHub repository for your website. If you don't know how, you can read about it [here](/learn/versioning).  2. Add and commit a new file to your repo, called `netlify.toml`. This file is a sort of recipe for Netlify on how to build your website. Inside the file, write the following:    3. Create a new Netlify account. Go to [app.netlify.com](https://app.netlify.com/) and **sign up using GitHub**. Select \"Authorize application\" when prompted.  4. Now create a \"**New site from Git**\".  ![New site from Git](https://d33wubrfki0l68.cloudfront.net/1a92de85be074abc024967fa7088c8b719c32466/f7496/images/hosting-and-deployment/hosting-on-netlify/netlify-add-new-site.jpg)  5. Follow the required steps. You will need to select and authorize GitHub again (this time with added permissions to your repos).  6. Select your new website repository. You can then change a few setup options.     - Keep publishing from the `master` branch     - The build command is: `hugo` (or the specified command in the `netlify.toml`)     - The default publish directory is `public`  7. Deploy!  ![Deploy with Netlify](https://d33wubrfki0l68.cloudfront.net/a9f55d92792a554cb775cd0d10eddf445338b83a/0a424/images/hosting-and-deployment/hosting-on-netlify/netlify-deploying-site.gif)  8. Once it's done, you should see a successful message and an URL for your website (which has been automatically generated for you). You can change the URL in \"Settings\". You can now **visit your live website!**  ![Netlify successful message](https://d33wubrfki0l68.cloudfront.net/e2ea775b0985b93f2e0d7c88ae134e90c3e7446e/8a3d7/images/hosting-and-deployment/hosting-on-netlify/netlify-deploy-published.jpg)  {{% warning %}} **You've made it!**  You created a website with continuous deployment. What does that mean?  It means that Netlify is now actively \"watching\" your GitHub repository.  **Every time you commit to your repository, the website will be automatically rebuilt and redeployed!** You no longer need to deal with Netlify. From now on, you can edit your website locally, and when you're happy about the edits, you can **simply commit**. Pretty neat, isn't it? {{% /warning %}}   - Learn how to [add a custom domain](https://docs.netlify.com/domains-https/custom-domains/) to your Netlify-hosted website - Learn how to [set up HTTPS on custom domains](https://docs.netlify.com/domains-https/https-ssl/) - Check out [other hosting and deployment options](https://gohugo.io/hosting-and-deployment/) - Read a more [detailed guide](https://gohugo.io/hosting-and-deployment/hosting-on-netlify/) on how to host your Hugo website on Netlify "}, {"objectID": "./tutorials/open-education/hugo-website/overview.md", "title": "Overview", "description": "Learn how you can launch your very own static website in a quick and easy way.", "keywords": "hugo, static, website, generator, class, academic", "code": [], "headers": ["Why to Launch a Website?", "The Basics", "First off, what is a static website?", "Enter the World of Static Site Generators", "Hugo"], "content": "  Whether you want to spread the word about your research work or to share content with your students in open education classes, getting your very own website to do so would help you immensely.  A few ideas of what you could do with a personal website:  - Showcase your own academic profile and research work in a professional way - Disclose data and share your results with the world - Run your open education classes, publishing the syllabus, lectures, exercises... - Maintain a collection of articles or blog posts - Virtually anything else!  However, getting a new website up and running is often perceived as a daunting and expensive task, especially if you're not really an IT guy. No worries! In this tutorial, you will learn how to launch a static website within minutes, **free of charge**, and without writing a single line of code.    A **static website** serves exactly the same content to all users (with just a few exceptions). All the web pages delivered to the users are also exactly as stored - think of Wikipedia.  By contrast, **dynamic websites** show different content depending on the user accessing them. For instance, your Facebook newsfeed would probably look a bit different from Barack Obama's one.  As you may have guessed, in this guide we will deal only with static websites because they are easier and cheaper to set up and maintain, they're faster, safer and work great for our purposes.   Static Site Generators (SSGs) combine content with templates to generate the pages of a website. For instance, you can write your content in Markdown and then pick a template to choose how the content will be displayed.  In other words, SSGs are not too rigid on how the frontend of your site is built: once you write the content, you're free to change \"how it looks\" as many times as you want. It's up to the SSG to generate the HTML, CSS and JS assets accordingly.  There are [many SSGs out there](https://jamstack.org/generators/) in just about any programming language. We encourage you to spend some time researching which one works best for you if you're serious about this.  {{% tip %}} Don't overestimate the importance of which programming language the SSG is based on. You don't need to know that language, unless you require some custom modifications. {{% /tip %}}   For the purposes of this tutorial, we choose **[Hugo](https://gohugo.io)**, a very popular SSG written in Go, which is *very* fast. You will learn how to use it in the next sections. "}, {"objectID": "./tutorials/open-education/hugo-website/pick-theme.md", "title": "Pick a Theme", "description": "Learn how to install a Hugo theme to make your website glossy.", "keywords": "hugo, hugo-tiu, tilburg, static, website", "code": [" ```toml echo theme = \"hugo-tiu\" >> config.toml ``` ```yaml echo theme: hugo-tiu >> config.yaml ``` "], "headers": ["Install a Theme", "Get Hugo-TiU", "How to install Hugo-TiU"], "content": " It's time you pick a theme for your website. This will determine how the content will look to the users. Head over to **[themes.gohugo.io](https://themes.gohugo.io)** and choose one!  - We suggest [Academia Hugo](https://themes.gohugo.io/academia-hugo/) as a simple theme **for personal websites**, where you can showcase your portfolio, resume, and research work.  ![Academia Theme](https://d33wubrfki0l68.cloudfront.net/f1ccb690b1e8ffff4a3680cfa84f08424da25a69/f122a/academia-hugo/screenshot-academia-hugo_hub5e9edd9f1f6717e4545a90cfadcc4ac_738415_750x500_fill_catmullrom_top_2.png)  - The [Wowchemy Academic Theme](https://sourcethemes.com/academic) **for personal websites too**.  ![Wowchemy Academic](../img/wowchemy.png)  - The [Hugo Dot Theme](https://themes.gohugo.io/dot-hugo-documentation-theme/) **for knowledge bases and documentation websites**, for instance a thesis companion:  ![Hugo Dot](../img/thesis-theme.png)  - Our brand-new **[Hugo-TiU](#get-hugo-tiu)** theme for open education classes. See below!   Each theme will have specific installation instructions. However, generally speaking, these are the required steps to set up a theme for your Hugo website:  1. You'll need to place the theme folder inside `yourwebsitename/themes`, usually by cloning the theme repository there. 2. You'll need to specify which theme you wish to use in the `config.toml`, `config.yaml`, or `config.json` file that is in the project's root directory.  {{% warning %}} This second step may seem unnecessary, but it is there because Hugo allows you to \"install\" more than one theme - which basically means that you can have more than a theme folder inside `yourwebsitename/themes`. This is very useful if you want to quickly swap a theme and see how your website would look with a new one.  As a consequence, you need to explicitly tell Hugo (in the configuration file) which theme to use when building the site. {{% /warning %}}   Meet `Hugo-TiU`, our new **Hugo open education theme for Tilburg University**, based on the original Hugo Book theme! The styling, fonts, and colors are inspired by the official Tilburg University website.  ![Hugo-TiU](https://github.com/tilburgsciencehub/hugo-tiu/raw/master/images/screenshot.png)  {{% tip %}} This theme is great for running open education classes. See, for instance, how Prof. Hannes Datta runs his [course on Online Data Collection and Management](https://odcm.hannesdatta.com) with `Hugo-TiU`. {{% /tip %}}  We may be biased, but we love our new theme and we encourage you to use it!  {{% cta-primary \"Learn more about Hugo-TiU\" \"https://github.com/tilburgsciencehub/hugo-tiu\"%}}   Installing `Hugo-TiU` is easy and similar to any other theme. However, being a custom-made theme, you won't find it on the official Hugo themes list at [themes.gohugo.io](https://themes.gohugo.io). Therefore, you can get it directly from its [GitHub repository](https://github.com/tilburgsciencehub/hugo-tiu):  1. Navigate to your Hugo project's root directory and run:  ``` git submodule add https://github.com/tilburgsciencehub/hugo-tiu themes/hugo-tiu ```  2. Set ```theme = \"hugo-tiu\"``` or ```theme: hugo-tiu``` in your configuration file (in `config.toml` or `config.yaml`, respectively), either manually or from the command line:    Alternatively, you can directly run Hugo and specify the theme as a flag: ``` hugo server --minify --theme hugo-tiu ```  That's it! In the next section, you will learn how to add some content to your website.  {{% summary %}} **A brief recap**  Here's a very short example on how to create a new website **from scratch**: ``` hugo new site mydocs; cd mydocs git init git submodule add https://github.com/tilburgsciencehub/hugo-tiu themes/hugo-tiu cp -R themes/hugo-tiu/exampleSite/content . ``` And then: ``` hugo server --minify --theme hugo-tiu ``` {{% /summary %}} "}, {"objectID": "./tutorials/open-education/hugo-website/add-content.md", "title": "Add Some Content", "description": "Learn how to customize your Hugo website and add content to it.", "keywords": "hugo, content, markdown, writing, articles", "code": [], "headers": ["The `content` Folder", "The Front Matter", "Preview Your Edits", "Build the Website"], "content": " Now that you've set up a theme for your Hugo website, you can finally start to write some content!  Just **manually create Markdown files** using a text editor and place them in the `content` folder. The path should look something like this: `content/<CATEGORY>/<FILE>.md`. Usually, you can create as many categories as you want, but it's better to check the theme documentation.  {{% example %}} For instance, the [About page of odcm.hannesdatta.com](https://odcm.hannesdatta.com/docs/about/) is placed inside `content/docs/about/`. {{% /example %}}   You must always **provide some metadata** inside your content files for them to work properly, like a title and a date. For instance, a new file could start with something similar to this: ``` --- title: \"My First Post\" date: 2019-03-26T08:47:11+01:00 draft: true ---  Your content goes here. You can write anything you want using the Markdown syntax. ```  What's within the `---` is called **front matter**. You can learn more about predefined and used-defined front matter variables [here](https://gohugo.io/content-management/front-matter/).  {{% tip %}} Notice the variable `draft: true`. When the site gets built, articles set to drafts won't be deployed. Set draft to false if you want to include them in your public website. {{% /tip %}}   You know how to edit your content now. Wouldn't it be great if you could preview how your articles will look like once the website gets published?  Luckily, Hugo allows you to do exactly so with a \"local server\". Just run:  ``` hugo server ```  And you will get something like:  ``` | EN +------------------+----+ Pages            | 10 Paginator pages  |  0 Non-page files   |  0 Static files     |  3 Processed images |  0 Aliases          |  1 Sitemaps         |  1 Cleaned          |  0  Total in 11 ms Watching for changes in /Users/bep/yourwebsitename/{content,data,layouts,static,themes} Watching for config changes in /Users/bep/yourwebsitename/config.toml Environment: \"development\" Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop ```  Now, simply **navigate to your site in your browser at [http://localhost:1313/](http://localhost:1313/)**.  If you edit or create new files, you should see them updating live. If they don't, try force refresh your browser (cmd/ctrl + R).   Once you're happy about your edits, it's time to actually build the website. It's *very* simple in Hugo. Simply run:  ``` hugo ```  Hugo will now create all your HTML, CSS and JS assets needed for your website. The output will be in the `/public/` directory.  Congratulations, **you've built your own website!**  Now, if you're run your own web server, just copy the `/public/` directory to it to deploy your website.  If you don't, in the next section you will learn how to publish your new website for free and in an easy way, using GitHub and Netlify.  {{% tip %}} Don't forget to tweak your configuration file before building and releasing your website to the public. In particular, you may want to update your base URL and website title accordingly. ``` baseURL = \"https://example.org/\" languageCode = \"en-us\" title = \"My New Hugo Site\" theme = \"hugo-tiu\" ``` {{% /tip %}} "}, {"objectID": "./tutorials/open-education/online-data-collection-and-management/index.md", "title": "Open Education Course on Data Collection", "description": "Learn to use web scraping and APIs to collect data for your empirical research project.", "keywords": "odcm, scraping, api, collection", "code": [], "headers": ["Overview", "Get access"], "content": "  The **[Online Data Collection and Management](https://odcm.hannesdatta.com)** course is an open-source Master level course taught at Tilburg University. All of its content is freely available and consists of lectures, live streams, self-study material, tutorials and examples.  The course teaches you the nuts and bolts about collecting data from the web. Unlike most other courses on this topic, this one not only teaches you the technicalities of using web scraping and APIs, but also introduces a comprehensive framework that helps you to think about scraping - specifically with regard to its application in academic marketing research.   {{% cta-primary-center \"Get started now\" \"https://odcm.hannesdatta.com\" %}} "}, {"objectID": "./tutorials/open-education/data-preparation-and-workflow-management/index.md", "title": "Open Education Course on Data Preparation", "description": "Engineer data sets from complex raw data and manage research projects efficiently.", "keywords": "dprep, preparation, raw data, cleaning, wrangling", "code": [], "headers": ["Overview", "Get access"], "content": "  The **[Data Preparation and Workflow Management](https://dprep.hannesdatta.com)** course is an open-source Master level course taught at Tilburg University. All of its content is freely available and consists of lectures, live streams, self-study material, tutorials and examples.  This course teaches you how to engineer data sets for statistical analysis. Many students and researchers perceive the process of \u201ccreating\u201d a data set for analysis as rather simplistic: a bit of cleaning here, a bit of merging there, and you\u2019re done. In this course, we take data preparation to the next level, by considering highly complex data preparation workflows (think multiple sources, structured and unstructured data, data from databases and data from files, multiple delivery batches, lots of missing data, different file versions, etc.).  And of course, throughout the course, we\u2019ll be using the workflow principles of reproducible science that we advocate at Tilburg Science Hub.   {{% cta-primary-center \"Get started now\" \"https://dprep.hannesdatta.com\" %}} "}, {"objectID": "./tutorials/open-education/educational-videos/educational-videos-overview.md", "title": "Overview", "description": "An educational video is meant for transferring knowledge to an audience. Learn how to make them.", "keywords": "educational, videos, shooting, camera, teaching, tips, lighting, lenses, microphone, audio", "code": [], "headers": ["What is an educational video?"], "content": " An educational video is meant for transferring knowledge to an audience. These often involve animations or presenting a slide deck. However, in most of the cases, you might want to show yourself - or the presenter - talking to the camera.  This guide will focus on the process of making these videos, not the content, and it's aimed at teachers or contributors of Tilburg Science Hub. In other words, we will talk about the technical preparation and equipment needed to record such videos, not on how to create animations or compelling presentations.  This first part deals with equipment. The second part of this guide covers the shooting workflow for production and post-production stages.  {{% cta-primary \"First part: Equipment\" \"../equipment-edu-videos\"%}}  {{% cta-primary \"Second part: Shooting\" \"../shooting-edu-videos\" %}} "}, {"objectID": "./tutorials/open-education/educational-videos/equipment-edu-videos.md", "title": "Buying Equipment for Educational Videos", "description": "Learn what equipment is need to get you started producing educational videos.", "keywords": "educational, videos, shooting, camera, teaching, tips, lighting, lenses, microphone, audio", "code": [], "headers": ["Option 1 - Home Setup with a Webcam", "Option 1.A", "Option 1.B", "Option 1.C", "Option 2 - Advanced Home Setup", "Lighting Gear", "Artificial Lights", "Audio Gear", "USB Microphones", "Option 3 - Studio-like Setup", "Camera Gear", "Cameras", "Lenses", "Tripods", "Lighting Gear", "Light Stands", "Light modifiers", "One Last Note", "Audio Gear", "Camera Microphones", "Lavalier Microphones", "XLR Condenser Microphones", "Optional Gear", "Putting All Together"], "content": " # Part 1  We're going to present three different setup options ordered by increasing complexity. Keep in mind that more complex setups require a higher budget, more physical space, and more time to put in place. The higher setup time is one-off. Actually, in all three options it's very quick and easy to start rolling once everything has been correctly configured for the first time.  The need of expensive gear to produce high quality images is a myth. A lot can be achieved just with proper **lighting**.  ![Hard lighting v. soft lighting.](../img/hard-soft-l.jpg)  Knowing the difference between hard and soft lighting will dramatically improve your image.  - **Hard light** casts sharp, strong, dense (i.e., dark), hard-edged shadows. This is typical of **small light sources**. You want to **avoid** hard lighting, because it's difficult to manage, it creates ugly shadows under the chin and around your nose, and unless you are a top model, imperfections in your skin will stand out. - **Soft light** makes soft shadows, with no defined edge, no shape, and no density. This is typical of **large light sources**. Soft lighting is **desirable**, because it's easier to control and will generally make you look better.  {{% warning %}} One fundamental concept to understand is that **the larger the light source *relative* to the subject, the softer the light** (and thus the shadows) will be. {{% /warning %}}  Notice the \"*relative*\"? It's not the absolute size that matters, but the [apparent size](https://en.wikipedia.org/wiki/Angular_diameter): how the size of the light source is perceived from the subject's perspective. The sun is a very big light source, but being very far from us, it acts like a point light source, casting hard shadows (on a sunny day with a clear and blue sky). A cloudy day is soft light, because the clouds **diffuse** the sun light, creating a larger light source.  What does that imply for you? Put simply, you should avoid (buying) a small light source as your key light (i.e., the one that will light up the subject's face)! Mini LED panels (like the [Aputure MC series](https://www.aputure.com/products/mc/)) won't work, unless you place them very close to the subject's face and frame your scene very tight. However, these mini panels are extremely useful for creative purposes in more complex lighting setups (lighting your background with a splash of color, for instance).  {{% warning %}} For the sake of simplicity, we assume that you will be the presenter (the subject) in your video. {{% /warning %}}   Luckily, one of the best large light sources is free and available to everybody - the sun! You can achieve a very nice, soft lighting by staying in front of a large window - unless the sun is low and comes straight into your window, obviously.  {{% tip %}} Always have the window **in front of you**, not behind, and place **your webcam** between yourself and the window, front-facing. Make sure your webcam is at **eye-level** so that you can make eye contact with the attendees.  If the light is too harsh, a white translucent curtain might help to diffuse it.  If your laptop doesn't come with a built-in camera, the [Logitech C920](https://www.logitech.com/en-us/products/webcams/c920-pro-hd-webcam.960-000764.html) is a good and reliable external webcam. {{% /tip %}}  One aspect that is often underrated is **audio**. This is especially true for teachers, who will often show slides while talking and may record at home, in non-treated rooms for acoustics or close to noise sources. If you have a limited budget and need to choose how to invest your money, we'd highly suggest spending it in a good microphone instead of a new camera.  {{% tip %}} If your laptop built-in mic is too noisy or poor quality, you can use a headset with a microphone, or your phone's earbuds (which often have a mic for phone calls too). {{% /tip %}}  A cheap yet effective way to improve your image quality is to use **your smartphone instead of a webcam**. Modern smartphones usually have better cameras than most built-in laptop webcams.  Just use a pile of books to place your smartphone on your desk and at eye-level. Make sure to turn your smartphone in **landscape orientation**. Alternatively, you can use a [Gorillapod](https://joby.com/global/gorillapod/) and attach your smartphone to whatever surface you want. Connect your smartphone to the computer via USB and use one of the many apps available to turn your smartphone into a webcam. We suggest [Camo for iPhone and iPad](https://reincubate.com/camo/) and [iV Cam for Android](https://play.google.com/store/apps/details?id=com.e2esoft.ivcam).  Alternatively, you could record yourself directly with your phone and import later the footage to your PC for editing. Apps like [Filmic Pro](https://www.filmicpro.com/products/filmic-pro/) unlock all your smartphone's camera potential and advanced features like flat or LOG gamma curves. In this case, you could either connect your microphone to the smartphone or record the audio separately from the video, connecting the microphone to your PC and using [Audacity](https://www.audacityteam.org) or similar software. We explain how to sync an audio track to the footage in the second part of this guide.   Another option is to record your webcam's stream and audio simultaneously via OBS. **[OBS](https://obsproject.com)** is a free and open source software for live streaming that also allows video (and audio) recording. Besides recording your webcam, you may also use it to easily record your screen.   Maybe you can't rely on the sun, or you don't have access to a large window, or maybe you want to record after sunset. If you are looking for a **consistent** video quality and you want to appear more professional (without a headset on your head), you will need:  - One or more continuous light sources - An external USB microphone    If you are not going to use the sunlight as your primary light source, you will need artificial light.  You should not use your desk lamp to light up your face on video. That's because not only \"normal\" bulbs are not powerful enough, they will also mess up the colors in your video.  What does that mean? It's important to take into account the **color temperature** of illumination. You should avoid too warm or too cold lights, and you should definitely avoid mixing both of them in the same shot. But this is not the end of the story.  When we said that the sun is one of the best light sources available, we meant it! That is because **daylight is considered full-spectrum light**, meaning that it covers the entire visible spectrum, from infrared to almost ultraviolet. Therefore, daylight (the midday sun with a blue sky, with a color temperature measured at 5600K) is considered a benchmark in photography and videography. Good quality lighting equipment should be full-spectrum and daylight-balanced, meaning that it emulates the quality of natural light and it's tuned to work at 5600K. Nowadays, some LED lights are also getting closer to full-spectrum, but these can get quite expensive. Using full-spectur lights ensures that you get no color cast in your image.  ![Light spectrum comparison.](../img/l-spectrum.jpg)  For your main light (or key light), the one that will be lighting your face, you should try to get a source as close as possible to daylight. Try to avoid incandescent light (usually way too warm, like a candle light). **Avoid at all cost fluorescent light**. Most video lights can be mainly HMI (hydrargyrum medium-arc iodide), tungsten or LED. You will be most likely to **use LED lights** since they are cheaper and widely available today.  We use so-called CRI and TCLI ratings to compare the \"quality\" of light sources: just remember that the higher these numbers are, the better. Ideally, they should be at least above 90.  As mentioned previously, you want a large light source for softer shadows. There are two ways you can get one. You can either buy a large light source, or a small light source and some sort of diffusion for it.  - A large light source is always large and soft light. You cannot make it smaller. It's also probably big and heavy. - A small light source, besides being lighter, can make both hard light, if used standalone, or soft light, if used with a diffuser (also called light modifier). We talk more about these in Option 3.  The [Elgato Key Light](https://www.elgato.com/en/gaming/key-light) is a relatively big LED panel and by far one of the easiest options available to set up. It can be attached to any desk and controlled via a mobile app or PC. It works great for fixed desk setups and if it's relatively close to the subject anyway. However, it has some downsides too: it's not very powerful and you cannot use it in any other configuration (e.g., standing, away from the desk).  Rollable LED panels like the [Falcon Eyes RX-24TDX](https://www.falconeyeshk.com/product-page/rx-24tdx?currency=EUR) are a valid, more versatile, lighter yet more expensive alternative. These can offer a large soft light especially if coupled with a softbox (like the [Falcon Eyes RX-24SB+HC](https://www.falconeyeshk.com/product-page/rx-24sb-hc?currency=EUR)). Softboxes for this kind of LED panels are way thinner than the traditional ones, so this is a great solution for small rooms.  Ring lights (like those from [Neewer](https://neewer.com/collections/ring-lights-1)) are quickly becoming popular among streamers. The light created is between that of a point light source and a larger, diffused one, giving the image a wrapped, contrasted and dramatic look. Due to their peculiar shape, they make a unique circular [catchlight](https://en.wikipedia.org/wiki/Catch_light) in the subject's eyes. These lights are best suited for beauty close-ups and are not very versatile: for best results the subject must stay close to it and should not move around too much.   There are many [different kinds of microphones](https://training.npr.org/2016/06/28/which-mic-should-i-use/) and connectors.   For this kind of setup, the easiest solution is to get a **USB microphone** (like the [Rode NT-USB](http://www.rode.com/microphones/nt-usb), the [Rode NT-USB Mini](http://www.rode.com/microphones/nt-usb_mini) or the [Blue Yeti](https://www.bluemic.com/en-us/products/yeti/)) and plug it into your PC. These microphones work great for casual recordings, however they are not very versatile and often offer inferior audio quality compared to XLR competition.   To take your setup to the next level and achieve a professional look (and sound), you will need:  - A camera and a good (i.e., fast) lens (on a tripod) - Professional lighting equipment (and some way to hold them) - An XLR microphone and an audio interface    Most of the current **still image cameras are capable of recording videos too**. The main difference with a proper movie or video camera is how well the latter can handle heat (actually, they can do much more, but it's not so relevant for our purpose). Cameras designed mainly to take still images will have their sensor to overheat after a while, when used to record videos. That's why most of these cameras will have **a time limit** (often 20 or 30 mins) for their video recordings. No harm in most of the cases, and you will simply need to continue recording with a new take.  However, using still cameras as video cameras has some advantages too. Good-quality video cameras with **interchangeable lenses** and comparable sensor size are way more expensive than their still camera counterparts.  But why would you need interchangeable lenses? Most of **the \"premium look\" that you are looking for comes from the lens, not the camera!** More specifically, you will need **a fast lens** (i.e., a lens with **a wide aperture**) if you like **a shallow depth of field or the [bokeh effect](https://en.wikipedia.org/wiki/Bokeh)**: you will be in sharp focus while **the background can be blurred**, out of focus. This is visually very pleasing and can help focusing the viewer's attention on the subject speaking.  If you had to choose, our advice is to **spend more on a good lens rather than on a better camera**. It's okay to buy relatively old or used cameras, since most of the latest models only marginally improved some aspects that are not so relevant for us, like low-light performance. If you learn how to correctly light your scene, you will never need to shoot in low-light conditions. Not to mention that a good lens lasts forever, while cameras can be changed or upgraded after some years.  Most cameras can be split among two categories: **DSLR and mirrorless cameras**. - A Digital Single-Lens Reflex has a mirror in it. Light passes through the lens, and then it reaches a viewfinder through some mirrors and/or prisms. When you shoot, the mirror flips up so that the light can reach the sensor and no longer the viewfinder. - A mirrorless camera has no mirror in it. That's the key difference. The light constantly hits the sensor, which is always-on, and you would frame your image through an electronic viewfinder (basically, a screen). Because of this, these cameras are usually smaller and lighter, although they are less energy-efficient.  It's irrelevant what kind of camera you choose. They **both work** for our purpose. Just make sure that it can shoot full HD videos at 25fps or more, that it has a manual mode, a built-in microphone and/or a 3,5mm microphone jack input.   Most lenses can be split between **zoom lenses and prime lenses**. - A zoom lens can vary its focal length. This means that you can change the field of view, tightening or widening the scene. - A prime lens has a fixed focal length. You can't change the field of view unless you actually move the camera closer or further away from the subject (i.e., \"zoom with your feet\") - although this has [a slightly different visual effect](https://www.slrlounge.com/how-to-change-perspective/) than varying the focal length.  They can both work, however, **we'd suggest buying prime lenses**. That's because they are easier to manufacture, and thus cheaper. Usually, you can find some fast prime lenses (i.e., prime lenses with a wide maximum aperture that allow for a more pleasing blurred background) at very affordable prices (like the [Canon 50mm f/1.8 STM](https://www.amazon.com/Canon-50mm-1-8-STM-Lens/dp/B00X8MRBCW/) or [Nikon 35mm f/1.8G](https://www.amazon.com/Nikon-AF-S-NIKKOR-Focus-Cameras/dp/B001S2PPT0)). In contrast, any zoom lens with a comparable wide aperture would cost much more.  **The focal length is measured in millimeters**. The higher the number, the narrower will be the field of view. For instance, a 85mm lens looks much more \"zoomed in\", narrower, than a 25mm one.  **The lens aperture is measured with the [f-number](https://en.wikipedia.org/wiki/F-number)**. The smaller the f-number, the wider the lens aperture (very counter-intuitive, we know). For instance, a 50mm f/1.8 lens has a much wider maximum aperture than a 50mm f/5.6 one, which implies a shallower depth of field (nicer blurred backgrounds) but also a better light-gathering capability. Imagine light passing through a hole: if the hole is larger, more light will pass through it. We won't stress this aspect too much, because you will probably shoot your videos with artificial lighting and not in a dark room anyway.  **So, which lens should you buy?** It really depends on what you plan to shoot with it. You should take into consideration how wide you want your scene to be, how many people you will be shooting at the same time, how much of the background you will be including in your shot, and how big is the space available to you (how far you can place the camera from the subject).  Typical focal lengths for portraiture range from 35-50mm (called normal lenses, those that reproduce a field of view close to the human eye) to 200mm (often incorrectly labeled as telephoto lenses).  Just remember this: **shorter focal lengths with the camera closer to the subject will show more of the background compared to longer focal lengths with the camera placed further away**.  However, to make things even more complicated, depth of field depends not only on the lens aperture, but also on the focal length! Longer focal lengths allow for a shallower depth of field. In the next image, notice how the cactus stays relatively the same while the background changes dramatically.  ![Focal lengths and moving the camera away.](../img/zoom-or-move.jpeg)  As regards the lens aperture, go with the lens with the lowest f-number, if possible. Ideally, you'd want a wide-aperture lens (we also call it a fast lens). If you're buying Canon or Nikon, I'd suggest their budget line of \"normal lenses\": the [Canon 50mm f/1.8 STM](https://www.amazon.com/Canon-50mm-1-8-STM-Lens/dp/B00X8MRBCW/) or the [Nikon 35mm f/1.8G](https://www.amazon.com/Nikon-AF-S-NIKKOR-Focus-Cameras/dp/B001S2PPT0).   Once you get your camera and lens, you will need some sort of support for them. The easiest solution is to buy **a tripod**. Any decently sturdy tripod will do the job. Their connection is universal, so they will work with any camera you buy.   As we said, there are two ways you can get soft light. We will now focus on the second option: using a smaller light source coupled with a light modifier.  **Smaller light sources are more versatile but require some extra gear**: a sturdier light stand and a diffuser mostly.  A relatively cheap option is the [Godox SL-60W](http://www.godox.com/EN/Products_Continuous_SL60_Video_Light.html) or the [Neewer SL-60W LED](https://neewer.com/collections/background-support-equipment). More advanced, high-quality lights are the [Aputure LS C120D II](https://www.aputure.com/products/ls-c120d-ii/), the [Lupo Superpanel series](https://www.lupo.it/en/video/superpanel-full-color-60), or the glorious, industry-standard [Arri Skypanel series](https://www.arri.com/en/lighting/led/skypanel).  When choosing your light, just make sure of the following: - That **it's not a flash or strobo light**. These may be confused with continuous light sources because they look similar, but they are actually meant for studio photography, not video. - It's **fanless**. You want your light to be silent, especially if it's close to your microphone. - It has **a Bowens mount**. This is an almost-universal mount for accessories and diffusers. Often, you can't mount diffusers on LED panels so you won't need this mount.   If you have enough space and you can place your lights at an angle from the subject's perspective, you can get any cheap light stand from Amazon. They will do the job, unless your light and modifier weigh a lot.  If you don't have enough space to place a light stand close to the subject, but you still want the light to be close to them - or maybe right above the camera - you will need a special light stand with a boom arm. These are called [C-stands](https://en.wikipedia.org/wiki/C-stand) and they are capable of supporting much heavier lights too. Through their arm, you can place the light away from their legs. Some of these stands may have wheels too.   There are [a lot](https://digital-photography-school.com/a-beginners-guide-to-light-modifiers/) of light modifiers out there. The underlying idea is to **use them to shape the light according to your needs**. The followings are some modifiers mainly used in photography - however, some of them can be used for video too. In cinema, much more expensive and elaborate solutions are involved.  ![Light modifiers.](../img/modifiers.jpg)  As we said, you want the make the light bigger, so you will need any kind of diffuser to make it appear bigger. The most common ones for this purpose are umbrellas, softboxes, and scrims.  The easiest one to use and control is any kind of **softbox** (a regular square softbox, a stipbox, an octabox, a parabolic softbox...), like those from [Godox](http://www.godox.com/EN/Products_Studio_Accessories_Softbox.html).  Most modifiers directly attach to your light source, so when choosing one, make sure it **has the same mount of your light source** (hopefully a Bowens mount).  Some modifiers are away from the source, like flags and scrims which often are placed on their own stands. They are common in cinema and usually work better in bigger spaces as they have a larger footprint. You can see how a scrim is used to diffuse a point light source in the next picture.  ![A scrim in use.](../img/scrim.jpg)   You could easily spend a lot of money into your lighting equipment and still get poor results. Learning the basics of light and how to shape it would help you tremendously to get closer to the look you have in mind.  But that's not all. Forget recreating the light setup of studio productions. **Studios are big, and your room is probably small**. This may sound trivial, but it's actually very important because of [light falloff](https://petapixel.com/2016/06/02/primer-inverse-square-law-light/).  Put simply, if your room is too small, **the light coming from your source might bounce on the walls** and affect dramatically your exposure in ways that you may not expect.  However, knowing this, you could **use your walls to your advantage as actual light modifiers!** Assuming that they are white, you could purposely point your source to the wall in front of you or to the ceiling to soften the light as if it was coming from a very large diffuser. Of course, you will lose any directional control over it and you will get a flat (and boring) lighting - but still better than hard lighting with some ugly shadows on your face.    An alternative to USB microphones are those specifically built to work with cameras, like the [Rode VideoMic](http://www.rode.com/microphones/videomic). These have a **3,5mm jack** output that can be inserted directly into the camera. This means that the audio is burnt into the footage, not on a separate track on your PC: quicker (you won't need to sync the tracks), but dangerous too. It's often a bit more difficult to monitor your audio in this way, and you run the risk of [clipping](https://en.wikipedia.org/wiki/Clipping_(audio)) your audio without even noticing.   Lavaliers - like the [Rode smartLav Plus](http://www.rode.com/microphones/smartlav-plus) - are great **if you are moving around**, because they are always on you. They can be positioned very close to your mouth to minimize background noise, but they are usually more expensive compared to shotguns of similar quality. Their frequency response may not match the best XLR mics and they can also easily pick up clothes rustling.   To step up your game, you will need a microphone with a **XLR connection**. Because you will not be moving while talking, we suggest a **[shotgun mic](http://www.learningaboutelectronics.com/Articles/What-is-a-shotgun-microphone)**, like the [Rode NTG3](http://www.rode.com/microphones/ntg-3) or the [Sennheiser MKH416](https://en-us.sennheiser.com/short-shotgun-tube-microphone-camera-films-mkh-416-p48u3) (Rode NTG1 and NTG2 are also cheaper yet valid options). You should point it directly towards your face and relatively close to you. It will reject any sound coming from the sides or back. This ensures that you will be recording only the sound of your voice and no background noises.  But how would you connect this mic to your PC? You'll need an external **audio interface**, with at least one XLR input and phantom power (condenser microphones require external power for their internal electronics, unless they run on battery too). If you will be recording only one microphone at a time, we suggest the [Scarlett Solo by Focusrite](https://focusrite.com/en/usb-audio-interface/scarlett/scarlett-solo).  This is by far **the best-sounding, most versatile and future-proof solution** (you can later buy a portable digital audio recorder for outdoor scenarios and still use the same microphone, for instance).  If you want to learn more about which microphone works best for you, [Beginner Guitar HQ](https://beginnerguitarhq.com/best-recording-microphone/) has a comprehensive guide on how to choose the right one.   A [teleprompter](https://en.wikipedia.org/wiki/Teleprompter) is a useful addition to this setup. By using a clear sheet of glass in front of your lens, it allows you to read the script while talking without the need to ever look away from the camera. Actually, it has some more use cases too: you could use it as a monitor and see yourself while recording, or your attendees in a live call while always looking them in the eyes. Take a look at [how this YouTuber configured](https://dslrvideoshooter.com/the-ultimate-video-desk/#teleprompter) his [GlideGear TMP-100](https://www.amazon.com/Glide-Gear-TMP100-Adjustable-Teleprompter/dp/B019AJOLEM). Notice that you will need a small external monitor. However, you could also use your iPad or tablet for this purpose. There are some great apps that will allow you to do so, like [PromptSmart](https://promptsmart.com). Bear also in mind that small teleprompters would work better with shorter focal lengths (wide angle lenses) because the camera can be placed closer to the subject speaking. Longer focal lengths require the camera to be placed further away, thus making it harder for the teacher to read from a teleprompter.   Setting up the camera, lights and microphone each time may take some time, but some people managed to attach all the equipment on a single stand or desk, allowing them to start recording quickly when needed. - [Full Studio on a Single Stand](https://www.youtube.com/watch?v=FilRCidYDcw) - [Full Studio on a Moving Stand](https://www.youtube.com/watch?v=rZANDiX18gk) - [Full Studio on a Single Desk](https://www.youtube.com/watch?v=WedG8LKO6ks)  Please notice that these solution will only work if you intend shooting one person at a time. For interviews, shooting more people and/or using multicam setups, you will need a bigger studio. Moreover, all the light sources should be placed further away. This implies that these lights must be more powerful ([inverse-square law](https://en.wikipedia.org/wiki/Inverse-square_law), if you're a geek). "}, {"objectID": "./tutorials/open-education/educational-videos/shooting-edu-videos.md", "title": "Shooting Educational Videos", "description": "Learn about the essential steps required to shoot and produce educational videos.", "keywords": "educational, videos, shooting, camera, teaching, tips, lighting, lenses, microphone, audio", "code": [], "headers": ["Step 1 - Set Up Your Scene", "Step 2 - Light Up Your Scene", "One Light Setup", "Two Light Setup", "Three Light Setup", "Four Light Setup", "Camera and Audio Settings", "Video", "Audio", "Recording", "Editing", "Alternative Way: Record Audio and Video Together"], "content": " # Part 2  In this second part of our guide you will learn about the essential steps required to shoot educational videos.   The first thing to do is to set up your scene, place your camera on a tripod, and compose the shot.  Here you face your first decision. You will need to **choose what to include in your frame**. This will dictate what focal length to use (i.e., what lens) as well as how far to place the camera. Ideally, you should **strike a balance between a boring shot and one that is too distracting**.  {{% tip %}} A close up shot of your face (also called a \"talking head\" shot) is ideal when stressing an important message. You remove any distraction from the background and the audience focuses solely on you and your speech. But long shots like this may become boring after a while. This is why we suggest to compose your shot a bit wider, showing more of the background. Indeed, the background should not be messy, too crowded or distracting. However, if you choose to shoot at a wider angle, you could always \"zoom in\" digitally on your face when needed, in post-production (i.e., cropping your footage).  We suggest you to **sit comfortably at your desk** and have some props on it to show and engage your audience with. It's also a good idea to avoid dark and plain backgrounds. **Show something relevant behind you**. For instance, if you're a data scientist, you could have a PC running behind you and some screens showing with what you usually work on.  When composing your frame, leave some \"room\" above your head, but not too much. {{% /tip %}}   In this section, we will assume that you're relying on artificial lighting.  How many light sources do you need? We did not address this question in the first part of the guide. That's because it really depends on your budget. You can achieve some good results with just two or three lights, and we will show some light schemes for the most common configurations.  **But why would you need more than one light?** That's because distinct light sources can have **different functions**.  - A **[key light](https://en.wikipedia.org/wiki/Key_light)** (or main light) is usually the most important one in the scene. It highlights the main subject in the frame from the front. It's usually placed at a 30\u201360\u00b0 angle (if the camera and the subject are marked at 0\u00b0) and it's the most intense one. If you have only one artificial light source, you will use it as a key light. If you don't include a key light in your shot, you get a silhouette effect. - A **[fill light](https://en.wikipedia.org/wiki/Fill_light)** is meant to \"fill in\" the shadows of the subject casted from the key light. It's usually less powerful than the key. If the two sources outputted the same intensity of light and were placed at the same distance from the subject, you would basically get two key lights, resulting in a flatter and more artificial-looking image. In fact, the point of having a fill light is to attenuate shadows, not remove them entirely. Often, having one side of a face brighter than the other one is very pleasing and closer to what naturally happens in the world. This intensity difference with the key light also helps in sculpting a more 3D look on the subject. - A **[rim light](https://en.wikipedia.org/wiki/Backlighting_(lighting_design))** helps to separate the subject from the background. This light is placed behind the subject, usually towards the camera (but not visible in the frame). It creates a glow around the edges of the subject and makes him/her stand out more. - A **[background light](https://en.wikipedia.org/wiki/Background_light)** is used to illuminate what's behind the subject. It's usually the one at the lowest intensity. It can also be set to different colors to create color-contrast in the scene and give a more modern and interesting look.  Lastly, let's talk about **[practical lights](https://en.wikipedia.org/wiki/Available_light)**. Practicals are sources that are clearly visible as models in your scene, like lamps and light fixtures, but also anything else that emits or reflects light, like the moon, televisions, vehicle headlights, buildings, and so on. As we said before, it's not a good idea to use those as your key light. However, they are great to **set the tone and give credibility to your scene and light scheme**!  **Your ultimate goal when placing your artificial lights is to create a natural-looking effect**. That's why photographers and cinematographers often talk about **motivated lighting**. They **seek a motivation on where to place the lights sources**. In other words, motivated lighting is used to **imitate natural (existing) light within a scene**, like lamps or windows, basically by enhancing and replicating practicals. For instance, if you have a window in your shot but the light coming from the outside is not enough, you could place your key light in a position that mimics the exterior light coming through the window. In this way, you will get a pleasant and natural-looking result because you are enhancing the pre-existing light.   If you only have one light source, use it as your key light to illuminate your subject's face. Place it at a 30\u201360\u00b0 angle from the subject and the camera, slightly above the head and pointing down towards it.  To reduce the contrast in your scene you might need a fill light. No worries if you don't have a second light source. You could use any reflective white surface to bounce off the key light from the other side of the subject and \"fill in\" those shadows. You could buy a [5-in-1 reflector](https://www.amazon.com/Neewer-43-inch-Collapsible-Multi-Disc-Reflector/dp/B002ZIMEMW) for this purpose, or just use a white foam board.  Don't forget about the background! Use practical lights from your house to make it more interesting and not leave it too dark.  ![One light setup](../img/l-setup-1.png)   This setup is very similar to the first one. Use your second light source as a fill light. Having a separate source instead of using a reflector enables you to control how much contrast you want between the bright side and the shadow side of the face, or the [lighting ratio](https://en.wikipedia.org/wiki/Lighting_ratio) between your key light and fill light. This allows you to achieve many different styles and moods.  ![Two light setup](../img/l-setup-2.png)   Add a third source as a background light. If you happen to have practicals in your background, remember to use motivated lighting when placing your artificial source. Generally speaking, the background should always be a bit darker than the subject. This helps the audience to concentrate on the speaker. Use practicals such as lamps, bulbs or similar to create a nice bokeh in the background if you use a shallow depth of field.  You can also get creative with your background light. Since it's not illuminating the subject, you could play with colors to set the mood in your scene. Use a RGB light source or a white one with some color gels on it. Usually, you will see some cold colors in the background, like blue or teal. This is done to increase the color contrast in the scene, which makes it visually more balanced and pleasing. Contrasting colors means using complementary colors, those from opposing segments of the color wheel. The foreground will be mostly warm and orange because of the subject's skin tone, so it's a good idea to have a colder background color.  ![Three light setup](../img/l-setup-3.png)   Lastly, you could add a fourth source as a rim light to separate the subject from the background. This is meant to light up the subject's hair. It's generally placed above the head, angled down and slightly forward to strike the top of the head and shoulders. Usually, it's on one side but it can also be placed directly behind the subject (you will need a C-stand with a boom arm for this, otherwise you'll see the light stand in your shot).  ![Four light setup](../img/l-setup-4.png)   For the sake of brevity, we won't explain a lot about the following camera settings. If you are already comfortable with setting your camera exposure, depth of field, focus and video options, skip this part and adjust to taste. If you are new to these topics, you can learn more about them [here](https://photographylife.com/what-is-exposure-triangle).  Anyway, the following are our recommended settings, but notice that they should be adapted case by case.   You will record footage directly on your camera storage option (probably an SD card), not to a PC. Make sure you have a fast SD card (class 10) to handle the large video files.  1. Set your camera in **manual (M) mode**. This allows you to manually control all the settings. 2. Enter the live view and/or **video mode** on your camera. 3. Set your video quality and settings: **full HD (1920 x 1080px) at 25fps**, use a flat or **neutral picture profile** and make sure the internal microphone is on and working properly (even if you're going to use an external microphone). Leave your white balance set to auto. If you see from the preview that your colors are off (there's an orange or blue color cast), set your white balance to 5500K (or match your light's color temperature). 4. After having turned on all of your lights, set the exposure on your camera. Use a **shutter speed of 1/50th** of a second (shutter speed = 1 / fps * 2). Set your aperture quite wide open (use a **small f-number**) for a shallow depth of field and an out-of-focus background. Lastly, **adjust the ISO** setting until your [exposure meter is set to 0](https://thelenslounge.com/exposure-indicator-how-to-control-exposure/) (the picture is correctly exposed). 5. Unless you own a modern, high-end expensive camera and a lens with ultrasonic quiet focusing, you need to **turn OFF the autofocus** on your camera. Entry-level autofocus system are not really meant for videos, especially on still (photographic) cameras. You might end up with unusable footage because the camera continuously refocused while shooting. Use manual focus instead (you will need a second person to focus for you while you're in the shot, if the camera is placed far away). Of course, this will only works if you will be standing still and won't move much while recording.  Notice that if you are going to use the same set and lighting options, you won't need to repeat all of these steps each time you will be recording. As long as you are not changing the lighting output or the distance between the sources and the subject, you won't need to set your exposure again from scratch.   You will record your audio directly into your PC. We recommend recording at **a sample rate of 48 kHz** and on a **WAV** file type.  1. Connect your microphone **to the first input** of your audio interface. 2. Turn on and connect your audio interface via USB to your PC. 3. If you're using a condenser microphone without batteries, turn on phantom power. 4. Launch a recording software on your PC. We suggest [Audacity](https://www.audacityteam.org) or any DAW like [Logic Pro X](https://www.apple.com/it/logic-pro/) or [Cubase](https://new.steinberg.net/cubase/). 5. Create a **mono audio track** with the first input from your audio interface. 6. Turn on phantom power for your microphone, if needed. Then, set the [appropriate gain level](https://www.sweetwater.com/insync/gain-staging/), without clipping. 7. Enable the track for recording (R). 8. **Disable the track monitoring**. You do not want to hear yourself talking while you are recording.   Finally, you can start recording. But before you start your presentation, there's one last essential step that you should take that will save you a lot of time in the editing process.  1. Start recording your audio from Audacity or your DAW of choice. 2. Press record on your camera. 3. **Clap your hands, loudly**. Make sure they are in-focus and clearly visible in your shot. This will act as a (cheap) clapperboard. 4. You can start your speech.   Once you are done with your presentation, you can stop both the recording.  You will now need some **editing software** to combine the audio and the footage you've just shot, as well as to cut down unnecessary bits, color correct your footage, add titles, logos, and export the final file. We suggest [Adobe Premiere Pro](https://www.adobe.com/products/premiere.html) on Windows and [Final Cut Pro](https://www.apple.com/it/final-cut-pro/) or [iMovie](https://www.apple.com/imovie/) (which is free) on MacOS. **[DaVinci Resolve](https://www.apple.com/it/final-cut-pro/)** is a free yet equally powerful and professional cross-platform alternative.  1. Start by importing your footage and audio (WAV) file. 2. Create a new project and timeline with the same settings as the original footage: 1920 x 1080px at 25fps. 3. Drag and drop your video and audio files on the timeline. 4. You will now need to **sync the video and the audio track**: remember, you did not start recording *exactly* at the same time. You should be seeing three tracks on your timeline: two from the video file (one for the footage and one for the audio recorded by the camera's built-in microphone) and one from your external microphone (WAV file). Most editing software show audio waveforms for audio tracks. You will need to move either the video or the audio track to **match the moment you clapped at the beginning**. You can either match the external microphone waveform to the one of the built-in microphone (if the clap was loud enough to be captured by the latter too), or visually to the actual footage. Most editing software allow for automatic syncing too: check [this article](https://filmstro.com/blog/how-to-sync-your-audio-to-your-video-in-premiere-pro) for Premiere Pro and [this one](https://support.apple.com/en-gb/guide/final-cut-pro/verc1fabc30/mac) for Final Cut Pro. 5. Link all of your tracks together, so that they will move all at once from now on and won't get out of sync. 6. Cut your footage according to your need. You can also add new \"levels\" on your timeline for titles, logos, and animations. 7. Render and export the final video file. Set the resolution to the same as the source video (1920 x 1080px) and the video codec to H.264.  That's all! You successfully managed to set up, record, and export your educational video.   If you don't want to spend time syncing your video and audio track in post-production, an alternative way is to record using **OBS**.  **[OBS](https://obsproject.com)** is a free and open source software for live streaming that also allows video (and audio) recording. Besides recording your screen, you may also record external video sources, like a webcam stream.  However, if you're looking for a better image quality, you may also **use your DSLR as a webcam**. In order to do so, you will need some extra hardware that converts your camera's HDMI output into a USB input, like the [Elgato Cam Link](https://www.elgato.com/en/gaming/cam-link-4k). "}, {"objectID": "./building-blocks/analyze-data/regressions/regression-analysis.md", "title": "Run a Regression Analysis", "description": "Build linear models to draw inferences from your datasets.", "keywords": "linear regression, model, lm, prediction, model evaluation, linear inferences", "code": [" ```R library(broom)  # estimate linear regression model # to estimate a logistic regression model use: # glm(formula = y ~ x, data = data, family = binomial) mdl <- lm(formula = y ~ x, data = data)  # check model assumptions autoplot(   mdl,   which = 1:3,   nrow = 1,   ncol = 3 )  # show regression coefficients summary(mdl) ``` ", " ```R library(dplyr) leverage_influence <- mdl %>%     augment() %>%     select(y, x, leverage = .hat, cooks_dist = .cooksd) %>%     arrange(desc(cooks_dist)) %>% ``` ", " ```R library(ggplot2)  ggplot(data = data, aes(x, y)) + geom_points() + geom_smooth(method = \"lm\", se = FALSE) ``` ", " ```R explanatory_data <- c(..., ..., ...)  prediction_data <- explanatory_data %>%   mutate(        y = predict(       mdl,       explanatory_data,       type = \"response\"     )   )  # See the result prediction_data ``` ", " ```R library(stargazer)  stargazer(mdl_1, mdl_2,           title = \"Figure 1\",           column.labels = c(\"Model 1\", \"Model 2\"),           type=\"html\",           out=\"output.html\"             ) ``` ", " ```txt clear all  * First,let's install the necesary package  ssc install estout, replace  * By typing the command below, you can open an example dataset provided by Stata :  sysuse auto  // Using esttab eststo clear * First regression :  eststo: regress price weight mpg    * Second regression  eststo: regress price weight mpg foreign  * Generating a table for the two regression results that we generated :  esttab  eststo clear   // Using outreg2 : Alternatively, you can export your results to an excel, pdf or doc file using outreg2 commandFir   clear all  * First,let's install the necesary package  ssc install outreg2, replace  sysuse auto  * First regression :  regress price weight mpg outreg2 using RegressionResults.doc, replace ctitle(Model 1)  * Second regression  eststo: regress price weight mpg foreign outreg2 using RegressionResults.doc, append ctitle(Model 2) ``` "], "headers": ["Overview", "Code", "Estimate Model", "Identify Outliers", "Plot Trend Line", "Make Predictions", "Export Model Output", "Exporting your findings in Stata"], "content": " In the social sciences, regresion analysis is a popular tool to estimate relationships between a dependent variable and one or more independent variables. It is a way to find trends in data, quantify the impact of input variables, and make predictions for unseen data.  In this building block, we illustrate how to estimate a model, identify outliers, plot a trend line, and make predictions.   Linear regression (`lm`) is suitable for a response variable that is numeric. For logical values (e.g., did a customer churn: yes/no), you need to estimate a logistic regression model (`glm`). The code sample below estimates a model, checks the model assumptions, and shows the regression coefficients.  * Model transformations can be incorporated into the formula, for example: `formula = log(y) ~ I(x^2)`. * The coefficients (`coefficients(mdl)`), predictions for the original data set (`fitted(mdl)`), and residuals (`residuals(mdl)`) can be directly derived from the model object. * A concrete example on how to evaluate model assumptions (mean residuals is 0, residuals are normally distributed, homskedascticiy) can be found [here]().    Compute the leverage of your data records and influence on `mdl` to identify potential outliers.    Plot a scatter plot of two numeric variables and add a linear trend line on top of it.     Given a linear regression model (`mdl`), make predictions for unseen input data (`explanatory_data`). Note that for multiple linear regression models, you need to pass an `explanatory_data` object with multiple columns.      You can export your model output using `stargazer`. This package will create a nicely-formatted regression table for you in a variety of formats. You can learn more about it [here](/export/tables).  Convert regression coefficients of `mdl_1` and `mdl_2` into a HTML file that can be copied into a paper.    Alternatively, you can do your regression analysis on Stata. First, you should clear your working directory. With the command \"sysuse auto\", we download an example data file provided by Stata itself.      {{% example %}} [This tutorial](https://dprep.hannesdatta.com/docs/building-blocks/regression-analysis/) outlines how to run, evaluate, and export regression model results for the `cars` dataset.  In particular, it analyzes the relationship between a car\u2019s speed and the stop distance.  ![A trend plot in R.](../images/trend_plots.png) {{% /example %}} "}, {"objectID": "./building-blocks/analyze-data/machine-learning/introduction-to-machine-learning.md", "title": "In-depth Introduction to Machine Learning and R", "description": "Get started with machine learning and R with 15+ hours of expert videos.", "keywords": "machine learning, classification, unsupervised, resampling, model, linear, tree, vector, r", "code": [], "headers": [], "content": " Get started with machine learning and R with more than 15 hours of videos at **[dataschool.io](https://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)**.  You will find 10 chapters ranging from linear regressions to unsupervised learning.  {{% cta-primary-center \"Go to dataschool.io\" \"https://www.dataschool.io/15-hours-of-expert-machine-learning-videos/\" %}} "}, {"objectID": "./building-blocks/automate-and-execute-your-work/automate-your-workflow/task-scheduling.md", "title": "Schedule recurring tasks (e.g., by day, hour)", "description": "Learn how to execute scripts at specified intervals with cronjob and Task Scheduler.", "keywords": "cron, cronjob, automation, task scheduler, task scheduling", "code": [" ```python \"<PATH OF YOUR PYTHON INSTALLATION>\" \"<PATH TO PYTHON FILE>\" pause ``` "], "headers": ["Overview", "cronjob (Mac / Linux)", "cron syntax", "Configuring cron jobs", "Task Scheduler (Windows)", "See Also"], "content": "  Task scheduling involves the automatic execution of scripts on your local computer. For example, you may want to run a web scraper on a daily basis without having to manually execute the script. Mac and Windows users can leverage cronjobs and the Task Scheduler respectively to automate repetitive tasks. Depending on your operating system, follow one of the guides below to learn how to configure task scheduling on your machine.  Most Mac and Linux distributions come with pre-installed cron by default. This is a program that you can only access through the terminal. Before we can schedule tasks, we first need to understand the cron syntax. After all, it uses a very specific way to define the task frequency.  The syntax consists of 5 symbols - each separated by a space - which denote the minute, hour, day, month, and weekday respectively. An asterisk symbol matches any value so `* * * * *` means every minute of every hour of every day. Note that weekdays can take on a value between 0 and 6 and start on Sunday (0 = Sunday, 1 = Monday, ... etc.)  ``` *    *    *    *    *   \u252c    \u252c    \u252c    \u252c    \u252c \u2502    \u2502    \u2502    \u2502    \u2514\u2500  Weekday  (0 - 6) \u2502    \u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500  Month    (1 - 12) \u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  Day      (1 - 31) \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  Hour     (0 - 23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  Minute   (0 - 59) ```  If needed, you can use commas to insert more than a single digit (e.g., `15,45 * * * *` = every 15th and 45th minute every hour), dashes to define a range (e.g., `0 0-5 * * *` = 0:00, 01:00, ..., 05:00 every day), and slashes to specify the relative frequency (e.g., `*/10 * * * *` = every 10 minutes).  {{% example %}} `30 * * * *` = every 30th minute of every hour every day     `30 5 * * *` = at 05:30 on every day   `30 5 1 * *` = at 05:30 the first day of the month    `30 5 1 1 *` = at 05:30 on January 1st   `30 5 * * 1` = at 05:30 on every Monday   `0 0 */3 * *` = every 3 days at midnight    `30 5 1,15 * *` = at 05:30 on every 1st and 15th day of the month      `*/30 9-17 * * 1-5` = every 30 minutes during business hours {{% /example %}}   1. Double-check whether your script works as expected and then save the Python script as a **`.py` file** (so not a `.ipynb` notebook!). For example, copy-paste the contents from Jupyter to Spyder. We recommend storing the script in your root directory (the parent directory of your Desktop and Documents among others) to avoid file permission issues.  2. Open the Terminal and type `crontab -e`. This opens the so-called vim editor which you can think of as a notepad within the terminal. 3. Press `I` so that you can edit the text. 4. Insert the following syntax (always specify the full path! - see the tip below on how to obtain the Python installation path): ``` # e.g., * * * * * /usr/bin/python3 /script.py <CRON CODE> <PATH OF YOUR PYTHON INSTALLATION> <PATH TO PYTHON FILE> ``` 5. Press Esc and type `:wq` followed by Enter to save your changes (if a window pops up choose OK).  6. If you now run `crontab -l` your newly scheduled task should be listed. We recommend keeping an eye out whether your scheduler works as expected, especially in the beginning. If not, you can make changes to the cron file at any time. To remove all existing tasks and clean up the cron file type `crontab -r`.  {{% tip %}} Since a single cron file can contain multiple tasks, we'd recommend putting a comment (`#...`) above each line that describes what the job is about. This serves as documentation for your future self. {{% /tip %}}    Windows 10 has a built-in graphical interface for scheduling tasks which provides plenty of functionalities and options you can adjust (e.g., frequency, start and end date, battery and network settings). Although you don't have to mess around with the terminal, you do need to create a so-called BAT script that executes your Python script. Follow the steps below to get started!  1. Double-check whether your script works as expected and then save the Python script as a **`.py` file** (so not a `.ipynb` notebook!). For example, copy-paste the contents from Jupyter to Spyder.  2. Open Notepad and create a new file with the following contents:    See the tip below on how to obtain the Python installation path. For example, the first line could look like `\"C\\Users\\Pieter\\AppData\\Local\\Programs\\Python\\Python37\\python.exe\" \"C\\Users\\Pieter\\Desktop\\scrape_websites.py\"`  3. Save the file as `run_script.bat` (doubling clicking the file should run the Python script)  4. Open **Task Scheduler** (search for it in your programs list)  5. Click **Create Basic Task** and fill out a **Name** and a **Description** for your task  6. Under **Security Options** choose **\"Run whether user is logged on or not\"** (i.e., the script will still run if the computer is in standby mode) and tick the box **\"Run with highest privileges\"**  7. Create a new trigger (e.g., Daily) and set a start and end date/time  8. Create a new action **\"Start a Program\"** and select the `.bat` file you created earlier (e.g., `run_script.bat`)  9. On the \"Conditions\" tab, you may want to untick the box **\"Start the task only if the computer is on AC power\"** (i.e., the script still runs if your laptop is on battery power) and tick the box **\"Wake the computer to run this task\"** to make sure the tasks are always executed on time.  10. Click **\"OK\"** and fill out your password. If done correctly, your newly scheduled task should be added to the Task Scheduler Library window.  {{% tip %}} Run the following command in, for example Spider or a Jupyter Notebook, to figure out the path of your Python installation. ```python import os, sys print(os.path.dirname(sys.executable)) ``` {{% /tip %}}   - [This](https://crontab.guru) website converts cron syntax into human-readable language. For some practice, click on *random* and try to write the cron expressions yourself (without looking at the answer of course)! "}, {"objectID": "./building-blocks/automate-and-execute-your-work/automate-your-workflow/workflow-checklist.md", "title": "Use this checklist to improve your project's structure", "description": "Audit data and computation efficient projects.", "keywords": "make, makefile, automation, recipes, workflow", "code": [], "headers": ["Overview", "Checklist", "Project level", "Throughout the Pipeline", "File/directory structure  ", "Automation & documentation", "Versioning", "Housekeeping", "Testing for portability", "See Also"], "content": "  As time goes on, projects tend to become messy which inhibits reproducibility. Hence, we recommend keeping an eye on this housekeeping checklist from time to time.   * Implement a consistent [directory structure](/tutorials/project-management/principles-of-project-setup-and-workflow-management/directories/#working-example): data/src/gen * Include [readme with project description](/tutorials/project-management/principles-of-project-setup-and-workflow-management/documenting-code/#main-project-documentation) and technical instruction how to run/build the project * Store any authentication credentials outside of the repository (e.g., in a JSON file), NOT clear-text in source code * Mirror your `/data` folder to a secure backup location; alternatively, store all raw data on a secure server and download relevant files to `/data`  * Create subdirectory for source code: /src/[pipeline-stage-name]/ * Create subdirectories for generated files in /gen/[pipeline-stage-name]/: temp, output, and audit. * Make all file names relative, and not absolute (i.e., never refer to C:/mydata/myproject, but only use relative paths, e.g., ../output) * Create directory structure from within your source code, or use .gitkeep * Create subdirectories for generated files in `/gen/[pipeline-stage-name]/`: `temp`, `output`, and `audit`. * Make all file names relative, and not absolute (i.e., never refer to C:/mydata/myproject, but only use relative paths, e.g., ../output) * Create directory structure from within your source code, or use .gitkeep   * Have a [`makefile`](/automate/project-setup) * Alternatively, include a [readme with running instructions](/tutorials/project-management/principles-of-project-setup-and-workflow-management/documenting-code/#main-project-documentation) * Make dependencies between source code and files-to-be-built explicit, so that `make` automatically recognizes when a rule does not need to be run (properly define targets and source files) * Include function to delete temp, output files, and audit files in makefile  * Version all source code stored in `/src` (i.e., add to Git/GitHub) * Do not version any files in `/data` and `/gen` (i.e., do NOT add them to Git/GitHub) * Want to exclude additional files (e.g., files that (unintentionally) get written to `/src`? Use .gitignore for files/directories that need not to be versioned  * Have short and accessible variable names * Loop what can be looped * Break down \"long\" source code in subprograms/functions, or split script in multiple smaller scripts * Delete what can be deleted (including unnecessary comments, legacy calls to packages/libraries, variables) * Use of asserts (i.e., make your program crash if it encounters an error which is not recognized as an error)  * Tested on own computer (entirely wipe `/gen`, re-build the entire project using `make`) * Tested on own computer (first clone to new directory, then re-build the entire project using `make`) * Tested on different computer (Windows) * Tested on different computer (Mac) * Tested on different computer (Linux)  {{% warning %}} **Versioned any sensitive data?**  Before making a GitHub repository public, we recommend you check that you have not stored any sensitive information in it (such as any passwords). This tool has worked great for us: [GitHub credentials scanner](https://geekflare.com/github-credentials-scanner/). {{% /warning %}}    - [This tutorial](/tutorials/project-management/principles-of-project-setup-and-workflow-management/overview/) covers the fundemantal principles of project setup and workflows underlying this checklist. "}, {"objectID": "./building-blocks/automate-and-execute-your-work/automate-your-workflow/fileexchanges.md", "title": "Share Large Temporary Files", "description": "File exhanges are essential to transfer data between different stages of your pipeline and between co-authors.", "keywords": "file exchange, file, exchange, share, sharing", "code": [], "headers": ["Amazon S3", "Setting up AWS Command Line Interface (AWS CLI)", "As an administrator of the research project", "As a user of the research project", "Using the S3 file exchange", "Dropbox"], "content": " [File exchanges](/tutorials/project-setup/principles-of-project-setup-and-workflow-management/directories/#4-file-exchange) are essential to transfer data *between different stages of your pipeline*, and between *different co-authors* working on a project.  The key requirements of a file exchange are:  - the programmatic access (i.e., via command line tools), - to data storage organized in directories and files, - allowing members of the project to upload or download data, - employing fine-grained access controls (i.e., to give users reading rights, or reading/writing rights).  Below, we show a few options that we've used in our research projects. Please check with your own institution whether the use of these services is permitted.   Amazon S3 offers you unlimited storage in so-called \"buckets\". Think of them as unlimited hard disks. It also offers a platform-independent command line tool (called AWS Command Line Interface, or in short, *AWS CLI*) so that you can write scripts to upload and download data from it.   - Download AWS Command Line Interface [here](https://aws.amazon.com/cli/). - Make sure it is callable from anywhere on your system, i.e., add the   installation path to your environment variables (for a recap, see our   [setup guide](/building-blocks/configure-your-computer/automation-and-workflows/environment-variables/)).   - Sign up for AWS S3 - Create a \"bucket\" that will hold the data (name of the bucket, and available region; since we're based in Europe, we choose `eu-central-1` as our location) - Create a set of user authentication credentials that you can give to your users  All details are [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example1.html).   - After [downloading and installing AWS CLI](https://aws.amazon.com/cli/), and making sure it is added to your environment variables, open a terminal and type `aws configure`. - You will be prompted to enter the following details, which you should have received by the project administrator:          AWS access key: [paste it here]         AWS secret access key: [paste it here]         default region name: eu-central-1 (if you're not located in Europe, choose the corresponding name for your region of the bucket)         default output format: [just hit enter here; we keep it empty]  - You're all set! You can now use the AWS CLI to download and upload data. Not sure why this is needed? Review our notes on [data management and directory structure](/tutorials/project-setup/principles-of-project-setup-and-workflow-management/directories/).   - Write scripts that upload and download data from S3, according to your [directory structure and workflows](/tutorials/project-setup/principles-of-project-setup-and-workflow-management/directories/).     - For **uploading**, we use the following (platform-independent) command: `aws s3 cp FOLDERNAME s3://BUCKETNAME --recursive --region eu-central-1`      - For **downloading**, we use the following commands: `aws s3 sync s3://BUCKETNAME TARGETFOLDER`, which downloads the entire contents of `BUCKETNAME` (you can use subdirectories, too!) to the directory `TARGETFOLDER` on your local computer.      - Sometimes you do want to download only *specific* files rather than entire folders. For example, `aws s3 sync s3://BUCKETNAME/directory1/ TARGETFOLDER --region eu-central-1 --exclude \"*\" --include \"*.json\"` downloads only *json* files to the `TARGETFOLDER` on your local computer.   Raw data files are ideally hosted on a secure server; in practice, many researchers store their data on Dropbox, though (e.g., because they want to enable co-authors *not* used to reproducible workflows to also access these files.  A good way then is to *host* your raw data files in a Dropbox folder. Thereby, - your co-authors have access to the files (and, e.g., can populate   the directories with their own data collections) - you can still programmatically access these files and reproduce   them to any directory on your hard disk.  [Here is a Python script](https://gist.github.com/hannesdatta/10422a6fbb584f245c83361245335741.js) that *downloads* the data to your local directory structure. In that way, it mimics the \"downloading\" part of the AWS S3 file exchange described above.  You can upload data simply by moving files to this shared Dropbox folder (yes, just use Dropbox!). "}, {"objectID": "./building-blocks/automate-and-execute-your-work/automate-your-workflow/stata-error-handling-make.md", "title": "Integrating Stata in Automated Workflows", "description": "Learn how to use R to check for errors and the completion of Stata code in batch mode or in a Makefile.", "keywords": "exception, handling, stata, R, make, error", "code": [" ```R # Define the arguments args = commandArgs(trailingOnly=TRUE)  # Test if there is at least one argument: if not, return an error if (length(args)==0) {   stop(\"At least one argument must be supplied (input file).n\", call.=FALSE) }  # Read the log file filecontent = readLines(args)  # Check if the log file includes an error massage and if so stop and display an error message if (any(grepl('^r[(][0-9]',filecontent,ignore.case=T))) {  stop(paste0('Log file for ', args, ' contains errors!')) } # Check whether the do-file was executed completely if (!any(grepl('(end of do-file)',filecontent,ignore.case=T))) {    stop(paste0('File (', args, ') has not been processed entirely!')) } # If no errors, report that there are no errors. cat(paste0('Log file for ', args, ' checked. No errors.')) ``` ", "  ```bash # Define a rule where you use a do-file target_file: prerequisite.do # define your target and prerequisites \trm prerequisite.log # remove older log file produced by prerequisite.do previously \tStataMP-64.exe -e do prerequisite.do # execute the do-file \tRscript logcheck.R prerequisite.log # check the log file for errors or incompletion ```  "], "headers": ["Overview <!-- Goal of the Building Block -->", "Code", "Create an R script that checks for errors and completion of the do-file", "Incorporate the check in the Makefile"], "content": "  When you run Stata within an automated research pipeline (e.g., using a `makefile`), Stata does *not stop the progression* of the Makefile, even if there is an error in your cod! Thus, you have no idea whether Stata code was executed without any errors until the end without checking the Stata log files.  To remedy this issue, you can use R to check for any error that may have occurred in the log file. If there was an error, we can make the workflow to interrupt.   We show you how to very simply check the Stata log files for errors and stop the Makefile if there are any errors.  - First off, make sure you have [Make set up](/get/make), [Stata](/get/stata) and [R](/get/r/) executables are added to your environment variable called \"PATH\". - Also make sure that your Stata do-file produces a log file.  Next, we create an R script called `logcheck.R` that checks for errors and the completion of the do-file from the log file.     Here, you can replicate any rule where you run a do-file which creates a log file. We just use some random rule:   "}, {"objectID": "./building-blocks/automate-and-execute-your-work/automate-your-workflow/software-environments.md", "title": "Use Software Environments to Ensure Replicability", "description": "Sometimes when we scrape the web, we need to automate our computer to open a web browser to gather information from each page.", "keywords": "environments, conda, software environments, virtual", "code": [" ```bash # Create new empty environment # (this will ask where to save the environment, default location is usually fine ) conda create --name test_env  # Activate environment conda activate test_env  # Install R 3.6.1 and data tables package # Note that R packages use prefix r- conda install r-base=3.6.1 conda install r-data.table  # Close environment (switches back to base) conda deactivate ``` "], "headers": ["Overview", "Why is keeping track of software/package versions important?", "Virtual environments", "General setup", "Code snippet", "Using the code", "Examples", "OS dependence", "Additional Resources  "], "content": "  The **main advantages** of using virtual software environments are:  1. **Ensures replicability**:     - Environment specifies versions for each program and package used     - Ensures that specified versions are the right ones (environment does not forget to update specified version if the version is updated in a project) 2. **Easy set-up on a different machine or the cloud**: run environment setup to install all required software/packages.  3. **Keeps projects separate**: adding or updating packages for one project does not affect others.  Here, we will explain how to use Conda to set up such virtual software environments. Conda is a package and environment manager and allows to install Python, R as well as additional software (e.g. pandoc) on Windows, Linux and MacOS.    If software or packages are updated, code that was written for one, may not work on the updated version. An obvious example here is the difference between Python versions 2.7.x and 3.x, where code written for one will not work on the other. For example, just try the basic `print 'Hello World'` on Python 2.7.x and 3.x.  In such cases an error occurs and it will be obvious that a different software version needs to be used. However, changes across versions can also be more subtle and harder to detect. If the syntax does not change, but the underlying functions do, then code will still run through but may produce different results.  {{% example %}} One example of a subtle change is the update of R from version 3.5.1 to 3.6.0. In this update, the way random numbers are produced by the `sample()` function was changed. This means that if the `sample()` function is used in a code, then running the same code on R 3.5.1 and 3.6.0 (or newer) will produce different results even if the seed is fixed. You can verify this easily by running this code in both R versions:      # Fix seed     set.seed(1)     # Draw a random number from 1,2,...10     r = sample(1:10,1)     # = 9 on 3.6.3     # = 3 on 3.5.1 {{% /example %}}  Given that such changes are bound to occur over time, it is important to keep track of which versions (of all packages!) are used for a project if it should still work and produce the same results in the future.   This is where virtual software environments come in. The environment basically is just a list specifying the full version specifications of all software and packages used. Of course, such an environment could just be a list typed by hand. But as projects develop over time with different packages being added and updated, the list would have to be kept up to date by hand, which is prone to errors and can be quite the effort.  Fortunately, there are software solutions that specify and keep track of software environments. The one we're using here is Conda, which is part of Anaconda which you will have already installed if you installed Python following the instructions [here](/building-blocks/configure-your-computer/statistics-and-computation/python/). Of course, other solutions exist (e.g. in Python itself) to set up such environments. These are linked below.  There are multiple advantages to using Conda (or similar setups): 1. Installs software and packages directly (at least for R and Python) 2. Automatically keeps track of versions 3. Export environments and easily set them up on a different machine (or in the cloud) 4. Easy to use (both command line and graphical interface) 5. Part of Anaconda (which we used on Windows to install Python in the first place)   For each project, create a separate software environment. This ensures that if you're updating versions for one project, it won't affect your other projects. Moreover, it creates fewer dependencies in a given project, as its environment will not also contain dependencies from other projects.  Note that now instead of using the familiar `pip install` or `install.packages()` commands in Python and R, it is necessary to install packages through Conda so that they are correctly added to the environment.   {{% tip %}} **Activate the right environment**  Having multiple projects means having multiple environments. So when working on a project, always make sure the corresponding environment is activated. {{% /tip %}}   <!-- Provide your code in all the relevant languages and/or operating systems. -->     If Conda is installed, the example can be run (line by line) in a terminal to set up a new software environment, install R and additional packages. The same structure can be used to set up environments for python, install additional packages and much more.  **Installation instructions for Conda**:  - If you've installed Python based on [these instructions](/building-blocks/configure-your-computer/statistics-and-computation/python/), Conda should be already available - Alternatively, detailed instructions to install Conda are provided [here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).  Note that with the full Anaconda install environments can also be managed using a graphical interface. Here, we focus only on the command line to have clear steps. Instructions for the graphical interface are provided [here](https://docs.anaconda.com/anaconda/navigator/tutorials/manage-environments/).  1. Add *conda-forge* channel to get newer R version          # Add 'conda-forge' channel (provides more recent versions,         # and a lot of additional software)         # Note: Sets overall config, not per environment         conda config --add channels conda-forge          # Install newer R version in test_env         conda activate test_env         conda install r-base=4.0.3   2. Search for available packages          # lists packages starting with r-data         conda search r-data*  3. Export environment into file (to be installed on a different machine)          # Activate environment to be saved         conda activate test_env          # Export environment to a yml-file (in current directory)         conda env export > test_env.yml          # Export enviornment to a yml-file, only packages explicilty requested         # (required if environment ported to different OS, see below)         conda env export --from-history > test_env.yml  4. Import and set up environment from file          # Create environment based on yml-file (created as above, in same directory)         conda env create -f test_env.yml  5. Deactivate environment (gets back to base)          conda deactivate  6. Remove environment          conda env remove --name test_env      Note that there's a bug; software environments sometimes can only be removed if at least one package was installed.  7. List installed environments          conda info --envs    Sometimes we work across different operating systems. For example, you may develop and test code on a Windows desktop, before then running it on a server that runs on Linux. As the Conda environment contains all dependencies, it will also list some low level tools that may not be available on a different OS. In this case, when trying to set up the same environment from a *.yml* file created on a different OS it will fail.  {{% example %}} Check the output of `conda list`, which will list various dependencies you never explicitly requested. {{% /example %}}  In this case, it is possible to use the `--from-history` option (see Example 3 above). When creating the environment `.yml` file, this option makes it so that the generated `.yml` file will only contain the packages explicitly requested (i.e. those you at one point added through `conda install`), while the lower level dependencies (e.g. compiler, BLAS library) are not added. If the requested packages exist for the different OS, this usually should work as the low level dependencies will be automatically resolved when setting up the environment.   1. More information on Conda environments: [https://conda.io/projects/conda/en/latest/user-guide/concepts/environments.html](https://conda.io/projects/conda/en/latest/user-guide/concepts/environments.html)  2. Python virtual environments: [https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)  3. Information on package environments in Julia: [https://julialang.github.io/Pkg.jl/v1/environments/](https://julialang.github.io/Pkg.jl/v1/environments/) "}, {"objectID": "./building-blocks/automate-and-execute-your-work/automate-your-workflow/auto-install-R-packages.md", "title": "Automatically Install R Packages Used in a Project", "description": "Easily find all the R packages used in a project and automatically install them.", "keywords": "R packages, installation, find, gist", "code": [" ```R # find all source code files in (sub)folders files <- list.files(pattern='[.](R|rmd)$', all.files=T, recursive=T, full.names = T, ignore.case=T)  # read in source code code=unlist(sapply(files, scan, what = 'character', quiet = TRUE))  # retain only source code starting with library code <- code[grepl('^library', code, ignore.case=T)] code <- gsub('^library[(]', '', code) code <- gsub('[)]', '', code) code <- gsub('^library$', '', code)  # retain unique packages uniq_packages <- unique(code)  # kick out \"empty\" package names uniq_packages <- uniq_packages[!uniq_packages == '']  # order alphabetically uniq_packages <- uniq_packages[order(uniq_packages)]  cat('Required packages: \\n') cat(paste0(uniq_packages, collapse= ', '),fill=T) cat('\\n\\n\\n')  # retrieve list of already installed packages installed_packages <- installed.packages()[, 'Package']  # identify missing packages to_be_installed <- setdiff(uniq_packages, installed_packages)  if (length(to_be_installed)==length(uniq_packages)) cat('All packages need to be installed.\\n') if (length(to_be_installed)>0) cat('Some packages already exist; installing remaining packages.\\n') if (length(to_be_installed)==0) cat('All packages installed already!\\n')  # install missing packages if (length(to_be_installed)>0) install.packages(to_be_installed, repos = 'https://cloud.r-project.org')  cat('\\nDone!\\n\\n') ``` "], "headers": ["Overview", "Code"], "content": "  You can use the following code to easily find all the the R packages used in a project, and automatically install the uninstalled ones on your machine.  Put this script in the root directory of your R project, and either source it or run it from the command line: ```> Rscript install_packages.R```    "}, {"objectID": "./building-blocks/automate-and-execute-your-work/automate-your-workflow/what-are-makefiles.md", "title": "Use Makefiles to Re-Run your Code", "description": "Learn how to use makefiles to specify instructions and recipes for your computer.", "keywords": "make, makefile, automation, recipes, workflow", "code": [], "headers": ["Overview", "Code", "Rules", "Multiple rules", "Advanced Use Cases", "Use directory names", "\"Phony\" targets", "Use variables", "Run make using `.bat` files", "See Also"], "content": "  Makefiles are instructions (\"rules\") for a computer on how to build \"stuff\". Think of makefiles as a recipe you may know from cooking (\"Baking a cake: First, take some flour, then add milk [...]\") - but then for computers.  Makefiles originate in software development, where they have been used to convert source code into software programs that can then be distributed to users.  Researchers can use makefiles to define *rules* how individual components (e.g., cleaning the data, running an analysis, producing tables) are run. When dependencies (e.g., to run the analysis, the data set first has to be cleaned) are well-defined, researchers can completely automate the project. When making changes to code, researchers can then easily \"re-run\" the entire project, and see how final results (potentially) change.    A rule in a makefile generally looks like this:  ``` targets: prerequisites    commands to build ```  * The __targets__ are things that you want to build - for example, data sets, outputs of analyses, a PDF file, etc. You can define multiple targets for one rule (separate targets by spaces!). Typically, though, there is only one per rule. Think of this as the dish (or part of the dish) that you want to create with your recipe.   * Example: `dataset.csv` (my final, cleaned dataset)  * The __prerequisites__ are the things that you *need* before you can build the target. It's also a list of file names, separated by spaces. These files need to exist before the commands for the target are run. They are also called dependencies. The cool thing is that `make` automatically checks whether any of the dependencies has changed (e.g., a change in the source code) - so it can figure out which rules to be run, and which ones not (saving you a lot of computation time!).You can view these as the ingredients of the recipe.   * Example: `rawdata1.csv clean.R` (before building `dataset.csv`, the raw data and a specific script to clean the raw data need to exist)  * The __commands__ are a series of steps to go through to *build* the target(s). These need to be indented with a tab (\"start with a tab\"), *not* spaces. The commands can be seen as the recipe instructions.   * Example: the command `R --vanilla < clean.R` opens R, and runs the script `clean.R`.  *Final example:*  ``` dataset.csv: rawdata1.csv clean.R   R --vanilla < clean.R ```   A makefile typically consist of multiple rules, which can depend on each other.  ``` # rule to build target2 target2: target1   commands to build  # rule to build target1 target1: prerequisite1   commands to build ```    You can easily use directory names in makefiles, e.g., to specify that a prerequisite is in one directory, and the target in another.  *Example* ``` gen/data-preparation/aggregated_df.csv: data/listings.csv data/reviews.csv \tRscript src/data-preparation/clean.R ```    Targets typically refer to output - such as files. Sometimes, it's not practical to generate outputs. We call these targets \"phony targets\".  * Creating a target `all` and `clean` is a convention of makefiles that many people follow. * The phony target `all` calls all targets. Think of it as a \"meta rule\" to build it all! * The target `clean` typically is used to remove generated/temp files, so you can start with a clean copy of your directory for testing.  {{% example %}} ``` all: one two  one:     touch one.txt two:     touch two.txt  clean:     rm -f one.txt two.txt ``` {{% /example %}}   Variables in a make script prevent you from writing the same directory names (or command to execute a program) over and over again. Variables are typically defined at the top of the file and can be accessed with the `$` command. Note that variables can only be strings.  {{% example %}} ``` INPUT_DIR = src/data-preparation GEN_DATA = gen/data-preparation  $(GEN_DATA)/aggregated_df.csv: data/listings.csv data/reviews.csv   $(INPUT_DIR)/clean.R ``` {{% /example %}}   Running a pipeline with make usually requires you to work frmo the command line/terminal (i.e., type `make` to start the workflow). In some circumstances, it may be more convenient to start the workflow from a batch file (e.g., on Windows, you would just have to double-click on such a file).  Here's a small code snippet to achieve just that. Just create a `.bat` file in your project's directory (i.e., the one where you would usually run `make` in). This snippet writes any output from make in a `make.log` file, which you can use to verify `make` has run properly.  ``` make -k > make.log 2>&1 pause ```   - [The Turing Way's Guide to Reproducible Research using `make`](https://the-turing-way.netlify.app/reproducible-research/make.html) - [Software Carpentry's Lesson on Automation and Make](http://swcarpentry.github.io/make-novice) - [Example makefile for an analysis](https://github.com/hannesdatta/brand-equity-journal-of-marketing/blob/c8c9ff7a6904b4f6a7ad718932f21c6b87d4d881/analysis/code/makefile) - [Example makefile for a data preparation pipeline](https://github.com/hannesdatta/brand-equity-journal-of-marketing/blob/c8c9ff7a6904b4f6a7ad718932f21c6b87d4d881/derived/code/makefile) "}, {"objectID": "./building-blocks/develop-your-research-skills/learn-to-code/learn-bash-commands.md", "title": "Learn Bash Commands", "description": "Learn how to navigate the terminal and command prompt", "keywords": "cmd, command prompt, terminal, cd, ls, dir, mv", "code": [" ```bash absolute_path = \"/Users/Pieter/Desktop/images/photo1.jpg\"  relative_path = \"images/photo1.jpg\"  # provided that your working directory is \"Desktop\" ``` ", " ```bash # move directories or files to a new location mv [FILE_NAME] [LOCATION]  # rename files mv [FILE_NAME] [NEW_FILE_NAME] ``` "], "headers": ["Overview", "Directories", "File Paths", "Open Command Line", "Commands", "Print Working Directory", "List Files", "Change Directories", "Create Folder", "Remove Files & Directories", "Move & Rename Files", "See Also"], "content": "  The command line lets you communicate with your computer without using your mouse. It's a text-based interface where you communicate using text alone. The commands differ between Unix-based systems (e.g., Mac / Linux) and Windows computers. On Apple and Windows computers the command line tool is called the Terminal and Command Prompt, respectively.   In programming-speak, all folders are called directories. A directory within another directory is called a subdirectory. When you open up a new Terminal window, you are automatically situated in your home directory. Your current directory is also referred to as your working directory.   Your computer understands two kinds of paths, absolute and relative ones. An absolute path starts from the root (or home directory). A relative path, on the other hand, starts from the current directory (for instance, your desktop).     *Mac*   Open the Terminal by going into your Applications folder, and then into your Utilities folder, and then double clicking on the Terminal icon. Alternatively, you can use Spotlight to search for the program (e.g., Cmd + Space).  *Windows*   Type `cmd` into the search bar and hit enter, it should launch the Windows Command Prompt.    *Mac*   To see the full path for your working directory, you can use the `pwd` command, which stands for print working directory.  ![terminal_pwd](../images/terminal_pwd.gif)  *Windows*   If you type `pwd` in the Windows Command prompt, it doesn't recognize the command. Instead, type `cd` (or `chdir`) to view the current working directory.  *Mac*   Unlike the graphical user interface, you cannot see what other directories or files are present just by being in a particular directory. You have to explicitly tell your computer to list them for you using the `ls` command.  ![terminal_ls](../images/terminal_ls.gif)  If you\u2019d like to explore another directory, you need to use ls with a path. For example, `ls ~/Documents` will show you what's inside Documents.  *Windows*   The `dir` command lists all files and folders in the current directory. In addition, it returns the file size, file type, and the date and time the file was last updated.  {{% tip %}} Press the up and down arrows on your keyboard to cycle through previously used commands in the terminal. Also, you can press `TAB` for auto-complete suggestions. {{% /tip %}}  *Mac & Windows*   The command `cd` allows you to change directories; it's the equivalent of double clicking on a folder. For example, if you want to move from the `samspade` directory into its subdirectory `Documents`, you would write `cd Documents`. To move back up to your home directory, you can use the `cd ..` command.  Note that the path in front of `$` indicates your working directory. This helps you keep track of where you're at.  ![terminal_cd](../images/terminal_cd.gif)   {{% tip %}} A handy shortcut to start in a specific directory immediately is to look up the folder in Finder, right-click on it, and choose \"New Terminal at Folder\". {{% /tip %}}  *Mac & Windows*    To make a new directory, you can use the `mkdir` command which takes the name of the new directory and the destination path of the directory. So, the command `mkdir to-do-lists Documents` would create a new to-do lists directory inside the documents folder.  {{% tip %}} Generally speaking, it's better to avoid special characters in directory names, you can use quotes to create a folder name that include spaces (e.g., `mkdir \"to do lists\" Documents`). {{% /tip %}}   *Mac*   You can delete a directory along with any files or subdirectories by running the `rm -r` command. For example, `rm -r to-do-lists` removes the to-do lists folder and its contents. Note that you can list multiple files and directories after `rm -r` to remove them all.  *Windows*    Separate files can be removed with the `del` command (e.g., `del to-do-list.txt`). To remove a directory, use the command `rmdir`.  {{% warning %}} You can't undo the `rm` and `del` commands, so be careful when you delete files. Removing files in command line is not the same as moving a file to the trash - it's permanent! {{% /warning %}}   *Mac*   The `mv` command has two applications: moving a and renaming files. It uses the following syntax:    Examples: * `mv monday_tasks.txt To_Do_Lists` moves the text file from its current directory to the to-do list folder. * `mv 'Monday Tasks.txt' monday_tasks.txt` renames the file by stripping the spaces.  *Windows*   The syntax to move and rename files are exactly the same as on Mac, but you need to use the `move` and `rename` commands instead, respectively. Note that you cannot use `move` to rename files on Windows!  * This [guide on the \"art of command line\"](https://github.com/jlevy/the-art-of-command-line). * This building block draws on material of [this](https://generalassembly.github.io/prework/cl/#/) interactive tutorial by General Assembly which is absolutely worth checking out! * [Cheatsheet](http://www.cs.columbia.edu/~sedwards/classes/2015/1102-fall/Command%20Prompt%20Cheatsheet.pdf) with the most common Windows Command Prompt commands. "}, {"objectID": "./building-blocks/develop-your-research-skills/learn-to-code/learn-python-and-julia.md", "title": "Learn Python and Julia", "description": "Learn how to program in Python.", "keywords": "learn, python, quantecon", "code": [], "headers": [], "content": " Learn Python with these great step-by-step tutorials from [Real Python](https://realpython.com).  [QuantEcon.org](https://quantecon.org/) offers fantastic resources for you to get started using Python and Julia.  In particular, we would like to refer you to their [open-source lectures](https://quantecon.org/lectures/), covering:  - [Quantitative Economics with Python](https://python.quantecon.org/) (and a fantastic, open-source book companion!) - [Quantitative Economics with Julia](https://julia.quantecon.org/) - [QuantEcon Data Science](https://datascience.quantecon.org/) "}, {"objectID": "./building-blocks/develop-your-research-skills/learn-to-code/learn-r.md", "title": "Learn R", "description": "Learn how to code with R.", "keywords": "learn, R, resources, datacamp", "code": [], "headers": [], "content": " Chance is that sooner or later, you're going to use R - one of the (if not *the*) most popular statistical programming languages in the world. It's not only that R is free which makes it so good, but because thousands of people across the world use it, document it, and develop it further.  Let us warn you: the R interface may be scary at first sight, especially if you're used to using Excel to do your data magic. But - trust us - investing in learning R will pay off greatly. Not only for your career at grad school, but also for your career in industry. R (next to Python) has been a de-facto standard when applying for jobs in marketing analytics or data science.  One thing that's great about R is that there are tons of resources available to learn it. Well - that brings the difficulty that you need to know which resources to use to learn it most efficiently.  {{% tip %}} **Learn R fast** Here are a few tips to learn R efficiently.  - Have a project!    \"Huh - why to have a project? I first need to learn R!\"    Well, if you *really* want to learn R, then you should already have a project in mind that you would like to tackle. R is such a powerful tool that you would get entirely lost if you didn't actually know what you would like to accomplish.   If you don't have a project, and still would like to learn R, we suggest you to do some googling for some interesting datasets to get you started (e.g., [Kaggle.com](https://www.kaggle.com)).  - Follow our [R/R Studio installation guide](/building-blocks/configure-your-computer/statistics-and-computation/r/) - Enroll R courses at Datacamp.com (free-to-use with a Tilburg University account!)   - Must do's for **novices**       - [Introduction to R](https://www.datacamp.com/courses/free-introduction-to-r)       - [Intermediate R](https://www.datacamp.com/courses/intermediate-r)   - Must do's if you have to **manage data** and prepare your own datasets       - Learn *data.table*, our preferred tool to wrangle with *large* data. Just       search for *data.table* at datacamp.com. These are our favorites courses: [Data manipulation](https://www.datacamp.com/courses/data-manipulation-in-r-with-datatable) and [Joining data](https://www.datacamp.com/courses/joining-data-in-r-with-datatable).       - Complementary to *data.table*, you should dive into *Tidyverse*, a collection of tools that will make your life much easier. This is our top-pick: [Introduction to Tidyverse](https://www.datacamp.com/courses/introduction-to-the-tidyverse)   - Learning your *method*       - As soon as your data is prepped, you can start analyzing. Of course, your method is informed by your specific research question. For most students, a refresher in [regression analysis](https://www.datacamp.com/courses/multiple-and-logistic-regression) (e.g., OLS, Logit) may be exactly what they need. {{% /tip %}} "}, {"objectID": "./building-blocks/develop-your-research-skills/learn-to-code/learn-regular-expressions.md", "title": "Learn Regular Expressions", "description": "Learn how to formulate powerful string search queries with regular expressions.", "keywords": "regular expressions, regex, strings, string processing, characters, regexp, search pattern, string searching", "code": ["  ```python import re my_string = \"The 80s music hits were much better than the 90s.\"  # a single digit: ['8', '0', '9', '0'] print(re.findall(r\"\\d\", my_string))  # double digits: ['80', '90'] # hereafter we learn how to write this more concisely using quantifiers print(re.findall(r\"\\d\\d\", my_string))  # combinations of 3 characters (even if it's not a complete word) # ['The', '80s', 'mus', 'hit', 'wer', 'muc', 'bet', 'ter', 'tha', 'the', '90s'] print(re.findall(r\"\\w\\w\\w\", my_string))  # combinations of 3 characters that start and end with a space (note: the first \"The\" is skipped!): [' 80s ', ' the '] print(re.findall(r\"\\s\\w\\w\\w\\s\", my_string)) ```  ", " ```python import re my_string = \"The 80s music hits were much better than the 90s.\"  # all words: ['The', '80s', 'music', 'hits', 'were', 'much', 'better', 'than', 'the', '90s'] print(re.findall(r\"\\w+\", my_string))  # one or more digits followed by a s: ['80s', '90s'] print(re.findall(r\"\\d+s\", my_string))  # words that are preceded or followed by more than one whitespace (i.e., to identify unnecessary spaces) print(re.findall(r\"\\s{2,}\\w+\\s{2,}\", my_string)) ``` ", " ```python my_string = \"The 80s music hits were much better than the 90s.\"  # words that consist solely of lower case letters # [' music', ' hits', ' were', ' much', ' better', ' than', ' the'] print(re.findall(r\"\\s[a-z]+\", my_string))  # one or more letters followed by a space, one or more digits, and a letter s: ['The 80s', 'the 90s'] print(re.findall(r\"[a-zA-Z]+\\s\\d+s\", my_string)) ``` ", " ```python import re my_string = \"Lara has 2 sisters who also study in Tilburg. Mehmet has 1 sister who was born last year. Sven has 19 cousins who are all older than him.\"  # store the person's name, the digit, and the number of relatives. # [('Lara', '2', 'sisters'), ('Mehmet', '1', 'sister'), ('Sven', '19', 'cousins')] family = re.findall(r\"([a-zA-Z]+)\\s\\w+\\s(\\d+)\\s(\\w+)\", my_string)  # next, you can reference elements like you're used to, for example: print(family[0][2])  # gives 'sisters' ``` ", " ```python import re my_string = 'Last year was our most profitable year thus far. Our year-on-year growth grew by 14% to $10B!'  # split on both \"! and \".\": # ['Last year was our most profitable year thus far',' Our year-on-year growth grew by 14% to $10B',''] re.split(r\"[!.]\", my_string)  # hide confidential data: # 'Last year was our most profitable year thus far. Our year-on-year growth grew by X% to $XB!' re.sub(r\"\\d+\", \"X\", my_string) ``` ", " ```python import re my_string = '<p>This is a paragraph enclosed by HTML tags.</p>'  # greedy approach: '' re.sub(r\"<.+>\", \"\", my_string)  # non-greedy approach: 'This is a paragraph enclosed by HTML tags.' re.sub(r\"<.+?>\", \"\", my_string) ``` "], "headers": ["Overview", "Code", "Match Characters", "Quantifiers", "Alternates", "Groups", "Split & Replace Data", "Web Scraping", "Advanced Use Cases", "See Also"], "content": "  Regular expressions provide a concise language for describing patterns in strings. They make finding information easier and more efficient. That is, you can often accomplish with a single regular expression what would otherwise take dozens of lines of code. And while you can often come a long way with the default `strip()` and `replace()` functions, they have their own set of limitations. For example, how do you extract emails or hashtags from a text? Or how do you strip away HTML tags from the web page source code? Regular expressions fill this void and are a powerful skill for any data scientist's toolkit!  {{% warning %}} At first sight, regular expressions can look daunting but don't be put off! In this building block, we break down the syntax into tangible pieces so that you can get up to speed step by step.  {{% /warning %}}    The `re` Python library contains a variety of methods to identify, split, and replace strings. The `findall()` function returns all matching cases that satisfy a character pattern. Each pattern starts with `r\"` followed by one or more symbols. Please note that \"regular\" characters can be chained together with these symbols. For example, `r\"\\d\\ds\"` refers to 2 digits (`\\d`) followed by a lower case letter `s`.  | Symbol | Definition | Example | |:---- | :---- | :---- | | `\\d` |  digit | 0, 1... 9 |    | `\\s` |  whitespace | (a space) | | `\\w`  | single letter, number of underscore | a, 4, _ |    | `.`  | any character  | b, 9, !, (a space)  |     The four examples below illustrate how to combine these symbols to extract various characters from `my_string`.     In the examples above, we explicitly formulated a pattern of 1, 2, or 3 characters but in many cases this is unknown. Then, quantifiers can offer some flexibility in defining a search pattern of 0, 1, or more occurences of a character. Note that the symbols always refer to the character preceding it. For example, `r\"\\d+\"` means one or more digits.  | Symbol | Definition | Example | |:---- | :---- | :---- | | `?` | zero or one | `colou?r` (you want to capture both `color` and `colour`) | | `*` | zero or more | `\\d\\.\\d*` (a single digit followed by zero or more decimals - e.g., 5.34, 8., 3.1)| | `+`  | one or more  | `#\\w+` (e.g., Twitter hashtags)| | `{n,m}`  | between n and m | `\\d{2}-\\d{2}-\\d{4}` (dates e.g., 05-03-2021)|       | Symbol | Definition | Example | |:---- | :---- | :---- | | `a|b|c`  | or  | `color|colour`  | | `[abc]` | one of | `[$\u20ac]` (a dollar or euro currency sign)| | `[^abc]` | anything but  | `[^q]` (any character except q) | | `[a-z]` | range  | `[a-zA-Z]+` (i.e., one or more lower or upper case letters) or `[0-9]` (digits)|      More often than not, you want to capture text elements and use it for follow-up analyses. However, not every element of a pattern may be necessary. Groups, denoted by parentheses `()`, indicate the pieces that should be kept, and each match is stored as a list of tuples.       Regular expressions can also be used to split (`re.split()`) or replace (`re.sub()`) characters. While the built-in `split()` function can split on a single character (e.g., `;`), it cannot deal with a multitude of values. The same holds for Python's `replace()` function.      In addition to Beautifulsoup, you can apply regular expressions to parse HTML source code. Say the source code of a webpage consists of a book title, description, and a table:  ```python html_code = ''' <h2>A Light in the Attic</h2> <p>It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers.</p> <table class=\"table table-striped\">     <tr>         <th>UPC</th><td>a897fe39b1053632</td>     </tr>     <tr>         <th>Price (incl. tax)</th><td>\u00c2\u00a351.77</td>     </tr>     <tr>         <th>Availability</th>         <td>In stock (22 available)</td>     </tr>     <tr>         <th>Number of reviews</th>         <td>0</td>     </tr> </table> ''' ```  Then, we can easily capture the text between two tags, a part of a row, or a specific section of the source code. Since the HTML code is split across multiple lines, the regex code `.+` does not work as expected: it only matches characters on the first line. If you print the `html_code` to the console, you also find that each line is separated by a newline separator (\\n). As a workaround, you can use the following set `[\\s\\S]+` to capture both spaces (\\s) and non-spaces (\\S = letters, digits, etc.). Note that in the examples below, we rely on groups `()` to only select the elements we are after.  ```python # title: ['A Light in the Attic'] re.findall(r\"<h2>(.+)</h2>\", html_code)  # availability: ['22'] re.findall(r\"(\\d+) available\", html_code)  # <h2> and <p> sections re.findall(r\"[\\s\\S]+</p>\", html_code)  # table section re.findall(r\"<table[\\s\\S]*\", html_code) ```   **Greedy vs non-greedy**   By default, regular expressions follow a greedy approach which means that they match as many characters as possible (i.e., returns the longest match found). Let's have a look at an example to see what this means in practice. Say that we want to extract the contents of `my_string` and thus remove the HTML tags.  Therefore, we replace the two paragraph tags (`<p>` and `</p>`) with an empty string. Surprisingly, it returns an empty string (`''`), why is that? After all, we would expect to see: `This is a paragraph enclosed by HTML tags.`.  It turns out that the `>` in `<.+>` refers to the `</p>` tag (instead of `<p>`). As a result, the entire sentence is replaced by an empty string! Fortunately, you can force the expression to match as few characters as needed (a.k.a. non-greedy or lazy approach) by adding a `?` after the `+` symbol.     * As you may have figured out by now, formulating regular expressions is often a matter of trial and error. An [online regex editor](https://regexr.com) that interactively highlights the phrases your regular expression captures can therefore be extremely helpful.  * Frequent applications of regular expressions are extracting dates and emails (and checking for validity), parsing webpage source data, and natural language processing. [This](https://www.analyticsvidhya.com/blog/2020/01/4-applications-of-regular-expressions-that-every-data-scientist-should-know-with-python-code/) blog post demonstrates how you can implement these ideas.  * This building block deliberately only revolved around the most common regex operations, but there are many more symbols and variations. [This](https://www.programiz.com/python-programming/regex) tutorial provides a more comprehensive list of commands (incl. examples!). "}, {"objectID": "./building-blocks/develop-your-research-skills/tips/search-literature.md", "title": "Searching for literature on the web", "description": "Find relevant literature more efficiently", "keywords": "search, literature, libkey, google scholar", "code": [], "headers": ["What to search for? Search effectively.", "Where to search", "Google Scholar.", "Crossref"], "content": "Although there is no exact recipe for an effective search, the following tips will surely help you find relevant literature for your research more effectively:    - **Identify your keywords.** Start by defining the research question that you aim to answer, what are the main ideas? Use such question toward identifying the keywords. Employing the jargon of your subject area will also help you to find the most effective words for your search.    - **Connect your keywords.** Use boolean logic to link your search words in specific ways:       - **OR** will broaden your search to either (or both) of the two words.              Elderly OR senior        - **AND** will narrow your search to both terms.              Elderly AND mental health        - **NOT** will exclude the terms, thus, narrowing your search.              (Elderly OR senior) NOT young    - **Search tricks**       - *Truncation* (`*`): Helps finding different word endings.              senior* --> seniors, seniority        - *Phrase searching* (`\"...\"`): Helps searching for common phrases              \"mental health\" will find results with both words together as a phrase        - *Wildcards* (`?`): Allow for spelling variations within the same or related terms              b?ll --> bell, bill and ball    - **Narrow and refine your search results**. For instance, filter by year of publication or date range and by source type (e.g. article, book or review).     Make things easier: use **Google scholar**. It provides a simple way to find scholarly literature from multiple disciplines and sources (articles, books, journals...).    If for some reason, you cannot access a specific article, paper or book, paste the DOI at [libkey.io](https://libkey.io/) to retrieve the article (you need to login through your institution).  {{% tip %}}   - **Useful tips from the search results:**     - **Relevant articles and cited by.** In just one click these two options will help you find a broad range of additional relevant literature for your research question.      - **Citation**. It allows you to download the citation in different styles and formats (e.g., Bibtex, Refworks...) without even having to access the specific document.      - **Linking the access from your institution to Google Scholar**. This allows you to access the resources available from your institution's online library directly. This can be done in 3 simple steps:          1- Go to Google Scholar [settings](https://scholar.google.co.uk/scholar_settings)          2- Click on the \"library links\" section and type in the name of your institution in the search box.          3- When the result appears, uncheck any other boxes to which you do not have access to and click on the save button.          Once set, you will see at each result of the search a \"Fulltext via [institution search tool]\". {{% /tip %}}    Another good alternative to find scholarly literature. Given that it relies on the metadata of publications,  its search is highly reliable providing with consistent links to the publications, with the correct author names.  {{% tip %}}   Click on **actions** under the search result to download citations in different styles and formats. {{% /tip %}}  {{% warning %}}   Only publications that have a DOI can be found! {{% /warning %}} "}, {"objectID": "./building-blocks/develop-your-research-skills/tips/stata-code-style.md", "title": "Stata coding style guidelines", "description": "Follow nice Stata coding style to make your code easier to understand and easier to the eye.", "keywords": "style, code, Stata, guidelines, best practices", "code": [], "headers": ["Coding styles in STATA", "1. Linear Format.", "2. Functional Style.", "Shy Functions in Stata", "Merging", "Miscellaneous"], "content": "We use two coding styles in Stata: a linear format for very short or simple scripts, and a functional style for longer or more complex scripts.   ```{stata} ***************************** * Prepare\u00a0data ***************************** * Format\u00a0X variables ...\u00a0 * Format Y variables ... ***************************** * Run\u00a0regressions ***************************** ...\u00a0 ***************************** * Output\u00a0tables ***************************** \u00a0... ``` {{% tip %}} If you include a comment as a header like this for one major block of code, you should include a similar header for every block of code at the same logical place in the hierarchy. This is a case where redundant comments are allowed. Comments are not there to provide information, but to make the code easy to scan. {{% /tip %}}   In the functional style in Stata, we enclose code within program... end blocks. **The first program is always called \u201cmain,\u201d and the .do file always ends with an \u201cExecute\u201d step.**  ```{stata} * PROGRAMS  program\u00a0main \u00a0\u00a0\u00a0\u00a0prepare_data \u00a0\u00a0\u00a0\u00a0run_regressions \u00a0\u00a0\u00a0\u00a0output_tables end  program\u00a0prepare_data \u00a0\u00a0\u00a0\u00a0X\u00a0=\u00a0format_x_vars \u00a0\u00a0\u00a0\u00a0Y\u00a0=\u00a0format_y_vars \u00a0\u00a0\u00a0\u00a0... end  program\u00a0format_x_vars \u00a0\u00a0\u00a0\u00a0... end  program\u00a0format_y_vars \u00a0\u00a0\u00a0\u00a0... end  program\u00a0run_regressions \u00a0\u00a0\u00a0\u00a0... end  program\u00a0output_tables \u00a0\u00a0\u00a0\u00a0... end  *\u00a0EXECUTE main ``` {{% warning %}} The `main` command must come at the end of the script is because Stata (like Python) reads in programs in order of appearance. {{% /warning %}}  In this example, these \u201cfunctions\u201d are really just blocks of code:  - They do not accept any inputs and outputs.   - But within this structure it is easy to add \u201csyntax\u201d commands to individual programs to define inputs and use \u201creturn\u201d calls to pass back outputs.  {{% tip %}}   - Functions in Stata should follow all the usual rules discussed in the [Code and Data for the Social Sciences](http://web.stanford.edu/~gentzkow/research/CodeAndData.pdf) manual for clear, logical abstraction.    - Pay special attention to the input/output structure, making sure your functions increase rather than decrease the readability of code. (Because Stata\u2019s way of passing inputs and outputs is so clunky, it is very easy to end up with lots of long argument lists and local variables that are actually harder to read than a simple linear version of the code.) {{% /tip %}}  Functions should be **shy** (see [Code and Data for the Social Sciences](http://web.stanford.edu/~gentzkow/research/CodeAndData.pdf)), that is, so that they operate only on local variables.   - **Problem:** Data in memory is by definition a global variable in Stata.  {{% example %}}  From the following code:    ```{stata}    use\u00a0x\u00a0y\u00a0z\u00a0using\u00a0autodata.xls,\u00a0clear    prepare_data    update_variables    merge_new_data\u00a0    regress\u00a0productivity\u00a0y_average\u00a0z_average    ```  there is no way to tell what are the inputs and outputs to the `prepare_data`, `update_variables`, and `merge_new_data` functions and no way to tell where the `productivity`, `y_average`, and `z_average` variables came from.  {{% /example %}}  - **Solution:**   - When possible, write functions that only operate on variables that are explicitly passed to the function and do not otherwise touch the data in memory.    - If a function creates new variable(s), the names of these variables can be specified as part of the function call.     - This should always be true of ado files.     - For programs only defined and used within a given .do file, it\u2019s a matter of judgment.          *  By default, we should use Stata\u2019s built-in merge command    *  In older versions of Stata, the merge command was not robust, and we therefore used the add-on command `mmerge` instead. Much of our old code uses mmerge. The only cases we should use mmerge in new code is when we require functionality like the `urename()` option that does not exist in the built-in merge command.  {{% tip %}}      * Guidelines for using `merge`:      *  Always specify the type of merge (`1:1`, `m:1`, or `1:m`). Failing to specify the merge type calls the old, non-robust version of merge.      *  Never do many to many (`m:m`) merges, or at least, only do them when you have a very good reason.      *  Always include the `assert()` option to indicate what pattern of matched observations you expect.      *  Always include the `keep()` option to indicate which observations are to be kept from the merged data set.      *   Whenever possible, include the `keepusing()` option and enumerate explicitly what variables you intend to be adding to the dataset; you can include this option even when you are keeping all the variables from the using data.      *   Use the `nogen` option except when you plan to explicitly use the `_merge` variable later. You should never save a dataset that has `_merge` in it; if you need this variable later, give it a more informative name.      *  You should follow analogous rules for `mmerge`. E.g., you should always specify the type() option. {{% /tip %}}     *  Always use forward slashes (`/`) in file paths. (Stata allows backslashes (`\\`) in file paths on Windows, but these cause trouble on non-Windows OS\u2019s.)    *  Use `preserve`/`restore` sparingly. Excessive use can make code hard to follow; explicitly saving and opening data files is often better.    *  We should always use `save_data`, our custom version of Stata\u2019s save command when saving datasets to /output/. The ado file and its help file can be found in the `/gslab_misc/ado/` directory of our `gslab_stata` GitHub repository. `save_data` replaces both `save` and `outsheet`, the standard Stata commands and requires the user to specify a key for the file. The command confirms that the key(s) are valid, places the keys at the top of the variable list, and sort the file by its key(s). File manifest information is stored in a log file if there is an output folder.    *  When the number of variables needed from a dataset is not too large, list the variables explicitly in the `use` command. "}, {"objectID": "./building-blocks/develop-your-research-skills/tips/principles-good-coding.md", "title": "Principles for Good Coding", "description": "Make your code easy to understand for humans. If your code looks very complex or messy, you're probably doing it wrong.", "keywords": "style, optimize, definition, human", "code": [], "headers": ["Write programs for people, not computers", "Define things once and only once", "Use a version control system", "Optimize software only after it works correctly", "Be a good code citizen", "Keep it short"], "content": "  Make your code easy to understand for humans. If your code looks very complex or messy, you're probably doing it wrong.  - Organization     - Define functions that do one mayor step each, instead of one giant script doing everything     - Write short scripts that do one task each     - Document only what your code **doesn't** say, but nothing else - Style     - Use meaningful and short variable names     - Use consistent code and formatting styles (oneExample, one_example, example-one)     - Make use of indents in your code   Let computers repeat and execute tasks.,  - Rule of 3: if you copy-paste code 3 times or more, write a function instead. - If you do things often, automate them     - e.g., by using scripts, macros, aliases/variables     - write a dictionary with definitions - Use build tools to [automate workflows](/tutorials/project-setup/principles-of-project-setup-and-workflow-management/automation/)   - Add all inputs, but no outputs/generated files     - DO: everything created by humans, small data inputs     - DON'T: things created by the computer from your inputs (generated files; those will be [reproduced via a workflow](/tutorials/project-setup/principles-of-project-setup-and-workflow-management/automation/)). Also [do not version large data inputs](/tutorials/project-setup/principles-of-project-setup-and-workflow-management/directories/). - Work in small changes     - Create [snapshots/commits](/building-blocks/share-your-results-and-project/use-github/versioning-using-git/) in small and logical steps. This will allow you to go back in time if necessary, and to understand progression. -  Use an issue tracking tool to document problems (e.g., such as the *Issue* tab on GitHub; email is not an issue tracker!)   Even experts find it hard to predict performance bottlenecks.  - Get it right, then make it fast - Small changes can have dramatic impact on performance - Use a profile to report how much time is spent on each line of code  - Team members should take the time to improve code they are modifying or extending even if they did not write it themselves. - A core of good code plus a long series of edits and accretions equals bad code.   - Why? The logical structure that made sense for the program when it was small no longer makes sense as it grows.  - It is critical to regularly look at the program as a whole and improve the logical structure through reorganization and abstraction. Programmers call this **refactoring**. Check this [link](https://refactoring.guru/refactoring/) to learn how to implement refactoring.  {{% tip %}} - Even if your immediate task only requires modifying a small part of a program, we encourage you to take the time to improve the program more broadly.  - At a minimum, guarantee that the code quality of the program overall is at least as good as it was when you started. {{% /tip %}} - No line of code should be more than 100 characters long.   - All languages we work in allow you to break a logical line across multiple lines on the page (e.g, using `///` in Stata or `...` in Matlab).  {{% tip %}}    Set your editor to show a \u201cmargin\u201d at 100 characters. {{% /tip %}} - Functions should not typically be longer than 200 lines. "}, {"objectID": "./building-blocks/develop-your-research-skills/tips/links-other-initiatives.md", "title": "Links to Other Great Initiatives", "description": "Here we post some links to material we've discovered recently and find useful.", "keywords": "links, initiatives, style, optimize, definition, human", "code": [], "headers": [], "content": " Here we post some links to material we've discovered recently and find useful.  - [Organizing data in spreadsheets](https://kbroman.org/dataorg/) - Initial steps [towards reproducible science](https://kbroman.org/steps2rr/) - [Minimal example for make](https://kbroman.org/minimal_make/) "}, {"objectID": "./building-blocks/develop-your-research-skills/tips/R-code-style.md", "title": "R coding style guidelines", "description": "Follow nice R coding style to make your code easier to understand and easier to the eye.", "keywords": "style, code, R, guidelines, best practices", "code": [], "headers": ["R"], "content": "We follow the general guidelines in [Google's R Style Guide](https://google.github.io/styleguide/Rguide.xml).  Exceptions: * We do not follow their naming conventions (don't use dots, and underscores or camel-case are fine). * See [conventions for line length](https://github.com/tilburgsciencehub/onboard/wiki/Code#keep-it-short) * We do not require that all functions have a comment block describing their uses, though these are encouraged when the purpose of a function would not be clear or where it will be used outside of the file in which it is defined. "}, {"objectID": "./building-blocks/develop-your-research-skills/tips/Python-coding-style.md", "title": "Python coding style guidelines", "description": "Follow nice language-specific coding styles to make your code easier to understand and easier to the eye.", "keywords": "style, code, python, best practices, guidelines", "code": [], "headers": ["PEP 8", "Supplemental resources:"], "content": "We follow Python's style guide [PEP 8](https://www.python.org/dev/peps/pep-0008/#programming-recommendations). Also we:  * Use docstrings for functions whose purpose may be unclear or that will be used outside of their own modules   * [The Hitchhiker\u2019s Guide to Python](http://docs.python-guide.org/en/latest/), especially the sections on [coding style](http://docs.python-guide.org/en/latest/writing/style/) and [packaging conventions](http://docs.python-guide.org/en/latest/writing/structure/).  * [Google's Python Style Guide](https://google.github.io/styleguide/pyguide.html), especially recommendations concerning [string formatting](https://google.github.io/styleguide/pyguide.html#Strings) and the rule to always explicitly close [files and sockets](https://google.github.io/styleguide/pyguide.html?showone=Files_and_Sockets#Files_and_Sockets).  Additional notes:  * When opening text files for writing or appending text, use [`open`](https://docs.python.org/2/library/functions.html)'s option `mode = \"wb\"` or `mode  =  \"ab\"` respectively to  write in binary mode. This improves portability across operating systems.  * When opening text files for reading, use [`open`](https://docs.python.org/2/library/functions.html)'s option `mode = \"rU\"` to enable universal newline support. "}, {"objectID": "./building-blocks/store-and-document-your-data/documenting-code-and-pipeline.md", "title": "Document your Code and Pipeline", "description": "", "keywords": "", "code": [], "headers": [], "content": ""}, {"objectID": "./building-blocks/store-and-document-your-data/documenting-a-project.md", "title": "Document a Project", "description": "", "keywords": "", "code": [], "headers": [], "content": ""}, {"objectID": "./building-blocks/store-and-document-your-data/store-data/environment-variables.md", "title": "Configure Environment Variables", "description": "Learn how to configure environment variables to store personal credentials and secret keys.", "keywords": "environment variables, configuration, password, secret, credentials", "code": ["  ```python import os # VARIABLE_NAME is the name of the environment variable you defined in the terminal api_password = os.environ['VARIABLE_NAME']    ```  ```R library(Sys) # VARIABLE_NAME is the name of the environment variable you defined in the terminal api_password = Sys.getenv(c(\"VARIABLE_NAME\")) ```  "], "headers": ["Overview <!-- Goal of the Building Block -->", "Configure Environment Variables", "Mac/Linux", "Windows", "Access Environment Variables", "See also"], "content": " When working with APIs or cloud services, you usually need to access some personal credentials or secret keys. With environment variables, we can access such variables without literally writing them down in a notebook or script (e.g., `password = \"...\"`). The basic idea is that these global variables are stored permanently and are attached to your operating system. Therefore, you can access these variables regardless of whether you are working in RStudio or a Jupyter Notebook.  As long as you never print its value, you can safely push a script that references an environment variable to a public Github repository without having to worry about disclosing any login credentials. However, you do need to inform others with whom you're collaborating about its value because it's not contained in the script itself.  {{% warning %}} Never upload a script that contains privacy-sensitive information to Github. Even if you'd remove it afterward, it still remains visible through the repository history.   {{% /warning %}}    1. Go to the terminal and type `printenv` to list all environment variables stored on your machine. 2. Open the terminal, go to your user directory (shortcut: `cd ~`), and type `nano .bash_profile` to open a text editor in the terminal. 3. Within this window you can create new variables as follows: `export [VARIABLE_NAME]=\"the string value you want to store\";`. Note that there is no space between the variable name and its value and that the string is enclosed in double-quotes. 4. Exit the editor by pressing Ctrl + X, choose `Y` (to save changes), and finally press `Enter`. 5. You can check whether everything worked out correctly by restarting your terminal and typing `printenv` (`VARIABLE_NAME` should be listed there now!). If the new environment variables didn't show up, you may need to use `nano .zshrc` instead of `nano .bash_profile` (see step 2).  1. Open up \"Control Panel\" > \"System and Security\" > \"System\". 2. In the left sidebar click on \"Advanced system settings\". 3. Click on \"Environment Variables\" in the bottom right. 4. Create a new \"User Variable\" (top list) and fill out the \"Variable name\" and \"Variable value\". 5. Double click \"OK\" twice.   After you have imported the `os` (or `Sys`) library, you can easily assign the value of the environment variable (`VARIABLE_NAME`) to a variable. Subsequently, you can re-use the variable throughout the script, for example for API authentication purposes.       - [Mac/Linux tutorial](https://www.youtube.com/watch?v=5iWhQWVXosU) - [Windows tutorial](https://www.youtube.com/watch?v=IolxqkL7cD8) "}, {"objectID": "./building-blocks/store-and-document-your-data/store-data/download-data.md", "title": "Download Data Programmatically", "description": "Learn how to download data right from its (online) source and store it locally with code.", "keywords": "download, import data, store, collect", "code": [" ```R download_data <- function(url, filename, filepath) {   # create directory   dir.create(filepath)   # download file   download.file(url = url, destfile = paste0(filepath, filename)) }  download_data(url = \"http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2020-12-12/visualisations/reviews.csv\", filename = \"airbnb_listings.csv\", filepath = \"data/\") ``` ", " ```bash R --vanilla < download.R ``` ", " ```R airbnb <- read.table(\"http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2020-12-12/visualisations/reviews.csv\", sep = ',', header = TRUE) ``` ", " ```bash pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib ``` ", " ```python import io, os from googleapiclient.http import MediaIoBaseDownload from googleapiclient.http import MediaFileUpload from google_drive import create_service  CLIENT_SECRET_FILE = 'client_secret.json' API_NAME = 'drive' API_VERSION = 'v3' SCOPES = ['https://www.googleapis.com/auth/drive']  service = create_service(CLIENT_SECRET_FILE, API_NAME, API_VERSION, SCOPES) ``` ", " ```python request = service.files().get_media(fileId=\"<FILE_ID>\") fh = io.BytesIO()  downloader = MediaIoBaseDownload(fd=fh, request=request)  done = False  while not done:     status, done = downloader.next_chunck()     print(\"Download progress {0}\".format(status.progress() * 100))  fh.seek(0)  with open(\"<FILE_NAME.extension>\", 'wb') as f:     f.write(fh.read())     f.close() ``` ", " ```python file_metadata = {     \"name\": \"<FILE_NAME.extension>\",     \"parents\": [\"<FOLDER_ID>\"] }  media = MediaFileUpload(\"<FILE_NAME.extension>\", mimetype=\"<MIME_TYPE>\")  service.files().create(     body = file_metadata,     media_body = media,     fields='id' ).execute() ``` "], "headers": ["Overview", "Code", "Advanced Use Cases", "Running the download code from the terminal", "Download data to different directories", "Open (rather than download) data", "Downloading data from Google Drive", "Google Cloud Platform", "API Connection", "Download a file", "Upload a file"], "content": "  Download a file from a URL and store it on your local machine. That way, it's super easy for *others* to run your workflow (e.g., team members), or to refresh the data once it's been udpated. All you need to do is rerun your code - that's it!   Here's an example of how to download data from within R.      If you want to download data to work on it in a data pipeline, it's useful to include the download snippet in a source file (e.g., `download.R`). You can then save the script, and run it from the terminal (e.g., as part of a `make` workflow).  In your command line/terminal, you can enter:     Keep in mind that the `filepath` is dependent on the location from where your R script is called. The use of absolute directory names (e.g., `c:/research/project`) should be avoided so that the code remains portable to other computers and work environments.   The code snippet above just *downloads* the data from the web, but does not yet open it in R. If the target data is in tabular format (i.e., has rows and columns), you could directly load it into R using the `read.table` function.     The Google Drive API offers a way to programatically download and upload files through, for example, a Python script. Keep in mind that this only works for files stored in your own Google Drive account (i.e., your own files and those shared with you).   {{% warning %}} Unfortunately, the procedure described in the first code snippet does not work for Google Drive sharing links. The steps below may require some set-up time and technical know-how, but they can be re-used for a variety of cloud services. {{% /warning %}}   Like Amazon and Microsoft, Google offers cloud services (e.g., databases and compute resources) which you can configure through an [online console](https://console.cloud.google.com/home). In Google Cloud Platform you can also enable services like the Google Drive API, which we'll make use of here. Follow the steps below to get started.  {{% tip %}} Google offers a 90-day trial with a \u20ac300 credit to use, but you can keep on using the Drive API after that because it's free to use. In other words, you can following along without filling out any creditcard or debit card details. {{% /tip %}}  1. Sign in to [Google Cloud Platform](https://console.cloud.google.com/home) and agree with the terms of use (if necessary).  2. Click on \"Create New Project\", give it a project name (e.g., `GoogleDriveFiles`), and click on \"Create\".  ![new-project](../images/new_project.png)   3. Next, we need to set-up a so-called OAuth2 client account which is a widely used protocol for authentication and authorization of API services.    * In the left-side bar click on \"APIs & Services\" > \"OAuth consent screen\".    * Set the user type to \"External\" and click on \"Create\".    * Give your app a name (can be anything) and fill out a support and developer email address. Click \"Save and continue\" (it may sometimes throw an app error, then just try again!).    * Click \"Save and continue\" twice and then \"Back to dashboard\".    * Click on \"Publish app\" and \"Confirm\".    * In the left sidebar, click on \"Credentials\" and then \"Create Credentials\" > \"OAuth client ID\" > \"Desktop app\" and click on \"Create\". It will show you your client ID and client secret in a pop-up screen. Rather than copying them from here, we will download a JSON file that contains our credentials. Click on \"OK\" and then on the download symbol: ![download-credentials](../images/download_credentials.png)     * Rename the file to `client_secret.json` and store it in the same folder as the scripts you'll use to download and upload the files.  4.  By default, the Google Drive API is not activated, so search for it in search bar and click on \"Enable\".    5. Download the following [Python script](https://github.com/RoyKlaasseBos/tsh-website/blob/master/content/building-blocks/store-and-document-your-data/store-data/google_drive.py) (\"Raw\" > \"Save as\") and put it in the same directory as the client secret.  6. Run the folllowing command to install the Google Client library:      7. Create a new Python script (or Jupyter Notebook) and run the following code to set-up an API connection.     The first time a new window may pop up that asks you to authenticate yourself with your Google account. Click on \"Allow\".   ![authenticate](../images/authenticate_Drive.png)  Depending on whether you'd like to download or upload a file, follow one of both approaches:    * You can find the `<FILE_ID>` by navigating towards the file in your browser and clicking on \"Open in new window\". The URL then contains the file ID you need. For example, the file ID of `https://drive.google.com/file/d/XXXXXX/view` is `XXXXXX`.   ![open_new_window](../images/open_new_window.png)  * Larger files may be split up in separate chuncks. The progress will be printed to the console.  * To save the file in a subdirectory, you can use the following syntax: `os.path.join(\"./<SUBDIRECTORY>\", \"<FILE_NAME.extension>\")` within the `open()` function.     * The `<FOLDER_ID>` can be obtained in a similar way as the `<FILE_ID`>: navigate towards the folder where you'd like to save the file and look for the identifier within the URL.  * The `MediaFileUpload()` function assumes that the file supposed to be uploaded is stored in the current directory. If not, add the subdirectory in which the file is stored to the path.  * `<MIME_TYPE>` informs Google Drive about the type of file to be uploaded (e.g., `csv`, `jpg`, `txt`). You can find a list with common MIME types over [here](https://learndataanalysis.org/commonly-used-mime-types/). For example, for a csv-file it is: `text/csv`. "}, {"objectID": "./building-blocks/store-and-document-your-data/store-data/aws-s3-buckets.md", "title": "Use AWS S3 Buckets", "description": "This block explains how to list contents, download content from and upload content to a AWS S3 bucket.", "keywords": "aws, s3, boto3, python, bucket", "code": ["  ```python # Import the necessary packages import boto3  # Define credentials and make sure you have reading and writing privileges on AWS user settings your_aws_access_key_id = \"\" # Enter your AWS Access Key your_aws_secret_access_key = \"\" # Enter your AWS Secret Key your_aws_region_name = \"\" # Enter your AWS bucket region   # Create an S3 client s3 = boto3.client(\"s3\",     region_name = your_aws_region_name,     aws_access_key_id = your_aws_access_key_id,     aws_secret_access_key = your_aws_secret_access_key)  your_aws_bucket_name = \"\" # Specify the bucket you use ```  ", "  ```python # List objects in your AWS bucket and print them objects = s3.list_objects(Bucket = your_aws_bucket_name) print(objects) ```  ", "  ```python # List the names of the objects in bucket contents for object in objects[\"Contents\"]:     print(object[\"Key\"]) ```  ", "  ```python # To upload a file # Source: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html filetoupload = open(\"hello.txt\", \"w\")  # Upload the file to S3 s3.upload_file(filetoupload, your_aws_bucket_name, \"remote-object-name.txt\") ```  ", "  ```python # Download a file # Source: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-download-file.html s3.download_file(your_aws_bucket_name, \"remote-object-name.txt\", \"local-file-name.txt\") ```  "], "headers": ["Overview <!-- Goal of the Building Block -->", "Code", "Establish a connection", "List objects in your AWS bucket and print them", "List the names of the objects in bucket contents", "Upload a file", "Download a file", "See also"], "content": "  [Amazon Simple Storage Service](https://aws.amazon.com/s3/) (AWS S3) is an industry-leader object storage service that allows you to store and retrieve any kind of files fast, securely and anywhere in the world - basically Dropbox on steroids. It can be used for a range of use cases, from websites to backup and archives. It can also be particularly useful for research projects - storing a huge amount of raw data, for instance.  If you are using AWS S3 to store data necessary for your research and want to incorporate downloading and uploading your data files to AWS S3 into a [`make`](/building-blocks/configure-your-computer/automation-and-workflows/make/) script, it is useful to use a code script to interact with your AWS S3 bucket instead of the command line. Keep reading to learn how to do it.  {{% tip %}} **Some terminology**  You will often read about AWS S3 \"buckets\". What are they? Put simply, a bucket is a container for objects (files) stored in Amazon S3. Every object must be contained in a bucket. A single user can have as many buckets as they want. Inside a bucket, you can create as many folders you want. {{% /tip %}}  {{% warning %}} You need to have access to an AWS S3 bucket and have the credentials to run the following code.  Whenever you interact with AWS, you must specify your security credentials to verify who you are and whether you have permission to access the resources that you are requesting. For instance, if you want to download a file from an AWS S3 bucket, your credentials must allow that access.  Learn more about AWS credentials [here](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html). {{% /warning %}}   We show you how to very simply upload and download files to a S3 bucket with [boto3]().  First off, install boto3 - the AWS SDK for Python - via [pip](/building-blocks/configure-your-computer/statistics-and-computation/python-packages/):  ```bash pip install boto3 ```             - Learn more about [AWS S3](https://aws.amazon.com/s3) - Learn more about [`boto3`](https://aws.amazon.com/s3), the AWS SDK for Python - Learn more on how to upload files with [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html) - Learn more on how to download files with [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-download-file.html)  {{% summary %}}  In this article, we showed you how to use Python to automate your uploads to and downloads from AWS S3. Hopefully, this is helpful to you.  {{% /summary %}} "}, {"objectID": "./building-blocks/store-and-document-your-data/store-data/choose-a-data-repository.md", "title": "Choose a Data Repository for Long-Term Storage", "description": "Part of responsible research is making sure your research data is properly stored. But how do you decide which data to keep, document and where to archive it?", "keywords": "archiving, data archiving, dataverse, zenodo, figshare, osf, dans", "code": [], "headers": ["Data Archiving", "Licensing Your Data", "Choosing a Data Repository", "Publishing Your Research Data", "See Also"], "content": "  Part of responsible research is making sure your research data are properly managed, both during your research as well as when your research is finished (and hopefully published). For replication purposes, the data needs to remain available for a minimum of 10 years. But how do you decide which data to keep, document and where to archive it?  This [tutorial on data management](https://tilburgsciencehub.com/tutorials/project-management/principles-of-project-setup-and-workflow-management/directories/#data-management-for-each-of-the-projects-components) and this [article on documenting your data](https://tilburgsciencehub.com/document/new-data) provide some insights.  Research data archiving is about storing and preserving research data for the long term. When you archive your data, you make sure you can read and access the data later on. Archiving your data does not necessarily mean your data is available to others, you can limit access and license your data if you want. You should store your data safely, in a suitable file format, with adequate documentation.   Attaching a usage license to your data tells others what they can or cannot do with the data. **[Creative Commons](https://creativecommons.org/licenses/https://creativecommons.org/licenses/)** licenses are widely used, for example a CC-BY license lets other distribute, re-use, mix, build upon your work as long as they give you credit. Adding NC to the license limits the use to non-commercial use only.  Another resource for selecting a license is [choosealicense.com](https://choosealicense.com/).  ![Licensing you data.](../images/licensing-data.png)   A data repository is a digital archive collecting, preserving and displaying datasets, related documentation, and metadata.  There are more than 1500 data repositories available to archive your research data, how do you know which one to choose? Sometimes journals or funders recommend a repository, a discipline specific repository might be commonly used, or maybe your university recommends a repository. For example, at Tilburg University, **[Dataverse](https://dataverse.nl/dataverse/tiu)** is recommended and the Research Data Office provides support depositing your data if desired.  Some well know repositories, besides Dataverse, are:  - **[Zenodo](https://zenodo.org/)** - **[FigShare](https://figshare.com/)** - **[Open Science Framework (OSF)](https://osf.io/)** - **[DANS EASY](https://easy.dans.knaw.nl/ui/home)**  Trustworthy repositories should meet the following minimum criteria[^1]:  1.\tProvision of Persistent and Unique Identifiers (PIDs)     - Allow data discovery and identification     - Enable searching, citing, and retrieval of data     - Provide support for data versioning 2.\tMetadata     - Enable finding of data     - Enable referencing to related relevant information, such as other data and publications     - Provide information that is publicly available and maintained, even for non-published, protected, retracted, or deleted data     - Use metadata standards that are broadly accepted (by the scientific community)     - Ensure that metadata are machine-retrievable 3.\tData access and usage licences     - Enable access to data under well-specified conditions     - Ensure data authenticity and integrity     - Enable retrieval of data     - Provide information about licensing and permissions (in ideally machine-readable form)     - Ensure confidentiality and respect rights of data subjects and creators 4.\tPreservation     - Ensure persistence of metadata and data     - Be transparent about mission, scope, preservation policies, and plans (including governance, financial sustainability, retention period, and continuity plan)  A well known certification for a trusted repository is for example [CoreTrustSeal](https://www.coretrustseal.org/).   If you want to make your data reusable for purposes beyond the one for which you collected them, you should publish your data.  Publishing your data is the act of publicly disclosing the research data you have collected, making them findable, accessible, interoperable and reusable (FAIR data).  There are multiple reasons to publish your data: -\tData publication may lead to increased visibility, reuse and citation and therefore recognition of scholarly work. -\tData archiving and publication has direct benefits for the research itself (more robust), for the discipline and for science in general by enabling new collaborations, new data uses and establishing links to the next generation of researchers. -\tThe openness of research data is at the heart of scientific ethics. -\tExternal drivers like research data management policies from research funders and publishers might require data archiving and publication.  {{% tip %}} Be sure to archive/publish only data you are allowed to archive/publish. Often archives do not allow sensitive data or non-anonymized data to be archived. And if you want to share your data, make sure (if relevant) the participants agreed to this when you collected the data (consent). {{% /tip %}}   - **[OpenAIRE Guide](https://zenodo.org/record/4077212#.YGHVqK8zY2z)** on how to find a trustworthy repository for your data - **[R3Data.org searchable database](https://www.re3data.org/search?query=)** of data repositories  --- This document was created with information provided by the Consortium of European Social Science Data Archives (CESSDA) and Kars Wijnhoven, [Research Data Office](mailto:rdo@tilburguniversity.edu) at Tilburg University.  CESSDA Training Team (2017 - 2020). CESSDA Data Management Expert Guide. Bergen, Norway: CESSDA ERIC. Retrieved from [cessda.eu](https://www.cessda.eu/DMGuide).  [^1]:     From [Science Europe](https://scienceeurope.org/media/4brkxxe5/se_rdm_practical_guide_extended_final.pdf). "}, {"objectID": "./building-blocks/store-and-document-your-data/document-data/readme-best-practices.md", "title": "README Best Practices", "description": "Learn how to write a convincing and effective README file.", "keywords": "git, commands, important, essential, cheat", "code": [" ```markdown # Project title  A subtitle that describes your project, e.g., research question   ## Motivation  Motivate your research question or business problem. Clearly explain which problem is solved.   ## Method and results  First, introduce and motivate your chosen method, and explain how it contributes to solving the research question/business problem.  Second, summarize your results concisely. Make use of subheaders where appropriate.   ## Repository overview  Provide an overview of the directory structure and files, for example:  \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u251c\u2500\u2500 gen \u2502\u00a0\u00a0 \u251c\u2500\u2500 analysis \u2502\u00a0\u00a0 \u251c\u2500\u2500 data-preparation \u2502\u00a0\u00a0 \u2514\u2500\u2500 paper \u2514\u2500\u2500 src     \u251c\u2500\u2500 analysis     \u251c\u2500\u2500 data-preparation     \u2514\u2500\u2500 paper   ## Running instructions  Explain to potential users how to run/replicate your workflow. If necessary, touch upon the required input data, which secret credentials are required (and how to obtain them), which software tools are needed to run the workflow (including links to the installation instructions), and how to run the workflow.   ## More resources  Point interested users to any related literature and/or documentation.   ## About  Explain who has contributed to the repository. You can say it has been part of a class you've taken at Tilburg University.  ``` "], "headers": ["Overview", "Markdown", "The Basic Structure", "Motivation", "Method and results", "Repository overview", "Running instructions", "More resources", "About", "The README Template", "Examples", "Advanced Use Cases"], "content": " A README is like the book cover of your project. It's the first thing a person sees when opening your repository. A great README not only gets people to jump into your project much quicker, but also helps your project to stand out from the sea of open source software on Github. Your README thus not only serves for documentation, but also for marketing purposes.  And while we all loathe sheazy marketing, documentation can't be sleazy because it solves a real purpose: teaching everyone about the project. In this building block, we provide you with a template and some examples you can use for your own projects.    A README is a markdown (`.md`) file that you can format text using a a plain-text editor. Like an academic paper, we recommend working with headers and subheaders to impose a structure. Better still, if you link to other files within the repository so that the reader not only knows what the project is about but also which files are a priority.  {{% tip %}} Below we list the most common markdown commands:  * `**This is bold text**` = **This is bold text** * `*This text is italicized*` = *This is bold text* * `This is a [link](https://tilburgsciencehub.com)` = This is a [link](https://tilburgsciencehub.com) * To create a heading, add 1-6 `#` symbols before your header. The number of hashtags will determine the size of the heading. * Images can be inserted by linking to either a image URL (e.g., [example](https://www.tilburguniversity.edu/sites/default/files/styles/epic_compact_large/public/image/TilburgU%20logo.jpg?h=f0edcced&itok=lnj4S1OC)) or a relative filepath ([`../git_workflow.png`](../git_workflow.png)). Use the following syntax: `![image description](link)` * Visit [this](https://docs.github.com/en/github/writing-on-github/basic-writing-and-formatting-syntax) cheatsheet for a comprehsensive list of markdown commands. {{% /tip %}}   We recommend to at least include the following sections in your README:     {{% tip %}}  Rather than creating the repository overview all by hand, you can leverage the `tree` command to automatically generate the directory structure in the terminal. Mac users may first need to install the [tree package](https://formulae.brew.sh/formula/tree) (`brew install tree`).  {{% /tip %}}   We provide a more comprehensive README template - which you can preview in the image below - that follows best practices defined by a number of data editors at social science journals. You can read here the [full list of endorsers](https://social-science-data-editors.github.io/template_README/Endorsers.html).  You can always access **[the most recent version of this template here](https://social-science-data-editors.github.io/template_README/)** or download them quickly:  {{% cta-primary-center \"Download the Markdown version\" \"https://raw.githubusercontent.com/social-science-data-editors/template_README/releases/README.md\" %}} {{% cta-secondary-center \"Download the Word version\" \"https://social-science-data-editors.github.io/template_README/templates/README.docx\" %}}  ![A preview of the README template](../preview-readme-template.png)   The repositories below serve as examples from which you can draw inspiration for your own README files.  * [Tilburg Science Hub](https://github.com/tilburgsciencehub/website) * [Categorization of Spotify Playlists](https://github.com/hannesdatta/spotify-playlist-clustering) * [Hiding of Instagram Likes](https://github.com/RoyKlaasseBos/Hiding-Instagram-Likes) * [musicMetadata](https://github.com/hannesdatta/musicMetadata)    By default Github showcases your pinned repositories on your profile page (click on your profile picture in the top right corner > \"Your profile\"). A little secret is that you can add a README to your profile page by creating a new repository called `<YOUR_USERNAME>`. Make sure it's public and initialize it with a README to get started. As you can see in [this](https://www.youtube.com/watch?v=Y1z7_GfEPiE) video, you can even spice things up with emojis and gifs!  {{% tip %}} Want to go the extra mile? Include your [GitHub Stats Card](https://github.com/anuraghazra/github-readme-stats) in your README! Simply add `https://github-readme-stats.vercel.app/api?username=<YOUR_USERNAME>` to the end of your README to incorporate a realtime widget of your number of stars, commits, PRs, issues, and contributions on Github ([see example](https://github-readme-stats.vercel.app/api?username=hannesdatta)). {{% /tip %}} "}, {"objectID": "./building-blocks/store-and-document-your-data/document-data/documenting-new-data.md", "title": "Document New Data", "description": "If your project contains data that has been newly created, learn how to include a documentation of that data in your project.", "keywords": "document, data, readme, raw, database, description", "code": ["  ```txt ==========================================================   D A T A S E T / D A T A B A S E  D E S C R I P T I O N ==========================================================  (template based on https://arxiv.org/abs/1803.09010)   * Name of the dataset/database:     ========================================================== 1. MOTIVATION ==========================================================  1.1  For what purpose was the dataset created?      Was there a specific task in mind? Was there \t\t a specific gap that needed to be filled?      Please provide a description.  1.2  Who created this dataset      (e.g., which team, research group) and on behalf of \t\t which entity (e.g., company, institution, organization)?  1.3  Who funded the creation of the dataset?      If there is an associated grant, please provide \t\t the name of the grantor and the grant name and number.  1.4  Any other comments?  ========================================================== 2. COMPOSITION ==========================================================  2.1  What do the instances that comprise the dataset represent      (e.g., documents, photos, people, countries)?      Are there multiple types of instances (e.g., movies, \t\t users, and ratings; people and interactions between them; \t\t nodes and edges)?      Please provide a description.  2.2  How many instances are there in total      (of each type, if appropriate)?  2.3  Does the dataset contain all possible instances or is it a sample      (not necessarily random) of instances from a larger set?      If the dataset is a sample, then what is the larger set?      Is the sample representative of the larger set \t\t (e.g., geographic coverage)? If so, please describe how this \t\t representativeness was validated/verified.      If it is not representative of the larger set, please describe why not      (e.g., to cover a more diverse range of instances, because      instances were withheld or unavailable).  2.4  What data does each instance consist of?      \"Raw\" data (e.g., unprocessed text or images)      or features? In either case, please provide a description.  2.5  Is there a label or target associated with each instance?      If so, please provide a description.  2.6  Is any information missing from individual instances?      If so, please provide a description, explaining why this information is      missing (e.g., because it was unavailable). This does not include \t\t intentionally removed information, but might include, e.g., redacted text.  2.7  Are relationships between individual instances made      explicit (e.g., users' movie ratings, social network links)?      If so, please describe how these relationships are made explicit.  2.8  Are there recommended data splits (e.g., training, development/validation,      testing)?      If so, please provide a description of these splits, explaining the      rationale behind them.  2.9  Are there any errors, sources of noise, or redundancies in the dataset?      If so, please provide a description.  2.10 Is the dataset self-contained, or does it link to or otherwise rely on      external resources (e.g., websites, tweets, other datasets)?      If it links to or relies on external resources,      a) are there guarantees that they will exist, and remain constant, \t\t over time;      b) are there official archival versions of the complete dataset      (i.e., including the external resources as they existed at the      time the dataset was created);      c) are there any restrictions (e.g., licenses, fees) associated with      any of the external resources that might apply to a future user?      Please provide descriptions of all external resources and any restrictions      associated with them, as well as links or other access points, as \t\t appropriate.  2.11 Does the dataset contain data that might be considered confidential      (e.g., data that is protected by legal privilege or by doctor-patient      confidentiality, data that includes the content of individuals'      non-public communications)?      If so, please provide a description.  2.12 Does the dataset contain data that, if viewed directly, might be offensive,      insulting, threatening, or might otherwise cause anxiety?      If so, please describe why.  2.13 Does the dataset relate to people?      If not, you may skip the remaining questions in this section.  2.14 Does the dataset identify any subpopulations (e.g., by age, gender)?      If so, please describe how these subpopulations are identified and      provide a description of their respective distributions within the dataset.  2.15 Is it possible to identify individuals (i.e., one or more natural persons),      either directly or indirectly (i.e., in combination with other data)      from the dataset?      If so, please describe how.  2.16 Does the dataset contain data that might be considered sensitive in      any way (e.g., data that reveals racial or ethnic origins, sexual      orientations, religious beliefs, political opinions or union memberships,      or locations; financial or health data; biometric or genetic data;      forms of government identification, such as social security numbers;      criminal history)?      If so, please provide a description.  2.17 Any other comments?  ========================================================== 3. COLLECTION PROCESS ==========================================================  3.1  How was the data associated with each instance acquired?      Was the data directly observable (e.g., raw text, movie ratings),      reported by subjects (e.g., survey responses), or indirectly \t\t inferred/derived from other data (e.g., part-of-speech tags, model-based \t\tguesses for age or language)? If data was reported by subjects or indirectly      inferred/derived from other data, was the data validated/verified?      If so, please describe how.  3.2  What mechanisms or procedures were used to collect the data      (e.g., hardware apparatus or sensor, manual human curation, \t\t software program, software API)?      How were these mechanisms or procedures validated?  3.3  If the dataset is a sample from a larger set, what was the sampling strategy      (e.g., deterministic, probabilistic with specific sampling probabilities)?  3.4  Who was involved in the data collection process (e.g., students, \t   crowdworkers, contractors) and how were they compensated (e.g., how \t\t much were crowdworkers paid)?  3.5  Over what timeframe was the data collected? Does this timeframe      match the creation timeframe of the data associated with the      instances (e.g., recent crawl of old news articles)?      If not, please describe the timeframe in which the data associated with the      instances was created.  3.6  Were any ethical review processes conducted (e.g., by an institutional      review board)?      If so, please provide a description of these review processes, including      the outcomes, as well as a link or other access point to any      supporting documentation.  3.7  Does the dataset relate to people?      If not, you may skip the remainder of the questions in this section.  3.8  Did you collect the data from the individuals in question directly,      or obtain it via third parties or other sources (e.g., websites)?  3.9  Were the individuals in question notified about the data collection?      If so, please describe(or show with screenshots or other information) how      notice was provided, and provide a link or other access point to,      or otherwise reproduce, the exact language of the notification itself.  3.10 Did the individuals in question consent to the collection and use of their      data?      If so, please describe (or show with screenshots or other information)      how consent was requested and provided, and provide a link or other access      point to, or otherwise reproduce, the exact language to which the      individuals consented.  3.11 If consent was obtained, were the consenting individuals provided with a      mechanism to revoke their consent in the future or for certain uses?      If so, please provide a description, as well as a link or other access      point to the mechanism (if appropriate).  3.12 Has an analysis of the potential impact of the dataset and its use on data      subjects (e.g., a data protection impact analysis)been conducted?      If so, please provide a description of this analysis, including the \t\t outcomes, as well as a link or other access point to any supporting \t\t documentation.  3.13 Any other comments?  ========================================================== 4. PREPROCESSING/CLEANING/LABELING ==========================================================  4.1  Was any preprocessing/cleaning/labeling of the data done (e.g.,  \t   discretization or bucketing, tokenization, part-of-speech tagging, \t\t SIFT feature extraction, removal of instances, processing of \t\t missing values)? If so, please provide a description. If not, you may skip \t\t the remainder of the questions in this section.  4.2  Was the \"raw\" data saved in addition to the      preprocessed/cleaned/labeled data (e.g., to support unanticipated \t\t future uses)? If so, please provide a link or other access point to \t\t the \"raw\" data.  4.3  Is the software used to preprocess/clean/label the instances available?      If so, please provide a link or other access point.  4.4  Any other comments?   ========================================================== 5. USES ==========================================================  5.1  Has the dataset been used for any tasks already?      If so, please provide a description.  5.2  Is there a repository that links to any or all papers or systems      that use the dataset?      If so, please provide a link or other access point.  5.3  What (other) tasks could the dataset be used for?  5.4  Is there anything about the composition of the dataset or the way it was \t\t collected and preprocessed/cleaned/labeled that might impact future uses? \t\t For example, is there anything that a future user might need to know to \t\t avoid uses that could result in unfair treatment of individuals or groups \t\t (e.g., stereotyping, quality of service issues) or other undesirable harms \t\t (e.g., financial harms, legal risks) If so, please provide a description. \t\t Is there anything a future user could do to mitigate these undesirable \t\t harms?  5.5  Are there tasks for which the dataset should not be used?      If so, please provide a description.  5.6  Any other comments?  ========================================================== 6. DISTRIBUTION ==========================================================  6.1  Will the dataset be distributed to third parties outside of the entity      (e.g., company, institution, organization) on behalf of which the      dataset was created?      If so, please provide a description.  6.2  How will the dataset will be distributed(e.g.,tarball on website, API, \t   GitHub)? Does the dataset have a digital object identifier (DOI)?  6.3  When will the dataset be distributed?  6.4  Will the dataset be distributed under a copyright or other intellectual      property(IP) license, and/or under applicable terms of use (ToU)?      If so, please describe this license and/or ToU, and provide a link or other      access point to, or otherwise reproduce, any relevant licensing terms or \t\t ToU (Terms of Use), as well as any fees associated with these restrictions.  6.5  Have any third parties imposed IP-based or other restrictions on the      data associated with the instances?      If so, please describe these restrictions, and provide a link or other      access point to, or otherwise reproduce, any relevant licensing terms,      as well as any fees associated with these restrictions.  6.6  Do any export controls or other regulatory restrictions apply to the      dataset or to individual instances?      If so, please describe these restrictions, and provide a link or other      access point to, or otherwise reproduce, any supporting documentation.  6.7  Any other comments?  ========================================================== 7. MAINTENANCE ==========================================================  7.1  Who is supporting/hosting/maintaining the dataset?  7.2  How can the owner/curator/manager of the dataset be contacted      (e.g., email address)?  7.3  Is there an erratum?      If so, please provide a link or other access point.  7.4  Will the dataset be updated (e.g., to correct labeling errors, add      new instances, delete instances)?      If so, please describe how often, by whom, and how updates will      be communicated to users (e.g., mailing list, GitHub)?  7.5  If the dataset relates to people, are there applicable limits on the      retention of the data associated with the instances      (e.g., were individuals in question told that their data would be retained \t   for a fixed period of time and then deleted)?      If so, please describe these limits and explain how they will be enforced.  7.6  Will older versions of the dataset continue to be      supported/hosted/maintained?      If so, please describe how. If not, please describe how its obsolescence      will be communicated to users.  7.7  If others want to extend/augment/build on/contribute to the dataset,      is there a mechanism for them to do so?      If so, please provide a description. Will these contributions be      validated/verified?      If so, please describe how. If not, why not? Is there a process for      communicating/distributing these contributions to other users?      If so, please provide a description.  7.8  Any other comments?   ``` ", "  ```txt ==========================================================           D A T A S E T   D E S C R I P T I O N ==========================================================  Name of the dataset:  ----------------------------------------------------------  1. Motivation of data collection (why was the data collected?)  [...]  2. Composition of dataset (what's in the data?)  [...]  3. Collection process (how was the data collected?)  [...]  4. Preprocessing/cleaning/labeling (how was the data cleaned, if at all?)  [...]  5. Uses (how is the dataset intended to be used?)  [...]  6. Distribution (how will the dataset be made available to others?)  [...]  7. Maintenance (will the dataset be maintained? how? by whom?)  [...] ``` "], "headers": ["Overview", "Documentation Template", "A Shorter Version"], "content": "  Ideally, your data description includes the *very elaborate questions* outlined in `Datasheets for datasets` by Gebru, Timnit, et al. (2018)[^1]. We **strongly** refer you to the [original paper](https://arxiv.org/abs/1803.09010), which explains in detail the seven key ingredients of a proper dataset documentation.  Below, we have reproduced these questions, and we recommend you to include those as a **`readme.txt`**, together with your datasets. For *derived* data, it may be enough to point to a relevant source code file, and provide a list of variables and their operationalization.  You can download a formatted version (`.docx`) of this template using the button below. Alternatively, you can find a plain text version of it for copy & paste below.  {{% cta-primary \"Download the template\" \"../Datasheets_for_DataSets.docx\" %}}  <!-- {{% cta-secondary \"Download the template\" \"../Datasheets_for_DataSets.docx\" %}} -->      That's **a lot of documentation**. So - if you don't have time, go with the bigger picture and answer the main questions only.    [^1]:     Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Daum\u00e9 III, H., & Crawford, K. (2018). Datasheets for datasets. arXiv preprint [arXiv:1803.09010](https://arxiv.org/abs/1803.09010). "}, {"objectID": "./building-blocks/configure-your-computer/task-specific-configurations/configuring-python-for-webscraping.md", "title": "Configuring Python for Web Scraping", "description": "Sometimes when we scrape the web, we need to automate our computer to open a web browser to gather information from each page.", "keywords": "web scraping, scraping, automation, browser, chromedriver", "code": [], "headers": ["Windows Users", "Mac Users", "Let's install Homebrew first!", "Let's proceed to installing Chromedriver", "Linux Users"], "content": " # Web Scraping Using an Automated Browser  Sometimes when we scrape the web, we need to automate our computer to open a web browser to gather information from each page. This is especially true when the site we want to scrape has content that is loaded dynamically with javascript.  We will install one package to help us here: [ChromeDriver](https://chromedriver.chromium.org).  Installing this stuff is operating-system specific, hence so are the instructions below.   Watch our YouTube video, in which we walk you through the setup on Windows.  {{< youtube l2aRxtYN3eY iframe-video-margins >}}  *   Install Google Chrome from [here](https://www.google.com/chrome/browser/desktop/index.html). *   Download the windows version of Chromedriver from [here](https://chromedriver.storage.googleapis.com/index.html?path=2.41/). *   Extract the contents from the zip file, and extract them into a new directory under `C:\\chromedriver`. If you do not have admin rights, you can put the file also in another folder, for example `C:\\Program Files\\chromedriver`, or `C:\\Users\\[your-username]\\chromedriver`. It does not matter where exactly the file will be put, as long as you remember where it is (it's not a good idea though to leave it in your downloads folder). *   Make sure that the chromedriver.exe file is directly under the PATH you specified, i.e. under `C:\\chromedriver` (or an alternative path). If your zip unpacker created a new folder with a different name inside your specified folder, move the .exe file to that path. *   Add the directory `C:\\chromedriver` (or whichever directory you chose above) to your PATH as described before (for instructions, see below) *   If this went successfully, open a terminal/command prompt, and enter `chromedriver --version`, you should get output that looks like `ChromeDriver [version number]`  {{% warning %}} **Making `chromedriver` available via the PATH settings on Windows.**  We need to update our PATH settings; these settings are a set of directories that Windows uses to \"look up\" software to startup.  - Open the settings for environment variables   - Right-click on Computer.   - Go to \"Properties\" and select the tab \"Advanced System settings\".   - Choose \"Environment Variables\" - Alternatively, type \"environment variable\" (Dutch: omgevingsvariabelen) in your Windows 10 search menu, and press Enter.  -  Select `Path` from the list of user variables. Choose `Edit`. - **Windows 7 and 8 machines:** \tIf you chose your installation directory to be `C:\\chromedriver` during your installation (i.e., you did use the default directory), copy and paste the following string without spaces at the start or end:          `;C:\\chromedriver`  - **Windows 10 machines:** \t- Click `New` and paste the following string:          `C:\\chromedriver`  \t- Click on `OK` as often as needed. {{% /warning %}}    Make sure your `Homebrew` package is up-to-date. To do so, open a terminal and enter  ``` brew update ```  If that returns an error, `Homebrew` is not installed.  - To install Homebrew, open a terminal and paste the following command:  ``` /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\" ```  - To verify that Homebrew installed correctly, enter the following into your terminal  ``` brew doctor ```  ...and you should see the following output  ``` Your system is ready to brew ```  Sometimes, `brew doctor` returns some warnings. While it's advisable to fix them (eventually), you typically don't have to do it to get started with Chromedriver - so just try to continue from here.   * We assume you have Google Chrome installed. If not, do this first, please.  *   Install `chromedriver` via Homebrew:  ``` brew cask install chromedriver ```  *   Verify your install, by entering the following in your terminal. The expected output is `ChromeDriver XX`  ``` chromedriver --version ```   *   Open a terminal session *   Install Google Chrome for Debian/Ubuntu by pasting the following and then pressing `Return` ``` sudo apt-get install libxss1 libappindicator1 libindicator7 wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb  sudo dpkg -i google-chrome*.deb sudo apt-get install -f ```  *   Install `xvfb` so chrome can run 'headless' by pasting the following and then pressing `Return` ``` sudo apt-get install xvfb ```  *   Install Chromedriver by pasting the following and then pressing `Return`: ``` sudo apt-get install unzip  wget -N https://chromedriver.storage.googleapis.com/2.41/chromedriver_linux64.zip unzip chromedriver_linux64.zip chmod +x chromedriver  sudo mv -f chromedriver /usr/local/share/chromedriver sudo ln -s /usr/local/share/chromedriver /usr/local/bin/chromedriver sudo ln -s /usr/local/share/chromedriver /usr/bin/chromedriver ``` *   Your install worked, you should get `ChromeDriver XX` returned if the installation was successful ``` chromedriver --version ``` "}, {"objectID": "./building-blocks/configure-your-computer/infrastructure-choice/infrastructure-requirements.md", "title": "Learn about the various ways to setup a research infrastructure", "description": "The main considerations before launching any empirical research project.", "keywords": "collaboration, security, data management, run-time, data storage, computation, code versioning", "code": [], "headers": ["Overview", "5 Step Plan", "1. Will you collaborate with others on writing code?", "2. What\u2019s the technical proficiency of your team members?", "3. What minimum security levels do you have to ensure? Can you make your code public?", "4. How will you manage your data?", "5. How long will the workflow run?", "See Also"], "content": "  Before diving right into the nitty gritty details, there are a couple of questions that you should always ask yourself at the start of any empirical research project. Here we present to you a 5-step plan that outlines these main considerations in a structured manner.  {{% tip %}} As with many of the workflow procedures, we realize that it may come across as a series of redundant steps. And truth be told, it probably does take more time before you experience the benefits. However, we also believe that you should not reinvent the wheel each and every time you launch a new project. In fact, these ideas may become an integral part of your workflow as you get more experience with it. {{% /tip %}}    If you do, using a version control system like Git is a must-have, as you need to be able to work on a project simultaneously, without running the risk of overwriting each other\u2019s work.  {{% example %}}  Unlike Google Docs, where you can work on the same file at the same time, in a code editor you tend to work independently from one another. Of course, it can still happen that one of your co-editors and you make changes to same file. For example, you may use the variable name `aggregated_df`, while your team member referred to it as `df`. In these cases, there is not a good or bad answer per se (both may work totally fine!). Yet, in the end, we do need to be consistent with how we reference variables or our script will likely break at some point.  To prevent his from happening, Git informs you about so-called *merge-conflicts*. That is, it indicates where two versions of the same file - that are about to be merged into one - differ from one another. Then, you can make the necessary changes (i.e., choose one of either approaches) and proceed with the merging procedure.  {{% /example %}}   It\u2019s not uncommon that your team members may be unfamiliar with tools like Git, make, or the terminal. First, that\u2019s not a problem at all. You can stay in charge of the main workflow, and integrate others\u2019 work as it is being updated (e.g., on Dropbox). However, it\u2019s way better to help team members develop the skills to use Git and automation, just to name a few.  {{% example %}} To illustrate why this is important, we should note that in any academic project a researcher has certain degrees of freedom. How are variables operationalized, on which level is the data aggregated, are outliers excluded (or not), just to name a few. As the number of these decisions tends to add up quickly, it is essential that not only you can reproduce your own results, but also that others can do so and critique your choices. This stands or falls, however, by your collaborators' technical proficiency to get the tools up and running, which is why this is an important infrastructure consideration. {{% /example %}}   Although the reproducible science community strongly advocates for transparency and openness, there are circumstances in which you are simply constrained by regulations, and you may even be required to take additional measures to prevent data leakage. This, in turn, affects how you should configure your tools and which safety measures you need to take.  {{% example %}} Industrial partners may ask you to sign a Non-Disclosure Agreement (NDA) that enforces you to keep the supplied data private. As data leakage can severely harm a company's reputation, this comes with a great responsiblity on your end.  For example, you may need to use a private (instead of a public) repository, configure 2-factor authentication for your Github account, and use environmental variables as placeholders in your script rather than hard-coded login credentials of databases or APIs). {{% /example %}}   Related to the previous consideration, some data management solutions are easier to implement than others. Cloud storage services like Dropbox and Google Drive may be easy to get started with as most people are familiar with them nowadays, yet they have their own set of limitations.  {{% example %}} Although most cloud services offer some free data storage, you may quickly reach its limits. In particular for longitudinal studies or web scraping projects, this is an important aspect to consider upfront. After all, you don't want to switch up your data storage in the middle of your project because you run out of space.  In addition to size limitations, there are other reasons to choose for a more dedicated data management solution. For example, you may want to enforce a data type (e.g., values in the `id` column can only be integers), link multiple tables together (e.g., normalized database schema), or GDPR regulations require you to rely on legacy systems (e.g., on-premise database) - as opposed to cloud solutions - to obtain the data you're after.  At the end of this building block, we provide some links to resources that help you get started with structured (e.g., `SQL database`) and unstructured (e.g., `JSON` files) data management solutions. {{% /example %}}     Even though importing a dataset and running a multitude of regression models typically happens with a matter of seconds, you may encounter circumstances in which you need to factor in the expected run time already in the project set-up.  For example, if you need to throttle API calls, experiment with a variety of hyperparameters, or run a process repeatedly (e.g., web scraping) the hardware of your local system may not suffice. Also, keeping your system up and running all day long (e.g., throughout the nights) is typically not desirable. In these cases, creating a virtual instance (i.e., a computer in the cloud that you can control) adjusted to your specific needs (e.g., more memory than your own machine) can overcome runtime issues.   {{% summary %}}  Together, these 5 considerations can guide your decision-making in terms of (a) code versioning, (b) raw data storage, and (c) computation (local vs remote).  {{% /summary %}}   * If you're unfamiliar with Git, you may first want to take a more comprehensive look at its features to better evaluate the pros and cons. [Here](/get/git) we explain how to install Git and create an account. [This](/use/git) building block illustrates how to get started with Git repositories.   * Amazon Simple Storage Service (often referred to as S3) provides an alternative way to store data into buckets. [This](/use/aws-s3) building block summarizes the main commands you need to download and upload files. * [This](https://www.youtube.com/watch?v=pd-0G0MigUA) video tutorial shows how to leverage Python SQLite to create a database, table, and run queries on a local SQL database. The same ideas can be applied to cloud databases as well. "}, {"objectID": "./building-blocks/configure-your-computer/infrastructure-choice/getting-started-research-cloud.md", "title": "Getting started with Research Cloud", "description": "Free cloud solution for conducting research", "keywords": "cloud, virtual computers, SURF, infrastructure, parallel, research cloud, service desk", "code": [], "headers": ["Overview", "How to get started", "Advanced use cases", "Adding collaborators", "Usage of Research Drive in combination with Research Cloud"], "content": "  Research Cloud is a highly customizable cloud solution for conducting your research. Think of it as a \u201cfree\u201d version of Amazon EC2 or Microsoft Azure machine, but then brought to you under the Dutch National e-Infrastructure.   -  **What can you do with it?**  Start virtual computers that run R or Python, attach some storage space, and share these computers with co-authors.   -  **The benefits?** As a project leader:      -  You are in charge of the IT infrastructure.      -  Decide what packages are installed. Make sure that your coauthors can focus on doing their research, rather than installing packages or coping with memory problems on their laptops.      - Research Cloud is also beneficial if you just want to work on a project on your own, by the way\u2026  At present, SURF offers year-long \u201cstart-up\u201d grants, which can then be renewed based on usage and feedback. For longer-term projects, they also offer the option to apply to [NWO grants](https://www.nwo.nl/en/calls/computing-time-national-computer-facilities-2021) that focus on computing resources.   {{% warning %}}  Check that you\u2019re affiliated with a Dutch research institution to set up the workspace for yourself.  {{% /warning %}}  1. Fill in the project [application form](https://www.surf.nl/en/research-ict/apply-for-access-to-compute-services). Select \u201csmall application\u201d.  2. Once your wallet is approved, you will receive an email from SURF to set up your initial workspace. This will look like this:    ![surf mail](../surf_mail.png)      Still waiting for your wallet to be approved? You can check the [status of your application at SurfSara's service desk](https://servicedesk.surfsara.nl).  3. After setting up the initial workspace, access it [here](https://portal.live.surfresearchcloud.nl/).  4. Detailed instructions to operate Research Cloud (including adding data sources) are [here](https://servicedesk.surfsara.nl/wiki/display/WIKI/How+to+get+on+board).     To give students/collaborators access to a VM you have already created, you can [invite into your CO](https://wiki.surfnet.nl/display/SRAM/Invite+CO+admins+and+members) (collaborative organization) in [SRAM](https://sbs.sram.surf.nl/):  Once they become a member of your CO they can:   -  Log in to the Research Cloud portal.  -  Set up TOTP.  -  See the workspaces that you have started for the CO.   The Dutch foundation SURF offers login via SurfConext, which is an easy way to keep things consistent. While this works for Research Drive which is operated by LIS <link here>, it is not yet enabled for Research Cloud. Therefore, one would need to create a separate account through eduID (step 4 under \u201clogging in\u201d [here](https://servicedesk.surfsara.nl/wiki/display/WIKI/How+to+get+on+board)).  {{% warning %}} SURF services are only available for researchers affiliated with Dutch institutions. {{% /warning %}} "}, {"objectID": "./building-blocks/configure-your-computer/automation-and-workflows/awscli.md", "title": "AWS Client", "description": "A command-line interface for using Amazing Web Services", "keywords": "AWS, CLI, command line, terminal, command", "code": [], "headers": ["Step 1: Install AWS CLI", "Step 2: Configure AWS client"], "content": " Amazon Web Services (AWS) provides cloud-computing and storage facilities that can be used for research projects. To make optimal use of AWS, you can control AWS resources (e.g., starting computers, downloading data) using a so-called command-line interface.  The AWS command-line interface is a light-weight tool that provides you with a \"remote control\" to the AWS cloud.  Here, we show you how to install and configure AWS CLI.   - Install AWS client by following the [installation instructions](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html) provided by Amazon  Note: here, we install AWS CLI version 1, not version 2 (required for backwards-compatibility for some of the projects run by Tilburg Science Hub).   - Open a new terminal / command line window - Type: `aws configure` - Enter your login credentials for AWS. Typically, these are provided to you by your research institution or team member.  ``` AWS Access Key ID [****************7EPQ]: [enter here] AWS Secret Access Key [****************PdsF]: [enter here] Default region name [eu-central-1]: eu-central-1 Default output format [None]: [just press enter] ``` "}, {"objectID": "./building-blocks/configure-your-computer/automation-and-workflows/make.md", "title": "Make", "description": "Make is a build tool that allows us to control the execution of a set of command line statements.", "keywords": "make, build, build tool", "code": [], "headers": ["Overview", "Installation", "For Windows Users", "Download make", "For Mac Users", "For Linux Users", "Verifying Your Installation"], "content": "  [`Make`](https://www.gnu.org/software/make/) is a \"build tool\", allowing us to control the execution of a set of command line statements to assemble pipelines that define what gets executed when. Using `make`, our entire workflow becomes transparent and reproducible.  {{% tip %}}  **Why Build Tools?**  Imagine you have built a complex project, consisting of dozens of datafiles and scripts. Also imagine you haven't worked on the project for a few weeks, and now wish to continue your work where you left off.  The big questions then become:  - In which order do the files need to be executed? - Which files are up-to-date and do not have to be executed again?  For this purpose, we use a *workflow management system* - or, in technical terms - \"build tool\". Build tools will allow us to control the execution of a set scripts by by running them from the command line.  Some reasons we push this topic are:  * Your workflow / order of execution is explicitly documented. * Each time you run `make`, it only executes each script if the output is expected to be different from the last time your ran it. That is, it runs 'partial builds.' * If multiple users work on the project, they can easily execute code that others have written.  {{% /tip %}} <!--    #* Its written in Python, which minimizes the learning curve needed to pick up the essentials relatively small #    #* It was designed for academic/professional research (in Bioformatics) so it feels more intuitive than most alternatives for our desired audience. -->   We will use `make` to automate the execution of our projects with a \"single click\", so that our entire work flow is reproducible.   We will install `make` so that it plays nicely with your Anaconda/Python distribution and the Windows command line.  Watch our YouTube video, in which we walk you through the setup on Windows.  {{< youtube 8bqH8AOPT3U iframe-video-margins >}}  Please [download `make` here](http://gnuwin32.sourceforge.net/downlinks/make.php).  {{% warning %}} **Making `make` available via the PATH settings on Windows.**  We need to update our PATH settings; these settings are a set of directories that Windows uses to \"look up\" software to startup. {{% /warning %}}      - Open the settings for environment variables         - Right-click on Computer.       \t- Go to \"Properties\" and select the tab \"Advanced System settings\".       \t- Choose \"Environment Variables\"     - Alternatively, type \"environment variable\" (Dutch: omgevingsvariabelen) in your Windows 10 search menu, and press Enter.  \t-  Select `Path` from the list of user variables. Choose `Edit`. \t\t- **Windows 7 and 8 machines:** \t\t\tIf you chose your installation directory to be `C:\\Program Files\\GnuWin32\\bin` during your installation (i.e., you did use the default directory), copy and paste the following string without spaces at the start or end:              `;C:\\Program Files (x86)\\GnuWin32\\bin`  \t\t- **Windows 10 machines:** \t\t\t- Click `New` and paste the following string:              `C:\\Program Files (x86)\\GnuWin32\\bin`  \t\t\t- Click on `OK` as often as needed.  <!---  within CygWin. Its time to go back to the **setup-x86_64.exe** we [told you not to delete](commandLine.md). We will use it to install make. Proceed as follows:  * Click through the installation until you arrive at the page \"Select packages.\" * Type make into the search function and wait for the results to be filtered. * Click the '+' next to \"Devel\" and then find the following lines:     * make     * gcc-tools-epoch1-automake     * gcc-tools-epoch2-automake  and then click on the word 'Skip' located next to each of these. 'Skip' should then be replaced with some numbers (the version which we will install).  * Now click on \"Next\" in the bottom right corner and continue accepting all options until the installation is complete.  -->   * Please install X-code command line tools, which includes `make`.  Open a terminal by searching for it with spotlight, `cmd + spacebar` then type terminal and press `Return` when it appears. Then, copy and paste the following:  ```bash xcode-select --install ```   `Make` is pre-installed on Linux operating systems so there is nothing to be done.    To check that `Make` is installed and working correctly, open a terminal session and type (then hit the return key):  ```bash make ```  If everything is working correctly, you should get the following output:  ```bash make: *** No targets specified or no makefile found. Stop. ``` "}, {"objectID": "./building-blocks/configure-your-computer/automation-and-workflows/environment.md", "title": "Making Path Settings Permanent", "description": "For our workflow to be command line driven, we need to be able to call software from the command line.", "keywords": "PATH, add, terminal, cli, bash", "code": [], "headers": ["Overview", "Windows Users", "Mac and Linux Users "], "content": "  For our workflow to be command line driven, we need to be able to call software from the command line. Whilst much of the software we have installed as automatically made this possible, it is not universally the case. Here we will make the extra few changes we need to complete the installation guide.   You will need local administration rights for your computer, but you should have these on your personal computers or ones owned by the Department.  Right-click on Computer. Then go to \"Properties\" and select the tab \"Advanced System settings\". Choose \"Environment Variables\" and select \"Path\" from the list of system variables.  Choose \u201cEdit\u201d and append (i.e., do not overwrite the previous value):          C:\\Path\\to\\program.exe  to the variable value \u2013 make sure the rest remains as it is and do not include spaces between the \";\" and the text.  Click on OK as often as needed.  If you accepted all defaults during your installation, and didn't have any other non-default setting prior to starting this guide, modifying the following string, with your relevant username *should* work:          ;C:\\Users\\ldeer\\AppData\\Local\\atom\\bin         ;C:\\Program Files\\Git\\bin         ;C:\\Program Files\\R\\R-3.X.X\\bin         ;    You will need to add a line to the file \".bash_profile\" or potentially create the file if it didn't already exist. This file lives in your home directory and it is hidden from your view by default.  We now provide a guide of how to create / adjust this file using a tiny editor called nano, if you are familiar with editing text files, just use your editor of choice.  Open a Terminal and type:  ```bash nano ~/.bash_profile ```  You should now see something similar to this. It's a text editor within the Terminal. Here you can add your environment variables.  ![PATH variables in the Bash profile](../img/bash-profile.png)  {{% warning %}} If .bash_profile did not exist already, you need to create it. {{% /warning %}}  You can add your environment variable by adding a new line: ```bash export PATH=$PATH:/path/to/folder ```  You can save by pressing `Ctrl + O` and exit by pressing `Ctrl + X`.  Once you're done, you'll need to reload `bash`. Close and launch again a new Terminal session, the type:  ```bash . ~/.bash_profile ```  Do this for each program you need to make accessible from the command line. Do this for:  * Sublime Text * Git * Matlab * R  For example, to make MATLAB accessible from the command line: ```bash export PATH=$PATH:/Applications/MATLAB_R2016a.app/bin/matlab ```    Your default locale settings may conflict with some of the programs we'll need. If you want to be on the safe side, add these lines to your .bash_profile file:          export LC_ALL=en_US.UTF-8         export LANG=en_US.UTF-8  Thanks for Hans-Martin von Gaudecker, and in turn Matthias Bannert for the tip on locale settings.  **Linux users**: For most distributions, everything here applies to the file .bashrc instead of .bash_profile. "}, {"objectID": "./building-blocks/configure-your-computer/automation-and-workflows/snakemake.md", "title": "Snakemake", "description": "Snakemake is an easy to use workflow management system.", "keywords": "snakemake, build, build tool, workflow", "code": [], "headers": ["Installing Snakemake", "Mac & Linux Users", "Windows users"], "content": "  [Snakemake](https://snakemake.readthedocs.io/en/stable/) is an easy to use workflow management system. Contrary to *make*, which was designed by computer scientists to build software, *snakemake* was designed for academic/professional research in Bioinformatics, so it may feel more intuitive for academic users.  Snakemake is a python package - so we can install using the default python installer, pip.   In a terminal window enter the command:  ```bash pip install snakemake ``` followed by pressing the `Return` key.  Verify that your installation worked correctly by entering  ```bash snakemake --version ``` into a terminal and pressing `Return.`  The expected output is the current version on the software, which should be greater than  ```bash 5.2.2 ```   We need one extra step here. In a cygwin window enter the command:  ```bash conda install datrie ```  followed by pressing `Return`.  If you get an access denied error, you may have not clicked on single user install in the Anaconda installation. De- and re-install Anaconda and try again.  If the above command works, your terminal will look something like this:  ```bash The following NEW packages will be INSTALLED: datrie: 0.7.1 proceed ([y]/n) ```  Type `y` and hit enter. Once this is done, type:  ```bash pip install snakemake ```  followed by pressing the `Return` key.  The expected output is the current version on the software, which should be greater than  ```bash 5.2.2 ``` "}, {"objectID": "./building-blocks/configure-your-computer/automation-and-workflows/teamviewer.md", "title": "TeamViewer", "description": "Teamviewer allows to remote-control your computer to solve technical issues.", "keywords": "teamviewer, software, installation, remote", "code": [], "headers": ["Installing Teamviewer", "Starting a TeamViewer session"], "content": " Getting feedback on code or solving technical problems is crucial to your learning experience. When working remotely, you can make use of **TeamViewer**, which allows team members to *remote-control* your computer and help you right away.   To install TeamViewer, go to the [TeamViewer homepage](https://www.teamviewer.com/en/download/windows/) and download the installer for your operating system. We recommend you to install Teamviewer *permanently*. If you do not have administrator rights, it's fine to only select *run once*.  Please indicate you download Teamviewer for *private use*, unless your organization provides you with a site license.  {{% warning %}} **Mac users need to change additional security settings.** After installing TeamViewer, Mac users need to change a few security settings to grant access rights so that a team member may steer your mouse and type on your keyboard. Please [follow this guide](https://community.teamviewer.com/t5/Knowledge-Base/How-to-control-a-Mac-running-macOS-10-14-or-higher/ta-p/44699#toc-hId--1220346050). {{% /warning %}}   Open TeamViewer via the start menu, select **remote control** on the left side of your screen (it's the default), and share **your ID** and **your password** with your team member.  ![Screenshot of Teamviewer](../teamviewer.png)  Also want to talk to your team member? Make sure you have a headset connected to your computer.  {{% warning %}} **Remember to change your temporary password!**  Please change your temporary password as soon as the session has ended.  To do so,  - just hover over the password area with your mouse, - click on the little blue icon appearing on the right of it, and - select generate a new random password.  This will keep your computer safe!  {{% /warning %}} "}, {"objectID": "./building-blocks/configure-your-computer/automation-and-workflows/commandline.md", "title": "Command Line Tools", "description": "A command-line interface or command language interpreter (CLI), also known as a terminal, is a means of interacting with a computer program.", "keywords": "CLI, terminal, Homebrew, brew, cygwin, command", "code": [], "headers": ["Installing Command Line Tools", "Windows Users", "Mac Users", "Linux Users"], "content": "  A command-line interface or command language interpreter (CLI), also known as a terminal, is a means of interacting with a computer program where the user issues commands to the program in the form of successive lines of text (command lines).  Throughout the course we will emphasize use of the terminal and executing commands within it as our modus operandi.   So that we can work as closely as possible to the Mac and Linux users we will install [Cygwin](https://www.cygwin.com/).  *   Download Cygwin [here](https://cygwin.com/install.html) and use the graphical installer. Accept all the default options. *   Choose any server from which to download cygwin and packages when prompted. *   Verify your installation by opening Cygwin. When it opens you should see a black box with some text that looks like:  ```bash userName@computerName: ~$ ``` <!--i.e. for Uli he sees: ```bash ubergmann@dhcp-wlan-uzh-10-12-130-xxx: ~$ ```  We will explain what all this means in the first day or so of the course. -->   {{% tip %}}  Why Cygwin? * We will uses Cygwin as our command line tool, and unlike other Windows shells such as PowerShell it uses Unix syntax. *  Anywhere throughout the remainder of the installation guide where we suggest you to enter a command into a terminal, enter the text-based command into your Cygwin terminal followed by pressing `Return`, for example:          userName@computerName: ~$ whoami  Should return your username.  {{% /tip %}}  {{% warning %}}  **Do not delete the setup-x86_64.exe file.** It needs to be kept so that we can add on some additional packages to use in the course.  {{% /warning %}}   A command line interface comes already installed with MacOS.  You will need to install some other software from the terminal thoughout the course, so it will be useful to install some additional \"command line tools\" now:  *   First we want to install X-code command line tools. Open a terminal by searching for it with spotlight, `cmd + spacebar` then type terminal and press `Return` when it appears. Then, copy and paste the following  ``` xcode-select --install ```  If you get an answer that the command line tools are already installed, you can just continue to the next step.  * Second, install [Homebrew](https://brew.sh) by opening a terminal and pasting the following command:  ``` ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/homebrew/install/master/install)\" ```  * To verify that Homebrew installed correctly, enter the following into your terminal ``` brew doctor ``` And you should see the following output ``` Your system is ready to brew ```  * Now we can use Homebrew to easily install software. To use the current R version 3.5.1, we want to make sure you have some basic system tools that some packages require. Let's (re)install them real quick. First `libxml2`:  ``` brew reinstall libxml2 ```  If you system tells you that it is not yet installed, then try  ```brew install libxml2``` instead.  We also want to link this so that terminal finds it later:  ``` echo 'export PATH=\"/usr/local/opt/libxml2/bin:$PATH\"' >> ~/.bash_profile ```  * Second, we also need `openssl`:  ``` brew reinstall openssl ``` Again, if it is already installed, then use ``` brew install openssl``` instead.  Again, we need it to link to terminal:  ``` echo 'export PATH=\"/usr/local/opt/openssl/bin:$PATH\"' >> ~/.bash_profile ```    * Finally, we need `libgit2`:  ``` brew install libgit2 ```  If terminal tells you it is not yet installed, then go for ```brew reinstall libgit2```   To use the current R version, we need to install some system tools. For this open a terminal session with `Ctrl` + `Alt` + `T`.  * Now copy the following command into terminal and press `Enter`:  ```   sudo apt-get install libcurl4-gnutls-dev librtmp-dev ```  * After the installation succeeded successfully repeat this one-by-one with the following two other commands:  ``` sudo apt-get install libxml2-dev sudo apt-get install libssl-dev ``` "}, {"objectID": "./building-blocks/configure-your-computer/statistics-and-computation/text-editor.md", "title": "Text editors", "description": "A good text editor lies at the heart of any serious programmer's toolkit.", "keywords": "text, editor, atom, installation, software", "code": [], "headers": ["Installing a text editor", "Installing Sublime Text", "Installing Atom", "Mac and Windows Users", "Linux/Ubuntu users", "Verifying Atom Installation"], "content": "  A good text editor lies at the heart of any serious programmer's toolkit: It can do almost anything and makes you much more productive. The editors built into each program often are not the best option (we will find certain cases where you may want to use them).  Please download, install it along with the necessary packages and stick with it for at least for some time to get a feel for how it works.  There is a slight learning curve, but soon you hopefully will be wondering why no-one forced you to do this before!  {{% tip %}} None of the skills on this website are tied to Atom, so if you do decide to move away to another editor, nothing will be lost. For example, [Visual Studio Code](https://code.visualstudio.com/) or [Vim](https://www.vim.org/) are also widely used text editors. {{% /tip %}}   Go to the [downloads page](https://www.sublimetext.com/3) and download the live installer for your operating system.-->    Go to the [downloads page](https://github.com/atom/atom/releases/latest) and download the installer for your operating system:  * Windows users download AtomSetup-x64.exe and execute the installer * Mac users download Atom-Mac.zip and copy the contained Atom.app into the applications folder   Enter the following information to add a repository that has the Atom installation, then press `Return`:  ```bash sudo add-apt-repository ppa:webupd8team/atom ```  Install Atom by entering the following commands into a terminal and then pressing `Return:`  ```bash sudo apt update; sudo apt install atom ```   We want Atom to be available from the command line. For Mac and Linux Users this is the default after you have started the program once. So please open Atom. Then open your terminal and type the following into the command line:  ```bash atom --version ```  followed by pressing `Return` you should see output like the following  ```bash Atom    : 1.28.2 Electron: 2.0.5 Chrome  : 61.0.3163.100 Node    : 8.9.3 ``` Make sure that the version numbers are above `1.26.x` or newer.  {{% warning %}} **Additional Step for Windows.** Getting things to run from the command line for Windows users is a bit harder. You will need local administration rights for your computer, but you should have these on your personal computers or those owned by the University.  - Right-click on Computer. - Then go to \"Properties\" and select the tab \"Advanced System Settings\". - Choose \"Environment Variables\" and select `Path` from the list of system variables. - If you accepted all defaults during your installation, and didn't have any other non-default setting prior to starting this guide, choose `Edit.`  - **On Windows 7 or 8 machines:** \tAppend the following (i.e., do not overwrite the previous value) modifying the string, with your relevant `username`:          `;C:\\Users\\username\\AppData\\Local\\atom\\bin`  \tto the variable value \u2013 make sure the rest remains as it is and do not include spaces between the \";\" and preceeding text.  - **On Windows 10 machines:** \tClick `New` and paste the following string, modifying the `username`  \t`C:\\Users\\username\\AppData\\Local\\atom\\bin`  Click on `OK` as often as needed.  Close your current terminal session, open a new one, and again try `atom .` - the Atom editor will open if this was successful. {{% /warning %}}  {{% tip %}} **Installing Additional Packages for Atom**  One of the advantages of Atom is that there are many *packages* that make your life easier, ranging from simple syntax highlighting to environments that can mimic a complete graphical user interface.  * To access Atom's settings press `Ctrl + ,` (`CMD + ,` on Mac) on your keyboard then click on the `Install` tab which is visible on the left hand side. * On the Installation page there is a prompt where you can type in a package name and then press `Return` and Atom will search for that package for you and return results with similar names. When you find the package that you need, you click the blue 'Install' button and the package will be installed. * If during the installation of a package Atom asks to install dependencies, always choose to accept.  * If you decide to stick with Atom, you may find the following packages useful in your day-to-day work:      *   tablr     *   tablr-json     *   autocomplete-R     *   autocomplete-python (choose Jedi as your engine when asked)     *   autoflow     *   language-r     *   linter     *   linter-lintr     *   platformio-ide-terminal     *   project-plus     *   language-markdown     *   markdown-table-editor     *   markdown-preview-plus     *   autocomplete-citeproc     *   open-unsupported-files     *   advanced-open-file     *   language-latex     *   language-matlab     *   language-stata     *   atom-latex     *   whitespace  {{% /tip %}} "}, {"objectID": "./building-blocks/configure-your-computer/statistics-and-computation/python.md", "title": "Python", "description": "Python is widely use programming language. Learn how to set it up on your computer.", "keywords": "python, anaconda, conda, distribution, installation, configure, PATH, statistics", "code": [], "headers": ["Installing Anaconda Python", "Instructions", "Verifying that the installation was successful", "Windows users", "Mac & Linux/Ubuntu users"], "content": "  Anaconda is a pre-packaged Python distribution for scientific users. To customize Python with packages and properly use it in combination with [automation tools](/building-blocks/configure-your-computer/automation-and-workflows/make/), we prefer a *locally installed Anaconda distribution* over cloud-based alternatives.  Watch our YouTube video, in which we walk you through the setup on Windows.  {{< youtube hGZSAuDcmQc iframe-video-margins >}}  Direct your browser to [Anaconda download page](https://www.anaconda.com/download/) and download the Python 3.x Graphical Installer for your machine. Sometimes, the download defaults to Mac, so if you're on Windows or Linux, make sure to select the right version.  Then, follow the steps provided on the website.  {{% warning %}} During the installation you will be asked whether you want Anaconda Python to be added to your PATH. **Click yes!** Even if the installation window gives a warning about adding it to your PATH, please still check that box. {{% /warning %}}  Note that the installation of Anaconda may take about 5-10 minutes, dependening on how fast your computer is.  {{% warning %}} **For Windows Users**  *   When asked if you want single or multiple user installation, choose **single user**. *   Accept all defaults that are set in the installation window. *   Check the box for adding Anaconda to your PATH. *   In the last step, you are asked if you want Visual Studio, click **Yes**. {{% /warning %}} {{% warning %}} **For Linux Users**  For some users Python was not added to the path. To quickly do this, please open a terminal window, paste ```echo '$HOME/anaconda3/bin:$PATH' >> ~/.bashrc``` and press `Return`. {{% /warning %}}   To verify that the correct version of Python has been installed and was made available in your PATH settings, close your terminal and open a **new** terminal interface and enter:   ```bash python --version ``` followed by hitting the `Return` key.  You should see the following information returned:   ```bash Python 3.x.x :: Anaconda, Inc. ```   ```bash Python 3.x.x :: Anaconda custom (64-bit) ```  {{% tip %}} **Python 2 versus Python 3**  Python 2 and 3 are incompatible in syntax. If you had Python 2 previously installed on your machine, you might have seen `Python 2.x.x` above. In that case try typing  ```python3 --version```  instead. Now you should see a message like the one above and are good to go.  {{% /tip %}} "}, {"objectID": "./building-blocks/configure-your-computer/statistics-and-computation/stata.md", "title": "Stata", "description": "Stata is a proprietary statistical software frequently used by scientific users.", "keywords": "Stata, software, installation, configuration, statistics", "code": [], "headers": ["Installing Stata", "Making Stata Available on the Command Prompt", "Windows users", "Mac users", "Verifying the installation", "Windows users", "Mac users"], "content": "  Stata is a proprietary statistical software frequently used by scientific users. First check with your local IT support whether your institution has Stata licenses available.  If not, you can purchase a copy on [Stata's website](https://www.stata.com/) and follow the [installation guide](https://www.stata.com/install-guide/).   You have just installed Stata. Later, we'd like to access Stata from the command prompt to automatically execute our source code. That way, you will be able to run a series of scripts in batch - which will significantly ease the burden of building complex data workflows.  For you to be able to use Stata from the command prompt, follow the steps below.  {{% warning %}} **Making Stata available via an environment variable.**  We need to update our environment variables.  - Right-click on Computer. - Go to \"Properties\" and select the tab \"Advanced System settings\". - Choose \"Environment Variables\" and create a new variable, which you call `STATA_BIN`. - Choose `Edit`. \t- Environment variable name: `STATA_BIN` \t- **Windows 7 and 8 machines:** \t\tPlease check where you have installed Stata on your computer, and     note the name of the Stata program file (on some machines, it is called       `StataSE.exe`, on others it is called `StataMP-64.exe`.        Then, update the path if required, and copy and paste the following string without spaces at the start or end:        `c:\\Program Files (x86)\\Stata15\\StataSE-64.exe`  \t  Using a different Stata version? Change the version number then in the path above.  \t- **Windows 10 machines:** \t\t- Click `New` and paste the following string:          `c:\\Program Files (x86)\\Stata15\\StataSE-64.exe`  \t\t- Click on `OK` as often as needed. {{% /warning %}}   For you to be able to use Stata from the command line, you have to add Stata to your environmental variables. A tutorial follows.  {{% warning %}} **Making Stata available via the PATH settings on Mac.**  - Open the Terminal. - Type `nano ~/.bash_profile`. - Add Stata to the environmental variables: \t\t- Add `export STATA_BIN=/Applications/Stata/StataMP.app/Contents/MacOS/stataMP` to a new line. You may need to replace stataMP to stataSE or so, which depends on which Stata you install on Mac. \t\t- Save by pressing `Ctrl + O` and exit by pressing `Ctrl + X`. \t\t- Relaunch the Terminal. Then type `source ~/.bash_profile` to bring the new .bash_profile into effect. - Type `$STATA_BIN -v` to check availability. Remember to type the `$` before `STATA_BIN`. {{% /warning %}}  <!--- Linux users not available yet -->    To verify that Stata has been correctly installed and configured via your PATH settings, follow the instructions below.   Open a **new** terminal interface and enter:  ```bash %STATA_BIN% ```  followed by hitting the `Return` key. You can check the Stata version.  {{% warning %}} If you obtain the following error: ```bash 'C:\\Program' is not recognized as and internal or external command ``` Add \" \" to your variable path, for instance `\"c:\\Program Files (x86)\\Stata15\\StataSE-64.exe\"`. {{% /warning %}}   Open a **new** terminal interface and enter:  ```bash echo $PATH ```  followed by hitting the `Return` key. "}, {"objectID": "./building-blocks/configure-your-computer/statistics-and-computation/latex.md", "title": "LaTeX", "description": "LaTeX is a great typesetting system that includes a lot of features that allow to produce scientific documents.", "keywords": "latex, tex, lyx, installation, software, configuration, paper, writing, text, typesetting", "code": [], "headers": ["Installing Latex", "Windows", "MacOS", "Linux (Ubuntu-based)", "Check your Installation", "Installing LyX, a $\\LaTeX$ Alternative", "Get LyX", "Making LyX Available on the Command Prompt", "Windows users", "Mac users", "Check your Installation"], "content": "  {{< katex >}}\\LaTeX{{< /katex >}} is [a great typesetting system](https://www.latex-project.org) that includes a lot of features that allow to produce scientific documents. Many researchers use Latex to produce their papers and presentations, and many journals require authors to hand in their articles in a TeX format.  Latex is free to use. To use the Latex system, a TeX distribution needs to be installed. Detailed instructions for the different platforms are provided below.   Download the file `install-tl-windows.exe` from **[here](https://www.tug.org/texlive/acquire-netinstall.html)** and follow the instructions.   You can install MacTeX from the **[official website](https://www.tug.org/mactex/)** or using [`Homebrew`](/configure/cli):  ```bash brew cask install mactex ```   Install it from the terminal using:  ```bash sudo apt-get install texlive-latex-extra ```   After following the instructions, check whether everything worked by checking the output of the following command:  ```bash tex --version ```  This should give an output similar to this one, where version numbers and details will change depending on your platform.  ```bash TeX 3.14159265 (TeX Live 2019/Debian) kpathsea version 6.3.1 Copyright 2019 D.E. Knuth. There is NO warranty.  Redistribution of this software is covered by the terms of both the TeX copyright and the Lesser GNU General Public License. For more information about these matters, see the file named COPYING and the TeX source. Primary author of TeX: D.E. Knuth. ```  Note that additional packages for Tex Live should be installed through the apt package manager as well (using `tlmgr` leads to problems due to different versions)    LyX is an open source document processor based on Latex. [Download LyX](https://www.lyx.org/Download).   You have just installed LyX and may need to access LyX from the command line.  For you to be able to use LyX from the command prompt, follow the steps below.  {{% warning %}} **Making LyX available via the PATH settings on Windows**  We need to update our PATH settings; these settings are a set of directories that Windows uses to \"look up\" software to startup.  - Right-click on Computer. - Go to \"Properties\" and select the tab \"Advanced System settings\". - Choose \"Environment Variables\" and select `Path` from the list of system variables. - Choose `Edit`. \t- Environment variable name: LYX_BIN \t- **Windows 7 and 8 machines:** \t\tIf you chose the default installation directory, copy and paste the following string without spaces at the start or end:         `c:\\Program Files (x86)\\Lyx`  \t- **Windows 10 machines:** \t\t- Click `New` and paste the following string:         `c:\\Program Files (x86)\\Lyx`  \t\t- Click on `OK` as often as needed. {{% /warning %}}   For you to be able to use LyX from the command line, you have to add LyX to your environmental variables:  {{% warning %}} **Making LyX available via the PATH settings on Mac**  - Open the Terminal. - Type `nano ~/.bash_profile`. - Add LyX to the environmental variables: - Add `export LYX_BIN=/Applications/LyX.app/Contents/MacOS/lyx` to a new line. - Save by pressing `Ctrl + O` and exit by pressing `Ctrl + X`. - Relaunch the Terminal. Then type `source ~/.bash_profile` to bring the new .bash_profile into effect. - Type `$LYX_BIN` to check availability. Remember to type the `$` before `LYX_BIN`. {{% /warning %}}  <!--- Linux users not available yet -->    To verify that LyX has been correctly installed and configured via your PATH settings, open a **new** terminal interface and enter:  ```bash $LYX_BIN ```  followed by hitting the `Return` key. "}, {"objectID": "./building-blocks/configure-your-computer/statistics-and-computation/r.md", "title": "R and RStudio", "description": "R is a widely used language for statistical computing and graphics. Learn how to set it up on your computer.", "keywords": "R, statistics, installation, software, RStudio, PATH, learn R, get R, install R, setup", "code": [], "headers": ["Installing R and RStudio", "Installing R", "Installing RStudio", "Verifying your Installation", "Installing additional R Packages", "Making R available on the command prompt", "Windows", "Making R find packages on the command prompt"], "content": "  R is a language for statistical computing and graphics. R's use in the data science, econometrics and marketing communities has taken off over recent years and (at a bare minimum) should be considered as an open source replacement to Stata and SPSS.   Watch our YouTube video, in which we walk you through the setup on Windows.  {{< youtube xvw4Xha10qg iframe-video-margins >}}  Go to the [R website and download the most recent installer for your operating system](https://cran.r-project.org/).  - Windows users: choose the \"base\" subdirectory, then proceed to the download. - Mac users: pick the release listed under \"latest release\" (pick the first, if it does not work, try the second).  We strongly suggest you to install R in the directory `C:\\R\\R-4.x.x\\` rather than the default directory, `C:\\Program Files\\R\\R-4.x.x\\`.   RStudio provides an easy to work with interface to R, and its format should feel familiar to other software environments like Stata or SPSS.  Download and install the **free version of RStudio** for your operating system from [here](https://www.rstudio.com/products/rstudio/download/).   Open RStudio from the start menu. After starting up, you should see the version corresponding to the one chosen on the website.  ![Screenshot of R Studio](../r.png)   You may need some additional libraries to work with R (e.g., some extra code that helps you to run your statistical analyses).   To install packages, open RStudio (if not already opened in the previous step). In the **console**, copy and paste the following:  ```r packages <- c(\"reshape2\", \"rmarkdown\",               \"data.table\", \"Hmisc\", \"dplr\",               \"stargazer\", \"knitr\",               \"xtable\",\"tidyverse\",               \"RSQLite\", \"dbplyr\")  install.packages(packages) ```  * If you are asked if you want to install packages that need compilation, type `n` followed by `Return`. Package compilation is likely to cause some errors, and you're all good going with packages that have already been compiled (typically, these are earlier versions of the package). * Wait until all the packages have been installed and the you are done. It *may* take a while, so be patient   You have just installed R and RStudio, and learnt how to open RStudio from the start menu. However, for many of the applications that follow, you are required to access R directly from the command prompt. For example, this will enable you to run a series of R scripts in batch - which will significantly ease the burden of building complex data workflows.   For you to be able to use R from the command prompt, **Windows users** need to follow the steps below. On Mac and Linux, R is available from the command line by default.  {{% warning %}} **Making R available via the PATH settings on Windows.**  We need to update our PATH settings; these settings are a set of directories that Windows uses to \"look up\" software to startup.  - Open the settings for environment variables     - Right-click on Computer.   \t- Go to \"Properties\" and select the tab \"Advanced System settings\".   \t- Choose \"Environment Variables\" - Alternatively, type \"environment variable\" (Dutch: omgevingsvariabelen) in your Windows 10 search menu, and press Enter.  -  Select `Path` from the list of user variables. Choose `Edit`. - **Windows 7 and 8 machines:** \tIf you chose your installation directory to be C:\\R\\R-4.x.x\\ during your installation (i.e., you did not use the default directory), copy and paste the following string without spaces at the start or end:          `;C:\\R\\R-4.x.x\\bin` (replace `4.x.x` by your actual version number!)  - **Windows 10 machines:** \t- Click `New` and paste the following string:          `C:\\R\\R-4.x.x\\bin` (replace `4.x.x` by your actual version number!)  \t- Click on `OK` as often as needed. {{% /warning %}}  {{% warning %}}  **Making R available via the PATH settings on Mac/Linux**  - Paste this command in your terminal: `nano ~/.bash_profile` - Add the following two lines to it:  ``` export R_HOME=\"/Library/Frameworks/R.framework/Resources\" export R_USER=\"/Library/Frameworks/R.framework/Resources\" ```  {{% /warning %}}   {{% tip %}} Keep in mind that after you add a new directory to the `PATH` variable, you need to start a *new* command prompt/terminal session to verify whether it worked. Sometimes it may take a couple of minutes until your PATH is recognized by the terminal.  {{% /tip %}}  **Now let's verify whether we can open R from the command prompt**  Open the command prompt/terminal and enter:  ```bash R --version ```  followed by pressing `Return`. The expected return begins with:  ```bash R version 4.x.x (20xx-xx-xx) -- \"Some Funky Name\" ```  Great job - you've managed to install R and configure it for use for data-intensive projects!  You can now access R directly from the command prompt. Nevertheless, code that runs perfectly on R Studio might return `Error in library(x)` on the command prompt.  Why is that? Sometimes, when running R from the command line, it doesn't find the packages that were installed in your *user library* paths.  **Solution:** Tell R where to find your user library.  {{% warning %}} **Making R find your user library via the PATH settings on Windows.**    - In RStudio, type `.libPaths()` and note the path to your user directory (typically the one that contains your user name).    - Open the settings for environment variables        - Right-click on Computer.        - Go to \"Properties\" and select the tab \"Advanced System settings\".        - Choose \"Environment Variables\"    - Select `New` and name it **`R_LIBS_USER`**. `Variable value` is the path (that you previously noted) to your user directory.  Rather want to set `R_LIBS_USER` on a Mac or Linux machine? [Read more here](https://tilburgsciencehub.com/setup/environment).  {{% /warning %}}  **Verify that you can access your packages**  Close all command prompts/terminals. Open one again, type `R` to open R and then enter:  ```bash library(x) ```  Note that the command `library` requires you to specify the package name without quotation marks (e.g., `library(tidyverse)`, *not* `library(\"tidyverse\")`).  Expect a return beginning with: ```bash Attaching package: 'x' ```  Get an error message? Try reinstalling the package using `install.packages(\"name_of_the_page\")`. "}, {"objectID": "./building-blocks/configure-your-computer/statistics-and-computation/git.md", "title": "Git and GitHub", "description": "Git is an open source version control system (VCS) that has gained a lot of traction in the programming community.", "keywords": "git, github, installation, software, configuration, versioning, account", "code": [], "headers": ["Installing and Setting Up Git & GitHub", "Let's create a GitHub account", "Install Git on your computer", "Additional instructions for Windows Users", "Additional instructions for Mac Users", "Linux Users", "Verifying your installation"], "content": "  Git is an open source version control system (VCS) that has gained a lot of traction in the programming community. We will use version control to keep track of the files we write, and the changes we make to them. Using Git, we can roll back to any previous version of a file, and easily collaborate with others on source code.  GitHub is a (commercial) platform which allows you to host your source code online. It also offers a set of collaborative tools to manage \"Issues\", and track progress on project boards.  - Please [register for a GitHub account](https://github.com/join), and   claim your [education benefits for students and professors](https://education.github.com) afterwards. - You can also watch our YouTube video where we will walk you through the sign-up procedure.  {{< youtube 3KOnOgz_dAA iframe-video-margins >}}   Watch our YouTube video, in which we walk you through the setup on Windows.  Download and install the latest versions of Git [here](https://git-scm.com/download).  {{< youtube VUzv5RcnW60 iframe-video-margins >}}   After installing the programs use Windows Explorer to go to a folder that contains some documents (any folder) and right click on it.  You should see some additional items - \"Git Bash here\", or \"Git GUI here\" appear in the context menu upon right-clicking.  <!--Whenever you wish to open Git, you  !!! danger \"Making Git available via the PATH settings on Windows\"     We need to update our PATH settings; these settings are a set of directories that Windows uses to \"look up\" software to startup.      - Right-click on Computer.     - Go to \"Properties\" and select the tab \"Advanced System settings\".     - Choose \"Environment Variables\" and select `Path` from the list of system variables.     - Choose `Edit`.     \t- **Windows 7 and 8 machines:**     \t\tIf you chose your installation directory to be C:\\R\\R-3.6.1\\ during your installation (i.e., you did not use the default directory), copy and paste the following string without spaces at the start or end:              `;C:\\Program Files\\Git\\bin`      \t- **Windows 10 machines:**     \t\t- Click `New` and paste the following string:              `C:\\Program Files\\Git\\bin`      \t\t- Click on `OK` as often as needed. -->   Download the setup files from the link above. If your system says the file can't be opened (because it is from an unidentified developer), then open it via right-lick and `open`.  <!--Also install the command-line auto-completion script. For this go to [this website](https://github.com/git/git/raw/master/contrib/completion/git-completion.bash). You should now see a the text file starting with  ``` # bash/zsh completion support for core Git. # # Copyright (C) 2006,2007 Shawn O. Pearce <spearce@spearce.org> # Conceptually based on gitcompletion (http://gitweb.hawaga.org.uk/). # Distributed under the GNU General Public License, version 2.0. ```  save this file as `git-completion.bash` to your user folder by pressing `CMD+s`. If you want to know where your user folder is, open a terminal and type ```pwd```. For Uli it is for example under `/Users/ubergmann`.  If you use Safari, make sure to save the file as `Page Source` and don't append a `.txt` to its filename (Chrome does this automatically). If everything went right, you can now type `ls` in your terminal window and should see `git-completion.bash` there between other files. -->  <!--  Follow the steps documented [here](https://git-scm.com/download/linux) to install on Linux from the terminal.  !!! danger     To install system software using `apt-get`, you need `Super User` rights. So please add `sudo` in front of each `apt-get` command in the document above, like so: ```sudo apt-get install git``` --> <!--  <!-- We will need to make Git accessible from the command line. Windows and Mac users will need to follow the steps on the page \"Modifying Path Settings.\" Linux users will already have git accessible from the command line. -->  <!-- To verify your installation, type the following command in a terminal and press the return key:  ```bash        git --version ```  You should get an output that looks like:  ```bash         git version 2.18.0 ```  Ensure that you have a version greater than `2.15.0` installed. --> "}, {"objectID": "./building-blocks/configure-your-computer/statistics-and-computation/matlab.md", "title": "MATLAB", "description": "MATLAB is a high-level programming language and an environment for numerical computation and visualization", "keywords": "Matlab, software, installation, configuration, statistics", "code": [], "headers": ["Installing MATLAB"], "content": "  MATLAB is a high-level programming language and an environment for numerical computation and visualization.  The University of Zurich has a campus-wide MATLAB license available to all students and employees. We will use this license to activate MATLAB on our machine.  If you study or work at Tilburg University and you do not have MATLAB installed on your machine, please register via [Mathworks](https://nl.mathworks.com/academia/tah-portal/tilburg-university-30348529.html) and follow the [installation instructions](https://servicedesk.uvt.nl/tas/public/ssp/content/detail/knowledgeitem?unid=9c895a61-020c-4692-92a7-96ef357211f7) available on the IT Self-Service Portal. "}, {"objectID": "./building-blocks/configure-your-computer/statistics-and-computation/python-packages.md", "title": "Python Packages", "description": "Learn how to install new Python packages with package management tools like pip.", "keywords": "python, pip, packages, selenium", "code": [], "headers": ["Installing Python packages", "Installation steps", "Common problems"], "content": "  Anaconda's Python Distribution comes with many of the packages we need to do scientific computing. If you're interested in all the packages included, click [here](https://docs.continuum.io/anaconda/packages/pkg-docs) and go to the Python 3.x tab.  However, you may come across packages that are not installed by default. In this case we recommend you use the `pip` package management tool to install them.  {{% warning %}} If your python 3 was found via `python3 --version` on the previous page, then type `pip3` instead of `pip` for all of the following python plugins. {{% /warning %}}  First let us update pip by typing the following into the terminal  ```bash pip install --upgrade pip ```  If you get an error, try typing instead: ```bash python -m pip install --upgrade pip ```  You also need the package `Selenium` as part of the web scraping tool kit we will build up. First let us install a depency for it via  ```bash pip install msgpack ```  We then install `selenium` by entering the following into a terminal:  ```bash pip install selenium ```  `pip` will then go through and install the package we asked for, and any other dependencies. If this succeeded, the last line it printed out should be:  ``` Successfully installed selenium-3.x.0 ```   {{% tip %}} **No administrator rights?**  If you do not have administrator rights to the computer you are using, please install packages only for your account. You can do so by typing `pip install --user packagename`, e.g., `pip install --user selenium`.  {{% /tip %}} "}, {"objectID": "./building-blocks/configure-your-computer/statistics-and-computation/pandoc.md", "title": "Pandoc", "description": "Pandoc is an extremely useful 'swiss army knife' for converting between different types of markup languages from the command line.", "keywords": "Pandoc, installation, software, configuration", "code": [], "headers": ["Installing Pandoc", "Linux", "Mac", "Windows", "Verify Your Installation"], "content": "  Pandoc is an extremely useful 'swiss army knife' for converting between different types of markup languages from the command line. For example, it readily builds PDFs with latex, and markdown - both of which are heavily used in academic research.  <!--We do not actively address how to use Pandoc - but we will utilize it in some lessons where we produce PDF, Word or HTML output from plain text files.-->   Open a terminal window and type  ```bash sudo apt install pandoc ```   to install pandoc from the command line   Open a terminal window and type ```bash brew install pandoc ```  to install pandoc from the command line   Go to the [Pandoc Homepage](https://pandoc.org/) and follow the installation instructions for your operating system.    Verify your install by typing the following into a command line:  ```bash pandoc --version ```  The expected output starts with the following information:  ```bash pandoc 1.19.2.1  ``` Ensure you have at least version 1.15.1 installed.  {{% warning %}} Because we want Pandoc available from the command line (it is by default for Mac and Linux), we again need to update our PATH settings.  Right-click on Computer. Then go to \"Properties\" and select the tab \"Advanced System Settings\". Choose \"Environment Variables\" and select `Path` from the list of system variables.  Check whether the following path has been added:          ;C:\\Users\\username\\AppData\\Local\\Pandoc  **Windows 7 or 8 machines:** If it has not been, and you accepted all defaults during your installation, and didn't have any other non-default setting prior to starting this guide, copy and paste the following string without spaces at the start or end, updating the `username`:          ;C:\\Users\\username\\AppData\\Local\\Pandoc  **Windows 10 machines:** If it has not been added, Click `New` and paste the following string, updating the username:          C:\\Users\\username\\AppData\\Local\\Pandoc  Click OK as often as needed.  After you have done this, open a **new** terminal and try and verify your install. {{% /warning %}} "}, {"objectID": "./building-blocks/configure-your-computer/statistics-and-computation/perl.md", "title": "Perl", "description": "Perl is an open programming language used for text processing, such as processing tables generated by other statistical softwares.", "keywords": "perl, installation, programming, language, configuration", "code": [], "headers": ["Installing Perl", "Mac and Linux", "Windows", "Verifying installation"], "content": "  Perl is an open programming language frequently used by engineers. We use Perl for text processing, such as processing tables generated by other statistical softwares.   Perl is already installed on Mac and Linux OS.   [Download Perl](https://www.perl.org/get.html) for Windows OS.  For Windows users to be able to use Perl from the command prompt, follow the steps below.  {{% warning %}} **Making Perl available via the PATH settings on Windows**  We need to update our PATH settings; these settings are a set of directories that Windows uses to \"look up\" software to startup.  - Right-click on Computer. - Go to \"Properties\" and select the tab \"Advanced System settings\". - Choose \"Environment Variables\" and select `Path` from the list of system variables. - Choose `Edit`. \t- **Windows 7 and 8 machines:** \t\tIf you chose the default installation directory, copy and paste the following string without spaces at the start or end:         `;c:\\Program Files (x86)\\Perl`  \t- **Windows 10 machines:** \t\t- Click `New` and paste the following string:         `c:\\Program Files (x86)\\Perl`  \t\t- Click on `OK` as often as needed. {{% /warning %}}   To verify that Perl has been correctly installed and configured via your PATH settings, open a **new** terminal interface and enter:  ```bash perl -v ```  followed by hitting the `Return` key. Perl is successfully installed if you can see the version of Perl in the command line. "}, {"objectID": "./building-blocks/collect-data/webscraping-apis/extract-data-api.md", "title": "Extract Data From APIs", "description": "Learn how to collect data from APIs.", "keywords": "api, application programming interface", "code": [" ```Python import requests search_url = \"https://icanhazdadjoke.com/search\"  response = requests.get(search_url,                         headers={\"Accept\": \"application/json\"},                         params={\"term\": \"cat\"}) joke_request = response.json() print(joke_request) ``` ", " ```Python def search_api(search_term):   search_url = \"https://icanhazdadjoke.com/search\"   response = requests.get(search_url,                         headers={\"Accept\": \"application/json\"},                         params={\"term\": search_term})   return response.json()  search_api(\"dog\") search_api(\"cow\") ``` ", "  ```python import requests url = 'https://www.reddit.com/r/marketing/about/moderators/.json'  headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=0', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}  response = requests.get(url, headers=headers) json_response = response.json() ``` "], "headers": ["Overview", "Examples", "icanhazdadjoke", "Reddit", "See Also"], "content": " An Application Programming Interface (API) is a version of a website intended for computers, rather than humans, to talk to one another. APIs are everywhere, and most are used to *provide data* (e.g., retrieve a user name and demographics), *functions* (e.g., start playing music from Spotify, turn on your lamps in your \"smart home\"), or *algorithms* (e.g., submit an image, retrieve a written text for what's on the image).  APIs work very similar to websites. At the core, you obtain code that computers can easily understand to process the content of a website, instead of obtaining the source code of a rendered website. APIs provide you with simpler and more scalable ways to obtain data, so you really have to understand how they work.  {{% warning %}} In practice, most APIs require user authentication to get started. Each and every API has its own workflow to generate a kind of \"password\" that you need to include in your requests. Therefore, you should always consult the documentation of the API you plan on using for configuration instructions. In this building block, we cover some of the common themes you encounter when working with APIs and provide examples. {{% /warning %}}  {{% tip %}} One of the major advantages of APIs is that you can directly access the data you need without all the hassle of selecting the right HTML tags. Another advantage is that you can often customize your API request (e.g., the first 100 comments or only posts about science), which may not always be possible in the web interface. Last, using APIs is legitimized by a web site (mostly, you will have to pay a license fee to use APIs!). So it's a more stable and legit way to retrieve web data compared to web scraping. That's also why we recommend using an API whenever possible. In practice, though, APIs really can't give you all the data you possibly want, and [web scraping](https://tilburgsciencehub.com/building-blocks/collect-data/web-scraping/web-scraping/ ) allows you to access complementary data (e.g., viewable on a website or somewhere hidden in the source code).  {{% /tip %}}    Every time you visit the [site](https://icanhazdadjoke.com), the site shows a random joke. From a technical perspective, each time a user opens the site, a little software program on the server makes an API call to the daddy joke API to draw a new joke to be displayed. Note that this is one of the few APIs that does not require any authentication.    *Multiple seeds*   Rather than having a fixed endpoint like the one above (e.g., always search for cat jokes), you can restructure your code to allow for variable input. For example, you may want to create a function `search_api()` that takes an input parameter `search_term` that you can assign any value you want.    *Pagination*   Transferring data is costly - not strictly in a monetary sense, but in time. So - APIs are typically very greedy in returning data. Ideally, they only produce a very targeted data point that is needed for the user to see. It saves the web site owner paying for bandwidth and guarantees that the site responds fast to user input.  By default, each page contains 20 jokes, where page 1 shows jokes 1 to 20, page 2 jokes 21 to 40, ..., and page 33 jokes 641 to 649.  You can adjust the number of results on each page (max. 30) with the limit parameter (e.g., `params={\"limit\": 10}`). In practice, almost every API on the web limits the results of an API call (100 is also a common cap). As an alternative, you can specify the current page number with the `page` parameter (e.g., `params={\"term\": \"\", \"page\": 2}`).     Reddit is a widespread American social news aggregation and discussion site. The service uses an API to generate the website's content and grants public access to the API.  To request data from the Reddit API, we need to include headers in our request. HTTP headers are a vital part of any API request, containing meta-data associated with the request (e.g., type of browser, language, expected data format, etc.).       * The Spotify Web API [tutorial](https://github.com/hannesdatta/course-odcm/blob/master/content/docs/tutorials/apisadvanced/api-advanced.ipynb) used in the Online Data Management Collection course illustrates how to generate access tokens and use a variety of API endpoints. "}, {"objectID": "./building-blocks/collect-data/webscraping-apis/read-write-data-apis.md", "title": "Read & Write Data From APIs", "description": "Learn how to store your API data locally and read in the data for future use.", "keywords": "api, application programming interface, read, write, export", "code": [" ```Python import csv with open(\"<FILENAME>.csv\", \"w\") as csv_file:     writer = csv.writer(csv_file, delimiter = \";\")     writer.writerow([\"<COLUMN_1>\", \"<COLUMN_2>\"])     for content in all_data:         writer.writerow([<DATAFRAME>[\"<COLUMN_1>\"], <DATAFRAME>[\"<COLUMN_2>\"]) ``` ", " ```Python import pandas as pd df = pd.read_csv(\"<FILENAME>.csv\", sep=\";\") ``` ", " ```Python import json with open(\"<NAME_OF_JSON_EXPORT>.json\", \"w\") as outfile:     json.dump(<JSON_FILE>, outfile) ``` ", " ```Python import json with open(\"<NAME_OF_JSON_EXPORT>.json\") as f:     data = json.load(f) ``` "], "headers": ["Overview", "Code", "Write Data to CSV", "Read CSV Data", "Write Data to JSON", "Read JSON Data"], "content": " After you have requested data from an API and extracted the required fields, you often want to convert the data into a format that is compatible with other software programs (e.g., Excel and R). A popular file format for tabular data is a Comma Separated Value (CSV) because it is simple, widespread, and compatible with most platforms. CSV files are file formats that contain plain text values separated by commas and can be opened by almost any spreadsheet program. In other cases, you may solely want to store the raw JavaScript Object Notation (JSON) data as a backup (simply because you don't know exactly what fields you'll need).   To faciliate writing to a CSV file, we'll make use of the `csv` library. If you want to add to an existing CSV file - rather than overwriting it - use the `a` flag (append) instead of the `w` flag (write).    {{% tip %}} If your data is already stored as a [pandas](https://pandas.pydata.org) DataFrame you can easily export it as follows: `<DATAFRAME>.to_csv(\"<FILENAME>.csv\", index=False)`. {{% /tip %}}   The `read.csv()` method from the `pandas` library automatically converts the data into a DataFrame which provides rich functionalities for data analysis.   {{% tip %}} If you encounter CSV files with a custom delimiter (i.e., symbol used to separate the data into rows and columns), you can explicitly indicate that with the `sep` parameter. For example, in this case the interpreter expects that data fields have been separated by semi-colons (`;`). {{% /tip %}}   The `json` packages makes exporting raw JSON data (`JSON_FILE`) straightforward:     In a similar way, you can import the JSON files with the same library.   "}, {"objectID": "./building-blocks/collect-data/webscraping-apis/scrape-static-websites.md", "title": "Scrape Static Websites", "description": "Learn how to scrape data and information from static websites.", "keywords": "scrape, webscraping, internet, beautifulsoup, static website", "code": ["  ```python import requests  url = \"https://www.abcdefhijklmnopqrstuvwxyz.nl\"  # make a get request to the URL request_object = requests.get(url)  # return the source code from the request object source_code = request_object.text ``` ", " ```Python base_url = # the fixed part of the URL num_pages = # the number of pages you want to scrape page_urls = []  for counter in range(1, num_pages+1):   full_url = base_url + \"page-\" + str(counter) + \".html\"   page_urls.append(full_url) ``` ", " ```Python from bs4 import BeautifulSoup  soup = BeautifulSoup(source_code, \"html.parser\")  # the first matching <h1> element print(soup.find('h1'))  # all matching <h2> elements (returns a list of elements) print(soup.find_all('h1'))  # first elementin in the list print(soup.find_all('h2')[0])  # strip HTML tags from element print(soup.find_all('h2')[0].get_text()) ``` ", " ```Python # element attributes soup.find(\"a\").attrs[\"href\"]  # HTML classes (note the underscore after class!) soup.find(class_ = \"<CLASS_NAME>\")  # HTML identifiers soup.find(id = \"<ID_NAME>\") ``` "], "headers": ["Overview", "Code", "Requests", "Seed Generation", "Beautifulsoup", "Finding content in a website's source code", "Selectors", "Advanced Use Case", "Task Scheduling", "See Also"], "content": " Say that you want to capture and analyze data from a website. Of course, you could simply copy-paste the data from each page but you would quickly run into issues. What if the data on the page gets updated (i.e., would you have time available to copy-paste the new data, too)? Or what if there are simply so many pages that you can't possibly do it all by hand (i.e., thousands of product pages)?  Web scraping can help you overcome these issues by *programmatically* grabbing data from the web. The tools best suited for the job depend on the type of website: static or dynamic. In this building block, we focus on the former which always return the same information.   Each site is made up of HTML code that you can view with, for example, Google Inspector. This source code contains data about the look-and-feel and contents of a website among others. To store the source code of a website into Python, you can use the `get()` method of the `requests` package:    In practice, you typically scrape data from a multitude of links (also known as the seed). Generally speaking, each URL has a fixed and variable part. The latter may be a page number that increases by increments of 1 (e.g., `page-1`, `page-2`). The code snippet below provides an example of how you can quickly generate such a list of URLs.    {{% tip %}} Rather than inserting a fixed number of pages (`num_pages`), you may want to leverage the page navigation menu instead. For example, you could extract the next page url (if it exists) from a \"Next\" button.   {{% /tip %}}    Next, once we have imported he `source_code`, it is a matter of extracting specific elements. The `BeautifulSoup` package has several built-in methods that simplify this process significantly.  The `.find()` and `.find_all()` methods search for matching HTML elements (e.g., `h1` is a header). While `.find()` always prints out the first matching element, `find_all()` captures all of them and returns them as a list. More often than not, you are specifically interested in the text you have extracted and less so in the HTML tags. To get rid of them, you can use the `.get_text()` method.     {{% tip %}} In practice, you often find yourself in situations that require chaining one or more commands, for example: `soup.find('table').find_all('tr')[1].find('td')` looks for the first column (`td`) of the second row (`tr`) in the `table`. {{% /tip %}}  Rather than searching by HTML tag, you can specify which elements to extract through attributes, classes and identifiers:    {{% tip %}} You can combine HTML tags and selectors like this:    ```soup.find(\"h1\", class_ = \"menu-header\"]``` {{% /tip %}}    See the building block on [task automation](http://tilburgsciencehub.com/building-blocks/automate-and-execute-your-work/automate-your-workflow/task-scheduling/) on how to schedule the execution of the web scraper (e.g., every day). Keep in mind that this only works with Python scripts, so if you're currently working in a Jupyter Notebook you need to transform it into `.py` file first.   * If you're aiming to strive a dynamic website, such as a social media site, please consult [this](https://tilburgsciencehub.com/building-blocks/scrape/dynamic-website) building block. "}, {"objectID": "./building-blocks/collect-data/webscraping-apis/avoid-getting-blocked.md", "title": "Avoid Getting Blocked While Scraping", "description": "Take steps to make sure your scraper keeps on running!", "keywords": "scrape, webscraping, headers, timers, proxies", "code": [" ```Python from time import sleep sleep(5) print(\"I'll be printed to the console after 5 seconds!\") ``` ", " ```Python headers = # a dictionary with header meta-data (user-agent, browser, etc.) requests.get(url, headers=headers) ``` "], "headers": ["Overview", "Code", "Timers", "HTTP Headers", "Proxies"], "content": " For better or worse, web servers can implement anti-scraping measures. For example, they want to protect users' privacy and avoid overloading their server by blocking unsuspicious traffic. To ensure the consistency of your data collection, it's therefore recommended to take steps to make sure your scraper keeps on running!   Briefly pausing between requests, rather than constantly visiting the same website, avoids that your IP address (i.e., numerical label assigned to each device connected to the internet) gets blocked, and you can no longer visit (and scrape) the website. For example, the `sleep` method below forces Python to wait for 5 seconds before it proceeds and executes the print statement.     Every time you visit a website meta-data associated with your HTTP request is sent to the server. This way, the server can distinguish a regular visitor from a bot or scraper, and may even decide to limit certain functionalities on the website. Some websites will automatically block requests with headers that indicate that they are accessing their server with a script rather than a regular web browser. Fortunately, you can work around this by passing a `headers` object to the `request` to set the meta-data to whatever you want.    The most common headers types are: * User-agent =  a string to tell the server what kind of device and browser you are accessing the page with. The Scrapy user agent [package](https://pypi.org/project/scrapy-user-agents/), for example, randomly rotates between a list of [user agents](https://developers.whatismybrowser.com/useragents/explore/software_name/googlebot/). * Accept-Language = preferred language (e.g., Russian may be more suspicious for a Dutch client's IP location). * Referer = the previous web page\u2019s address before the request is sent to the web server (e.g., a random origin website seem more plausible)   The idea is that you use an IP address that is not your own. Hence, if get blocked, you switch to another IP address. Either you can use a package like [`scrapy-proxy-pool`](https://github.com/rejoiceinhope/scrapy-proxy-pool) or you use a Virtual Private Network (VPN) to alternate between IP addresses. "}, {"objectID": "./building-blocks/collect-data/webscraping-apis/scrape-dynamic-websites.md", "title": "Scrape Dynamic Websites", "description": "Learn how to scrape data and information from dynamic websites.", "keywords": "scrape, webscraping, internet, beautifulsoup, website", "code": [" ```Python import selenium.webdriver  driver = selenium.webdriver.Chrome() driver.get(\"https://www.abcdefhijklmnopqrstuvwxyz.nl\")  # retrieve first H1 header driver.find_element_by_tag_name(\"h1\").text ``` ", " ```Python # HTML classes driver.find_element_by_class_name(\"<CLASS_NAME>\").text    # HTML identifiers () driver.find_element_by_id(\"<ID_NAME>\").text  # XPath driver.find_element_by_xpath(\"<XPATH>\").text  ``` ", " ```Python # scroll down the page driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')  # click on element (e.g., button) element_to_be_clicked = driver.find_element_by_class_name(\"<CLASS_NAME>\") element_to_be_clicked.click() ``` "], "headers": ["Overview", "Code", "Selenium", "Finding content in a website's source code", "Selectors", "User interactions", "Advanced Use Case", "Task Scheduling", "See Also"], "content": " While it's easy to get started with Beautifulsoup, it has limitations when it comes to dynamic websites. That is, websites of which the content changes after each page refresh. Selenium can handle both static and dynamic websites and mimic user behavior (e.g., scrolling, clicking, logging in). It launches another web browser window in which all actions are visible which makes it feel more intuitive. Here we outline the basic commands and installation instructions to get you started.    Running the code snippet below starts a new Google Chrome browser (`driver`) and then navigates to the specified URL. In other words, you can follow along with what the computer does behind the screens. Next, you can obtain specific website elements by tag name (e.g., `h1` is a header) similar to BeautifulSoup.     {{% tip %}} The first time you will need to (1) install the Python package for Selenium, (2) download a web driver to interface with a web browser, and (3) configure Selenium to recognize your web driver. 1. Open Anaconda Prompt (Windows) or the Terminal (Mac), type the command `conda install selenium`, and agree to whatever the package manager wants to install or update (usually by pressing `y` to confirm your choice). 2. Once we run the scraper, a Chrome browser launches which requires a web driver executable file. Download this file from [here](https://sites.google.com/a/chromium.org/chromedriver/downloads) (open [this](https://www.whatismybrowser.com/detect/what-version-of-chrome-do-i-have) site in Chrome to identify your current Chrome version). 3. Unzip the file and move it to the same directory where you're running this notebook. Either you can set a `chrome_path` manually (e.g., `/Users/Pieter/Documents/webscraping/chromedriver`) or ideally you make Chromedriver available through a so-called PATH variable so that you can access it regardless of your current directory. To do so, ...     * Windows users can follow the steps described [here](https://tilburgsciencehub.com/building-blocks/configure-your-computer/task-specific-configurations/configuring-python-for-webscraping/). If that doesn't work for you, [this](https://zwbetz.com/download-chromedriver-binary-and-add-to-your-path-for-automated-functional-testing/) guide may help.     * Mac users need to move the `chromedriver` file to the `/usr/local/bin` folder. `cd` to the directory of the `chromedriver` file and type `mv chromedriver /usr/local/bin`. {{% /tip %}}  Alternatively, you can specify which elements to extract through attributes, classes and identifiers:     {{% tip %}} Within Google Inspector you can easily obtain the XPath by right-clicking on an element and selecting: \"Copy\" > \"Copy XPath\". {{% /tip %}}   One of the distinguishable features of Selenium is the ability to mimic user interactions which can be vital to get to the data you are after. For example, older tweets are only loaded once you scroll down the page.      See the building block on [task automation](http://tilburgsciencehub.com/building-blocks/automate-and-execute-your-work/automate-your-workflow/task-scheduling/) on how to schedule the execution of the web scraper (e.g., every day). Keep in mind that this only works with Python scripts, so if you're currently working in a Jupyter Notebook you need to transform it into `.py` file first.   * Looking for a simple solution that does the job without any bells and whistles? Try out the BeautifulSoup package and review [this](https://tilburgsciencehub.com/building-blocks/scrape/static-website) complementary building block. "}, {"objectID": "./building-blocks/collect-data/workflows-for-online-data-collection/ensure-legal-compliance-web-scraping.md", "title": "Safeguard Legal Compliance When Scraping", "description": "Obtain legal advice to limit your exposure to legal risks when web scraping", "keywords": "legal, debug, assessment, data quality, webscraping, scraping", "code": [], "headers": ["Overview", "Key Issues - Legal Advice", "1. Purpose and framing of the research objective", "2. Scale and scope of data capture", "3. Characteristics of the data source (website/API)", "4. Relationship of researcher with the data provider", "5. Data management and usage"], "content": " The legality of web scraping is an ongoing and complex issue, which might be perplexing for scholars interested in collecting and using web data. There is no clear consensus about whether collecting web data for scientific purposes is permissible under American and international intellectual and cybersecurity laws. Therefore, researchers should obtain legal advice to limit their exposure to legal risks when going forward. As legal experts may not be fully aware of all the steps involved in collecting and using web data, it is imperative to address aspects such as the purpose and framing of the research objective, the scale and scope of data capture, the characteristics of the data source, the relationship of the researcher with the data provider, and any details on the researcher\u2019s data management and usage. Here we provide a template for researchers to prepare such meetings.   Collecting data via web scraping or APIs may violate a firm\u2019s intellectual property or contractual rights. Valid reasons (e.g., including but not limited to a research project's scientific objective) may exempt researchers from these rules.  * Is the exact research question known/formulated? * Is answering the research question societally relevant and urgent? * Is the research carried out by members of a recognized research organization (e.g., university, research institute, other organization with a primary goal of conducting scientific research)? What is the role of the person(s) involved in collecting the data within that organization (e.g., student, employee)? * Has the research project been approved by the institution\u2019s legal or ethical review board? * How much does the study depend on the web data (e.g., is the data entirely based on one scraped data set, or only vaguely uses scraped data for some less-critical control variables)? * Is the data provider the only data source with comparable data? Do viable alternatives exist (e.g., feasibility to gather alternative data, including projected resource use)? * Are private parties involved in the project (e.g., for knowledge dissemination, as part of an industry collaboration), or is it a purely scientific project? * Are the results of the scientific research capable of commercial exploitation? If so, what product will be commercially exploited (e.g., the collected data, the results based on the data, a product developed based on the data)? Who will be exploiting the results for commercial purposes (e.g., the research organization vs. potential corporate collaborators)? * Does any party have preferential access to the data, results, or derived product (e.g., an artificial intelligence algorithm trained on the data) of the research project?   Web data collections vary in **scale** (e.g., number of instances and entities collected) and **scope** (e.g., real-time data collection for an unforeseeable time vs. a one-shot data collection within a day).  * What type of data is collected (e.g., on whom, from which location, has permission been obtained from users, has permission been obtained from the website or API \u2013 either implicitly or explicitly, does it contain personal or sensitive data? Is obtaining permission feasible?)? * What is the scope of the data extraction (e.g., the absolute volume of data extracted, volume relative to all data available on the website; what is the frequency of the data collection (e.g., once, at regular intervals, in real-time); can the retrieval limit be determined and respected?)? * Which extraction technology is used (e.g., identifying as a data collector, degree of intrusiveness, software toolkit)? * How is the data extraction software deployed (e.g., outsourced to a data collector vs. self-administered via commercial tools, Python packages, or entirely self-coded software)? * From which location is the data extraction software deployed (e.g., geographical region, part of the research organization\u2019s infrastructure or not)?   Some types of data providers explicitly make data available for use in scientific research projects. Others merely provide data as a way to grow their business ecosystem (\u201cdeveloper API\u201d). * Where is the website or API owner located (e.g., registered office, central administration, place of business)? From where is the data extracted (e.g., server location)? * What type of data provider is data being extracted from (e.g., scraped from a website, collected from an API; is the data provider explicitly offering the data for extraction, or is the data provided as part of a development platform? Is the data source aggregating/collecting data from third parties (e.g., through web scraping or APIs)? Does the data provider have the right to display the data?)? * How public is the data access (e.g., public on the web, hidden behind a login screen but publicly accessible for anyone, hidden after login screen only accessible with special permissions)? * Is the displayed data personalized or customized in any way (e.g., not personalized, customized for a larger group of persons, customized for a specific user)? * Could potential harm be inflicted from using the data, either for the users/entities or pertaining to the firm\u2019s business model (e.g., competition)? What is the risk for users and/or firms if the raw data would become public unintentionally? * Does the data provider disallow the use of automated collections, either via robots.txt, the site\u2019s or API\u2019s terms of use, or any other contracts? Have contracts been accepted (implicitly by continuing to use the site/service, or explicitly by signing up for an account with the website or service)? * Does the data provider attach a specific license to the use of the data, and what does this license imply for subsequent data use?  Some of the legal concerns could be resolved if the researcher obtained the data provider\u2019s permission.  * Has the data provider approved the data collection (e.g., implicitly or explicitly)? * What are the conditions under which approval has been given (e.g., disguise the identity of the data provider in the paper)? * Has the data provider been notified about the data collection (e.g., How? How many times? Has the data provider reacted?)? Would notification be feasible? * Are any fees being paid for the data collection (e.g., for an API)?   A researcher may risk violating privacy regulations or other laws when storing, working with, or sharing the data. * Is the raw data being stored during the collection or directly parsed? * How (e.g., file server, file format), why (e.g., reproducibility/verification of research results), and for how long is the raw data stored after the collection? * Is the data publicly accessible, are shared with members of a research unit or department? Or is the data only accessible to coauthors? * How will the stored data be used? How has the raw data been aggregated? Have entities been disguised (anonymization?) * At what level of detail are statistical results reported (e.g., summary statistics, correlation tables, parameter estimates from a model, cross tabs, package with a trained machine learning model based on the data)? * Has a license been chosen for the final data set? Is the license compatible with potentially inherited licenses (e.g., creative commons)? "}, {"objectID": "./building-blocks/collect-data/workflows-for-online-data-collection/data-availability-assessment.md", "title": "Assess what Data is Available", "description": "Learn how to assess which particular data is available on the site or API.", "keywords": "data availability, assessment, API, webscraping, scraping", "code": [], "headers": ["Overview", "Entity Coverage and Linkages", "Which entities are available?", "How many entities are available?", "How are entities identified?", "How are entities linked to one another?", "How can entities be linked to external entities?", "Which lists could serve as potential seeds?", "Time Coverage", "For what time period is data available?", "How is time encoded, and how accurate is it?", "Can data be modified after it has been published?", "How often is the site/endpoint refreshed?", "Algorithmic Transparency", "Which mechanisms affect the display of data?", "Can the researcher exert control over the data display?"], "content": "  Assessing data availability is vital to guarantee a minimum level of rigor. Investigating data availability may help you to enhance the relevance of your study.    Familiarize yourself with the structure of the website or API to understand which entities (e.g., consumers, products, reviews) are available.  {{% example %}} An ecommerce website like Amazon lists information on the products, sellers, reviews, and reviewers. {{% /example %}}  Try to understand how many entities are available on the site, and how many of those you can actually retrieve.  {{% example %}} The category pages on Amazon.com only list a few hundred products - out of potentially ten thousand of products per category. So, while it seems data is abundantly available, it\u2019s not so straightforward that all data can actually be retrieved easily. {{% /example %}}  The location of most entities typically consists of a URL of a website or API appended with an identification number.  {{% example %}} Amazon uses ASINs (UPCs) to point to a product page (e.g., https://www.amazon.com/dp/B087BC4DJH/), and the same ID is used to also point to the review pages (e.g., https://www.amazon.com/product-reviews/B087BC4DJH/). {{% /example %}}  A crucial assessment to make is how entities are linked to one another, if at all.  {{% example %}} The product overview pages at Amazon list the ASINs of each product in the website\u2019s source code. Using this list, you can then visit the product pages of each product, and thereby start constructing your data set. {{% /example %}}   After assessing the internal linkage structure, critically reflect how entities may potentially be linked to external data sources.  {{% example %}} In its early days, music intelligence provider Echonest.com allowed users to query their API for so-called Musicbrainz IDs. These identifiers were and still are used widely by other services. {{% /example %}}   Which entities serve as an entry point for your data collection. These \u201centry points\u201d are commonly referred to as \u201cseeds\u201d.  {{% example %}} Datta et al. (2018) visited the service\u2019s \u201crecently active users\u201d page for a duration of 1 months, collecting thousands of user names, from which a final sample of 5,000 users was drawn that entered the actual data collection. {{% /example %}}   The time period that your website or API covers.  {{% example %}} Amazon lists historical data on reviews which means you can easily go back in time and download all of these reviews. {{% /example %}}   Timestamps can be given in a users' time zone but also in UTC. Check whether you need to convert the time to a common format for storage. Also, check for accuracy of the timestamps as some providers aggregate timestamps in descriptions (e.g., \"more than a year ago\")  {{% example %}} In the Spotify playlist data retrieved via the Chartmetric API, most of the music tracks have been added to playlists in January, 2016. However, that does not mean it actually took place like this. In fact, the addition dates in January 2016 merely reflect the starting point of Chartmetric\u2019s data collection. {{% /example %}}   What looks like archival data does not need to be strictly archival. Users can often change and delete their contributions after publication.  {{% example %}} Social music network Last.fm set the bootstrap flag to 1 in case users have reset their profiles, which may render their historical data incomplete. {{% /example %}}   When you plan to scrape data in real-time from a website, try to get a feeling for how often the site is actually refreshed. This may help you to make a decision on how often you need to collect data from the site.     Design choices and algorithms that make a site easily navigable can cause problems in collecting and using data for scientific purposes. Typical mechanisms that affect the display and retrieval of data are: sorting algorithms, recommendation algorithms, experimental conditions, and sampling.  {{% example %}} Suppose you want to calculate average prices in a product category, and you start scraping data from Amazon.com. Chance is you\u2019ll end up scraping prices for only the most popular products - which certainly are not representative of the whole product assortment on the platform. {{% /example %}}   When screening a site for data availability, it\u2019s crucial to look out for options to exert control about which data is shown.  {{% example %}} You can sort products alphabetically, which - arguably - isn\u2019t related to popularity, and may hence be a better sampling scheme if you\u2019re interested in random samples. {{% /example %}} "}, {"objectID": "./building-blocks/collect-data/workflows-for-online-data-collection/online-data-collection-management.md", "title": "Online Data Collection and Management", "description": "Learn to use web scraping and APIs to collect data for your empirical research project.", "keywords": "odcm, scraping, api, collection", "code": [], "headers": [], "content": " The **[Online Data Collection and Management](https://odcm.hannesdatta.com)** course is an open-source Master level course taught at Tilburg University. All of its content is freely available and consists of lectures, live streams, self-study material, tutorials and examples.  The course teaches you the nuts and bolts about collecting data from the web. Unlike most other courses on this topic, this one not only teaches you the technicalities of using web scraping and APIs, but also introduces a comprehensive framework that helps you to think about scraping - specifically with regard to its application in academic marketing research.  {{% cta-primary-center \"Get started now\" \"https://odcm.hannesdatta.com\" %}} "}, {"objectID": "./building-blocks/collect-data/workflows-for-online-data-collection/calculate-sample-size.md", "title": "Calculate Sample Sizes for Web Scrapers", "description": "Learn how to determine the number of units that can be obtained from a website or API.", "keywords": "sample size, sample, n, data collection, compute", "code": [], "headers": ["Overview", "Formula", "Example"], "content": " Sampling from websites and APIs can be tricky: limits to server load (e.g., retrieval limits) or snowballing effects (e.g., seed of 100 users, sample 100 of their peers and obtain all of their peers\u2019 consumption patterns for 50 weeks). In addition to the minimum sample size necessary to satisfy statistical power requirements, an important consideration is therefore the *technically feasible sample size*. That is, the sample size that can be obtained from a website or API while considering resource constraints.   {{< katex display  >}} N = \\frac{req \\times S}{r \\times freq} {{< / katex >}}  whereby - $N$ = sample size (i.e., number of instances of an entity to extract data from), - $req$ = retrieval limit (maximum number of requests per time unit, allowed for each scraper or authenticated API user), - $S$ = number of scrapers used (e.g., computers with separate IP addresses, or authenticated users of an API), - $r$ = number of URL calls to make to obtain data for each instance, and - $freq$ = the desired sampling frequency for each entity per time unit.  {{% tip %}} Convert all input parameters to the same time unit (e.g., the retrieval limit may initially be expressed in fifteen-minute intervals but needs to match the desired sampling frequency, which may be expressed in hours)! {{% /tip %}}   Suppose you wish to know the technically feasible sample size for collecting data from an online social network. In other words, you want to solve for $N$.  The input parameters are:  - $req$ = 5 requests per second = 5 x 60 x 60 requests per hour (18,000) - $S$ = 1 scraper, authenticated via the service's API - $r$ = 2 (the scraper needs to visit two URLs: one to obtain users' meta data, and one to obtain suers' usage history) - $freq$ = Each user should be vsisited at least once every fifteen minutes (once every 15 minutes = 4 times per hour).  {{< katex display  >}} N = \\frac{req \\times S}{r \\times freq} = \\frac{18,000 \\times 1}{2 \\times 4} = 2,250  {{< / katex >}} "}, {"objectID": "./building-blocks/collect-data/workflows-for-online-data-collection/monitor-data-quality.md", "title": "Monitor and Safeguard Your Data Quality", "description": "10 common issues and potential solutions to troubleshoot your data collection", "keywords": "monitor, debug, assessment, data quality, webscraping, scraping", "code": [], "headers": ["Overview", "10 Common Issues", "1. It cannot be verified whether all the data that should have been collected was indeed collected.", "2. The data collection has been interrupted or stopped unexpectedly.", "3. It is cumbersome/takes a lot of time to conduct quality checks.", "4. The data collection takes place in a highly dynamic environment.", "5. The data collection breaks frequently.", "6. The data collection is unexpectedly slow.", "7. Less data than expected is retrieved.", "8. Disk space is exceeded, or many files are generated.", "9. The computer, which collects the data, crashes.", "10. Cloud service provider bills exceed projected costs."], "content": " After carefully planning and prototyping, researchers can begin the actual data collection. It is important to note that the data collection process is best considered \u201cwork-in-progress.\u201d Thus, researchers need to remain agile and adapt the code where required. Here we outline the 10 most common data quality issues and how you can resolve them.   * Log each web request (i.e., URL call), along with response status codes, timestamps of when the collection was started, and when the request was made. * Save raw HTML websites, along with the parsed data, and compare them.  * Verify timestamps in log files or content of parsed data to gauge severity. * Try to identify and fix the cause of the interruption. * Record issue in a logbook (e.g., in the documentation) and notify potential users of the data. * Set up a monitoring tool to timely alert you to any future issues. * Move  data collection to a more stable environment (e.g., cloud computer rather than local office PC).  * Automatically generate reports on data quality (e.g., using RMarkdown). * Automate data preparation workflows.  * Monitor the data collection environment (e.g., subscribe to the focal firm\u2019s blog, keep an eye on the websites of related firms, etc., to identify any additional data that needs to be captured). Record any important events in the documentation.  * Choose more stable data selectors (e.g., CSS is less stable than HTML tag words; \u201cEnglish\u201d class names may better than cryptic class names, etc.). * Make use of error handling to avoid interruptions (e.g., try and except in Python), but do not blindly ignore any error. * Store raw HTML files, along with parsed data for recovery of data. * Consider using an API (over web scraping).  * Check whether you obey the retrieval limit or fair use policy and implement timers/pauses where possible. * Check for traces of being banned/blocked/slowed down by the website (e.g., by investigating the retrieved content). * Notify data provider(s) about potential bandwidth issues (e.g., in the case of using a provider\u2019s API). * Update the technically feasible retrieval limit, and re-calculate desired sample size, extraction frequency, etc.  * Check for any time-outs and erroneous server responses. * Verify that the extraction software is suitable for the type of data source (e.g., static vs. dynamic websites). * Store raw HTML files of websites or JSON response of an API during the datacollection, inspect what data is available in the raw data, and verify it has been parsed correctly.  * Move data to a remote file location (potentially zip files before uploading them to save bandwidth, e.g., every day). * Make use of external databases (e.g., by the university or in the cloud).  * Verify whether the computer has had an uninterrupted power supply (and ask University to place the computer on a secure power line or notify you about planned power outages). * Move scraping software to the cloud, implement checks that data collection runs.  * Verify that computing resources are appropriate (e.g., downscale servers on which collection scripts run, verify that database runs optimally). Consider the costs of data transfer. * Verify whether any backup data can be moved to a different location or placed in a glacier for which lower storage costs will be charged. * Consider writing data to files after collection, rather than keeping them in an actively running database in the cloud. "}, {"objectID": "./building-blocks/prepare-your-data-for-analysis/data-preparation/large-datasets-R.md", "title": "Import Large Datasets Into R", "description": "This is a handy function that can be used to clean social media data scraped from the web.", "keywords": "import, data, data preparation, big data, large datsets, memory, RAM", "code": [" ```R # import package library(data.table)  # import data with data.table package df <- fread(<YOUR_DATASET.csv>)  # only import the first couple of rows for exploratory analysis  df <- fread(<YOUR_DATASET.csv>, nrows=500)  # only import the data you actually use  df <- fread(<YOUR_DATASET.csv>, select=c(1, 2, 5))  # column indices df <- fread(<YOUR_DATASET.csv>, select=c(\"date\", \"country\", \"revenue\"))  # column names  # print object size in bytes (for a quick comparison) object.size(df)  # store the derivative file for future use fwrite(df, <YOUR_CLEANED_DATSET.csv>) ``` "], "headers": ["Overview", "Code"], "content": "  Many R-users rely on the `dplyr` or `read.table` packages to import their datasets as a dataframe. Although this works well for relatively small datasets, we recommend using the `data.table` R package instead because it is significantly faster. This building block provides you with some practical tips for dealing with large datsets in R.  As a starting point, make sure to clean your working environment in RStudio. Oftentimes, there are datasets stored memory that you have worked with earlier but you're no longer using. Click on the broom icon in the top right window to remove all objects from the environment.   In addition, switching from the `read.csv()` function to `fread()` can greatly improve the performance of your programme in our experience. Below we illustrate how you can import a (subset of the) data, determine the object size, and store the derivative version of the file for future use.    "}, {"objectID": "./building-blocks/prepare-your-data-for-analysis/data-preparation/data-preparation-workflow-management.md", "title": "Data Preparation and Workflow Management", "description": "Engineer data sets from complex raw data and manage research projects efficiently.", "keywords": "dprep, preparation, raw data, cleaning, wrangling", "code": [], "headers": [], "content": " The **[Data Preparation and Workflow Management](https://dprep.hannesdatta.com)** course is an open-source Master level course taught at Tilburg University. All of its content is freely available and consists of lectures, live streams, self-study material, tutorials and examples.  This course teaches you how to engineer data sets for statistical analysis. Many students and researchers perceive the process of \u201ccreating\u201d a data set for analysis as rather simplistic: a bit of cleaning here, a bit of merging there, and you\u2019re done. In this course, we take data preparation to the next level, by considering highly complex data preparation workflows (think multiple sources, structured and unstructured data, data from databases and data from files, multiple delivery batches, lots of missing data, different file versions, etc.).  And of course, throughout the course, we\u2019ll be using the workflow principles of reproducible science that we advocate at Tilburg Science Hub.   {{% cta-primary-center \"Get started now\" \"https://dprep.hannesdatta.com\" %}} "}, {"objectID": "./building-blocks/prepare-your-data-for-analysis/data-preparation/clean-social-media-counts.md", "title": "Clean Data Scraped From Social Media", "description": "This is a handy function that can be used to clean social media data scraped from the web.", "keywords": "clean, wrangling, scraping, follow, likes, network", "code": [" ```R # Function to convert textual social media counts to proper digits social_media_cleanup <- function(x) {   if (class(x)%in%c('integer','numeric')) {     warning('Input is already numeric.')   }   numerics <- gsub('[A-Za-z]','',x)   units <- gsub('[0-9]|[.]|[,]','',x)   multipliers <- rep(1, length(x))   multipliers[grepl('K', units, ignore.case = T)]<-1000   multipliers[grepl('M', units, ignore.case = T)]<-1E6   multipliers[grepl('B', units, ignore.case = T)]<-1E9    return(as.numeric(numerics)*multipliers) }  # Example social_media_cleanup(c('21.5k', '214m', '1204', 'NA', '642b')) ``` "], "headers": ["Overview", "Code"], "content": "  This is a handy function that can be used to clean social media data scraped from the web.  Usually, when scraping social media, the output data can contain letters like K's (Thousands), M's (Millions) and B's (Billions). You are won't be able to analyze them unless you first replace these letters with the appropriate zero digits.    "}, {"objectID": "./building-blocks/prepare-your-data-for-analysis/data-preparation/manipulate-data.md", "title": "Manipulate Data", "description": "GitHub limits the size of files allowed in repositories. Use Git-LFS, an open-source Git extension for large file storage.", "keywords": "manipulate, cleaning, wrangling, prep, data", "code": [], "headers": [], "content": " [Chris Albon](https://chrisalbon.com/), Director of Machine Learning at the Wikimedia Foundation, hosts a great selection of notes and code snippets on data science, machine learning, data manipulation, data visualization, and much more.  {{% cta-primary \"Go to chrisalbon.com\" \"https://chrisalbon.com/\" %}} "}, {"objectID": "./building-blocks/organize-your-project/checklists-to-audit-your-project.md", "title": "Checklists to Audit your Project", "description": "", "keywords": "", "code": [], "headers": [], "content": ""}, {"objectID": "./building-blocks/organize-your-project/mirror-your-code-to-gitHub.md", "title": "Mirror your Code to GitHub", "description": "", "keywords": "", "code": [], "headers": [], "content": ""}, {"objectID": "./building-blocks/organize-your-project/pipelines-and-project-components.md", "title": "Pipelines and Project Components", "description": "", "keywords": "", "code": [], "headers": [], "content": ""}, {"objectID": "./building-blocks/organize-your-project/data-management-and-directory-structure.md", "title": "Data Management and Directory Structure", "description": "", "keywords": "", "code": [], "headers": [], "content": ""}, {"objectID": "./building-blocks/organize-your-project/remote-raw-data-setup/_index.md", "title": "Remote Raw Data Setup", "description": "", "keywords": "", "code": [], "headers": [], "content": ""}, {"objectID": "./building-blocks/collaborate-and-share-your-work/publish-on-the-web/shiny-apps.md", "title": "Build Interactive Dashboards With R Shiny", "description": "Learn how to build your own interactive R Shiny app.", "keywords": "shiny, app, data visualisation, dashboard, r, dataviz, plots, charts", "code": [" ```R library(shiny) ui <- fluidPage() server <- function(input, output){} shinyApp(ui = ui, server = server) ``` ", " ```R ui <- fluidPage( \tsidebarLayout( \t\tsidebarPanel( \t\t\t\"This is the sidebar\" \t\t), \t\tmainPanel( \t\t\t\"Main panel goes here\" \t\t) \t) ) ``` ", " ```R tabsetPanel( \ttabPanel(title = \"tab 1\",     h1(\"Overview\"),     \"Content goes here\"), \ttabPanel(title = \"tab 2\", \"The content of the second tab\"), \ttabPanel(title = \"tab 3\", \"The content of the third tab\") ) ``` ", " ```R # ui plotOutput(outputId = \"plot\"), tableOutput(outputId = \"table\"), textOutput(outputId = \"text\")  # ----------------------------  # server output$plot <- renderPlot({ \t               plot(x, y, ...) })  output$table <- renderTable({ \t               data })  output$text <- renderText({ \t           \"Some text\" }) ``` ", " ```R # textbox that accepts both numeric and alphanumeric input textInput(inputId = \"title\", label=\"Text box title\", value = \"Text box content\") ``` ", " ```R # text box that only accepts numeric data between 1 and 30 numericInput(inputId = \"num\", label = \"Number of cars to show\", value = 10, min = 1, max = 30) ``` ", " ```R # slider that goes from 35 to 42 degrees with increments of 0.1 sliderInput(inputId = \"temperature\", label = \"Body temperature\", min = 35, max = 42, value = 37.5, step = 0.1) ``` ", " ```R # slider that allows the user to set a range (rather than a single value) sliderInput(inputId = \"price\", label = \"Price (\u20ac)\", value = c(39, 69), min = 0, max = 99) ``` ", " ```R # input field that allows for a single selection radioButtons(inputId = \"radio\", label = \"Choose your preferred time slot\", choices = c(\"09:00 - 09:30\", \"09:30 - 10:00\", \"10:00 - 10:30\", \"10:30 - 11:00\", \"11:00 - 11:30\"), selected = \"10:00 - 10:30\") ``` ", " ```R # a dropdown menu is useful when you have plenty of options and you don't want to list them all below one another selectInput(inputId = \"major\", label = \"Major\", choices = c(\"Business Administration\", \"Data Science\", \"Econometrics & Operations Research\", \"Economics\", \"Liberal Arts\", \"Industrial Engineering\", \"Marketing Management\", \"Marketing Analytics\", \"Psychology\"), \tselected = \"Marketing Analytics\") ``` ", " ```R # dropdown menu that allows for multiple selections (e.g., both R and JavaScript) selectInput(inputId = \"programming_language\", label = \"Programming Languages\", \tchoices = c(\"HTML\", \"CSS\", \"JavaScript\", \"Python\", \"R\", \"Stata\"), \tselected = \"R\", multiple = TRUE) ``` ", " ```R # often used to let the user confirm their agreement checkboxInput(inputId = \"agree\", label = \"I agree to the terms and conditions\", value=TRUE) ``` ", " ```R # either insert a hexadecmial color code or use the interactive picker library(colourpicker)  # you may first need to install the package colourInput(input = \"colour\", label = \"Select a colour\", value = \"blue\") ``` ", " ```R ui <- fluidPage(   downloadButton(outputId = \"download_data\", label = \"Download\") )  server <- function(input, output) {     output$download_data <- downloadHandler(         filename = \"download_data.csv\",         content = function(file) {             data <- filtered_data()               write.csv(data, file, row.names = FALSE)         }     ) } ``` "], "headers": ["What is a Shiny App?", "Code", "Skeleton", "Lay-out", "Placeholders", "Control Widgets", "Download Button", "An Example", "See Also"], "content": " The **[Shiny library](https://shiny.rstudio.com)** helps you turn your analyses into interactive web applications without requiring HTML, CSS, or Javascript knowledge, and provides a powerful web framework for building web applications using R.  Being able to create Shiny apps is a great skill to have because it enables you to communicate your insights to non-technical stakeholders and give them the tools to conduct their own analysis!  ![R Shiny](https://shiny.rstudio.com/images/shinySiteBandOne.png)   The skeleton of any Shiny app consists of a user interface (`ui`) and a `server`. The UI is where the visual elements are placed such as a scatter plot or dropdown menu. The server is where the logic of the app is implemented, for example, what happens once you click on the download button. And this exactly where Shiny shines: combining inputs with outputs.      **Sidebar**   Create a 2-column structure with a small panel on the left and a main panel on the right.     **Tabs**   Distribute your data across multiple tabs (alternative to a sidebar layout).    {{% example %}}  ![Tabs](../images/tabs.png)  {{% /example %}}  Define a placeholder for plots, tables, and text in the user interface (`ui`) and server side (`server`). * Text can be formatted as headers (e.g., `h1()`, `h2()`) or printed in bold (`strong()`) or italics (`em()`) format. * The [`ggplotly()` function](https://www.rdocumentation.org/packages/plotly/versions/4.9.3/topics/ggplotly) can convert a `ggplot2` plot into an interactive one (e.g., move, zoom, export image features that are not available in the standard `renderPlot()` function). *   Similarly, the `DT::dataTableOutput(\"table\")` (in the `ui`) and the `DT::renderDataTable()` (in the `server`) from the `DT` package enrich the `renderTable` function. See a live example [here](https://royklaassebos.shinyapps.io/dPrep_Demo_Google_Mobility/).         **Text box**    {{% example %}} ![Text Box](../images/text_box.png) {{% /example %}}  ---  **Numeric input**    {{% example %}} ![Numeric input](../images/numeric_input.png) {{% /example %}}  ---  **Slider**    {{% example %}} ![Slider](../images/slider_regular.png) {{% /example %}}  ---  **Range selector**     {{% example %}} ![Range selector](../images/range_slider.png) {{% /example %}}  ---  **Radio buttons**    {{% example %}} ![Radio buttons](../images/radio_button.png) {{% /example %}}  ---  **Dropdown menu**    {{% example %}} ![Dropdown menu](../images/dropdown_menu.png) {{% /example %}}  ---  **Dropdown menu (multiple selections)**    {{% example %}} ![Dropdown menu multiple selections](../images/dropdown_menu_multiple.png) {{% /example %}}  ---  **Checkbox**    {{% example %}} ![Checkbox](../images/checkbox.png) {{% /example %}}  ---  **Colorpicker**    {{% example %}} ![Checkbox](../images/colour_picker.png) {{% /example %}}  Add a download button to your Shiny app so that users can directly download their current data selection in csv-format and open the data in a spreadsheet program (e.g., Excel).     The [Shiny app](https://royklaassebos.shinyapps.io/dPrep_Demo_Google_Mobility/) below  visualizes Google\u2019s COVID-19 Community Mobility Reports of the Netherlands. A step-by-step tutorial (incl. source code) can be found [here](https://dprep.hannesdatta.com/docs/building-blocks/deployment-reporting/).  ![](../images/demo_app.png)  * [A course on learning Shiny](https://debruine.github.io/shinyintro/) * [Interactive Web Apps with shiny Cheat Sheet](https://shiny.rstudio.com/images/shiny-cheatsheet.pdf) * [Shiny User Showcase](https://shiny.rstudio.com/gallery/) "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/project_management/team-transition.md", "title": "Bring Open Science Practices to your Research Group", "description": "Learn from how world-leading research groups have embraced open science practices to re-think the way they work on research projects.", "keywords": "team, onboarding, department", "code": [], "headers": ["Overview", "Experiences in transitioning to open science", "Training material"], "content": "  Open Science practices can boost your enjoyment and productivity when working on empirical research projects. Yet, imagine not only you, but also other members of your department and school to use similar tools and methods to collaborate?  Luckily, you don't have to reinvent the wheel and can follow best-practices of leading research groups.   - [Lowndes et al. 2019, \"Supercharge your research - a ten-week plan for open data science\"](https://doi.org/10.1038/d41586-019-03335-4)  - [Lowndes et al. 2017, \"Our path to better science in less time using open data science tools\"](https://www.nature.com/articles/s41559-017-0160)  - [Markowetz 2015, \"Five selfish reasons to work reproducibly\"](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0850-7)   - Openscapes.org's [\"Champions Program\"](https://www.openscapes.org/champions/) (including an [ebook](https://openscapes.github.io/series)) - Ohi-Science's [Data Science Training](http://ohi-science.org/data-science-training/) "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/project_management/git-project-board.md", "title": "Manage your Projects using the GitHub Project Board", "description": "Projects boards on GitHub help you organize and prioritize your work using the Scrum framework for project management.", "keywords": "git, github, project board, sprint, planning", "code": [], "headers": ["Overview", "Get Started with GitHub Projects"], "content": "  Projects boards on GitHub help you organize and prioritize your work using the [Scrum framework for project management](https://tilburgsciencehub.com/learn/scrum). The benefit from project boards is that you can link your repositories. This way all issues that are related to different projects can be organized in a unique project board.   - On your profile page on GitHub, you can find a tab \u201cProjects\u201d that enables you to view all your current projects board and make a \u201cNew Project\u201d, for instance:    ![Projects](../projects.png)  - Provide a name and a description for your project  - Set \u201cBasic Kanban\u201d as your project board template:    ![kanban](../basicKanban.png)    The \u201cBasic Kanban\u201d template ensures that you have almost all necessary columns to work using the Scrum framework.  - Link the repositories that are associated with your project. You can always add repositories to your project later on.    - After creating the project board,ensure that you  **add the \u201cBacklog\u201d column**.  {{% tip %}}  Share your project board with your team members. Click on settings to add collaborators. Give these collaborators admin rights so that they can create issues, rename issues and prioritize issues as you go along.  {{% /tip %}} "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/project_management/meeting_preparation.md", "title": "How to Prepare for (and Follow-up On) Scrum Meetings", "description": "When collaborating with others, meeting preparation & planning follow-ups are crucial. Read here about the most important steps to take care of.", "keywords": "meeting, preparation, follow-up, github, issues, project management", "code": [], "headers": ["Prepare the meeting", "During the meeting", "After the meeting"], "content": " # How to Prepare for (and Follow-up on) Scrum Meetings  When using [Scrum](https://tilburgsciencehub.com/learn/scrum) and [GitHub Project Boards](https://tilburgsciencehub.com/use/projectboard) to manage work on projects, it's tempting to forget actually preparing and following-up on meetings. Sol here are our tips to make meetings as effective as possible.   - Ensure that all the issues that are completed are moved to the \u201cDone\u201d section. - If you have a follow-up question/tasks on an issue you have moved to \u201cDone\u201d, make sure to add a comment to that issue. This can be done by clicking on the issue and on the button \u201cGo to issue for full details\u201d that appears in the right column of your screen. - Close any issues that have no further follow-up questions. Do not archive the issue, yet! - Make sure that you know the status of all the issues you have moved to the \u201cIn Progress\u201d section.     - The issues that are set to \u201cDone\u201d should be discussed first.   - Then discuss the issues set to \u201cIn Progress\u201d and \u201cTo Do\u201d. Make a quick note of any relevant tasks that are discussed during the meeting in the \u201cBacklog\u201d section. You will revise these quick notes after the meeting.   - Together with the team members, discuss which issues from the \"Backlog\" should be worked on during the next sprint.     - Move all the issues that are set to \u201cDone\u201d and require further discussion to be done. Make sure you add notes that specify the follow-up tasks you need to complete.   - Archive all the issues that are set to \u201cDone\u201d and do not require further attention.   - Rewrite all the quick notes you have taken during the meetings, transform them into issues, and link the associated repository to the issue. Be thorough in renaming the issues so that you understand what needs to be done. If necessary, break the note down into smaller issues. You could also categorize all your issues with labels to better manage the project.   - Move the issues you need to work on during the upcoming sprint into the \u201cTo Do\u201d section (either the follow-up actions, or new items from the \"Backlog\").   - Prioritize the issues by dragging them up or down the list.  You're now ready to start working on your next sprint! "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/project_management/write-good-issues.md", "title": "Write GitHub Issues That Get the Job Done", "description": "Use GitHub Issues to formulate what you or your team members should work on. Here, you'll learn how to write issues that really get the job done.", "keywords": "issues, project, git, github, issue", "code": [], "headers": ["Overview", "Why Issues?", "Getting Started with Issues", "Best Practices for Writing GitHub Issues", "Use Descriptive Titles!", "Set Goals with Clear Descriptions", "Comments Document Your Progress", "Provide Deliverables", "Closing Issues", "Setting Priorities"], "content": "   [Github Issues](https://guides.github.com/features/issues/) can be used to clearly define tasks that either you or your team members will eventually work on. In combination with Kanban-style GitHub Project boards, they become a compelling way of coordinating teamwork. They also help you keep track of what's happening on projects and provide a durable, replicable record of the work for future reference.   1. Every issue has a __creator__ (i.e., the person who created it) and should have __one or more assignees__ (the person(s) who will execute it). The creator chooses the assignee when the issue is created. 2. An issue is a __discrete, well-defined unit of work on a project__. Usually, this means that an issue should not be more than a couple of weeks' worth of work and should not be open for more than a month or two. 3. Issues are __adaptive__. For example, an issue that started with manageable scope may grow as the project expands or new questions arise. At this point, carve off subparts into separate issues and close the original issue with an interim summary. 4. Issues are prioritized using the __Project Board on GitHub__. If you have time left, you can work on open issues that are not yet part of your current sprint.  All such rules are just a guideline. Using your time productively takes precedent over the priority ordering of tasks.   {{% warning %}} One common problem with Issues is that they are formulated too broadly. They then are likely to become open-ended and end up mixing multiple work threads.  \"Write the follow-up paper\" or \"Do the analysis\" are usually not good issues (unless the project is tiny).  {{% /warning %}}   <!-- Issues should not be opened until the assignee is ready to work on them actively (or will be soon). To-do items that we plan to work on in the future should be placed on a project outline in the repository's Github wiki. -->  <!-- If work stops on an issue for any reason and is not expected to resume soon, the issue should be closed with an interim summary. If we plan to continue work later, this can be noted on the project outline in the repository's Github wiki, along with a link to the original issue. -->    - The issue title should be descriptive enough that somebody looking back at it later will understand what the purpose of the issue was and how it fits into the larger context. - Titles should use the *imperative mood*, and not end in a period (\"Revise the main figure\") not (\"main figure.\"). - [This post](https://chris.beams.io/posts/git-commit/) by Chris Beams has an excellent discussion of what makes a good Git commit message; the same principles apply to good issue titles as well.  {{% tip %}}  __Good issue titles__  * Revise abstract * Add new data to main robustness figure * Run bootstrap for IV regressions  __Bad issue titles__  * abstract * Robustness * Incorrect inputs causing error.  Note that you can also *change* the title of an issue to make it more accurately reflect the current task.  {{% / tip %}}   - State the __goals of the issue clearly__. - Be __explicit about the deliverables__. - Like the title, it should usually be written in __imperative mode__. - The description should be __precise enough that a third party can judge__ whether the issue was completed or not. - It should include enough __explanation and context__ that someone who is not intimately familiar with the other work going on at that moment can understand it clearly -- remember that we will often be returning to these issues many months or even years later and trying to understand what was going on. - If an issue __relates directly to one or more other issues__, this should be stated in the description with a link to the other isssue(s) (e.g., \"Follow-up to #5\").  <!--The same principles regarding hyperlinks, @-references, etc. in the discussion of [comments](https://github.com/tilburgsciencehub/onboard/wiki/Issues#comments) below apply to issue descriptions as well. -->  {{% tip %}}  __Good description__  *Following #22, re-run the analysis on Research Cloud to see if that improves performance.* * *Run a minimal version of the base model on VM1* * *Test the subsampling procedure on VM1* * *Run a minimal version of the base model on VM2* * *Test the subsampling procedure on VM2*  *Document necessary code changes to implement our updated code and potential bottlenecks. In the long term, we want to migrate all of our model computations to Research Cloud.*  __Bad description__  *Redoing everything on Research Cloud, including the subsampling. Remember we want to test VM1, not only VM2.*  {{% / tip %}}   Comments in Github Issue threads are the main way we communicate about our work.  - You can add comments to the thread in a browser or by replying to a notification email about the issue. When commenting by email reply, remember to *delete the quoted text of the email thread below your actual reply*. Otherwise, this will add duplicate text to the comment thread and make it hard to read. - You (the assignee) should post comments regularly summarizing progress.   - The comment threads are your real estate, and you are free to include updates as often as you find helpful.   - Preliminary output, \"notes to self,\" etc., are acceptable.   - No issue should be left for more than two weeks without a comment updating the status, even if the comment only says: \"Have not done any work on this issue in the last week.\" - If you have a question that requires input or attention from another lab member, you should write a comment, including an '@' reference, that clarifies precisely what information is needed.   - For example, `@hannesdatta, Where would you like me to store the data files?`   - Users should keep email notifications for `@` references turned on. Anyone who is not the assignee of an issue will assume by default that comments not @-referencing them do not require their attention.  - It is up to you to judge the optimal time to request feedback from the reporter (or PIs on the project, etc.). You should usually not send results until you have made sure they are correct and make sense. When you request feedback, you should provide a clear and concise summary, making the situation clear and exactly what input you need. At the same time, you should not feel shy about requesting feedback when you are confident it will be efficient and valuable.  - If you have an important interaction about an issue outside of GitHub -- in person, over video chat, etc. -- add a comment to briefly summarize the content of that interaction and the conclusions reached.  - Issues are referenced by their Github issue number (e.g., \"#5\") when it is clear from the context what repository the issue is in, or by the name of the repository plus the issue number (e.g., \"news_trends #5\") when it is not. Any reference to a Github issue in a comment thread, email, etc., should be hyperlinked to the issue itself. Note that Github does this automatically if you type \"#\" followed by a number in a Github issue thread.  - Any reference to a file, directory, paper, or webpage should be hyperlinked to a permanent URL. [This page](https://help.github.com/articles/getting-permanent-links-to-files/) has instructions for getting permanent URLs for files in Github repositories. Links to Dropbox files and directories can be copied from the web or desktop client.     __Every issue must conclude with a reproducible deliverable.__  * It is up to you (the assignee) to judge when the objectives in the task description plus any issues that have come up in the comment stream have been resolved. As a rule, you do not need to request confirmation from the issue\u2019s creator (or project lead).  * Each task should have a final deliverable containing all relevant results. The form of the deliverable may be any combination of:   - Content at Tilburg Science Hub   - Content added to a research project (e.g., a draft of a paper, slides, analysis) in the project repository   - A PDF or markdown file   - A summary in the final comment in the issue thread  * The deliverable must be __self-contained__. It should usually begin with a concise summary of the task goal and the conclusions (e.g., answer to an empirical question), followed by supporting detail. A user should learn all relevant results from the deliverable without looking back at the comment thread or task description.  {{% tip %}} __Use this template to start writing issues__  ``` # Goal of this issue  Clearly define the goal.  # Resources  Say which resources to use (e.g., where to find relevant code, papers, etc.).  # Deliverables  Clearly define deliverables. Mention deadlines if necessary. ```  {{% /tip %}}  <!--  By default, we produce PDF/markdown deliverables inside the repository, following the same rules to produce papers, slides, etc. Code and documents specific to the issue and that we will not want to carry forward in the repository after the issue is complete can be created in the issue [branch](https://github.com/tilburgsciencehub/onboard/wiki/Workflow#branch) and then deleted before merging back to master. Such files are usually placed in a separate subdirectory called `/issue/` at the top level of the repository. [Permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) to deliverables in the `/issue/` subdirectory will continue to work even after the directory is deleted.   The deliverable must contain enough information that another user could replicate its results. For figures, tables, or other results produced by code, a user should identify the relevant code and reproduce the output. This will usually be automatic when the output is produced inside the repository. For output produced by hand (e.g., literature reviews, manual calculations), the deliverable should include enough information about the steps performed that a user could have a decent shot at repeating them.  -->    * When an issue is complete, you should post a final summary comment and then close it. * All closed issues must have one and only one summary comment.  * If changes made after the issue is closed (e.g., during peer review) require changes, you should edit the summary comment in place rather than creating a new one. * At this point, you will also normally open a [pull request](https://github.com/tilburgsciencehub/onboard/wiki/Workflow#pull-request) to peer review the issue and merge the issue branch back to master.  {{% tip %}} __Close an Issue with a final comment__  * Your final comment should begin with \"Summary\" on the first line (usually bold or title font). * It must also include a brief (usually a couple of paragraphs) recap of what was accomplished in the issue. * It must include a [revision-stable](https://help.github.com/en/github/managing-files-in-a-repository/getting-permanent-links-to-files) pointer to the deliverable -- usually a link along with additional information if needed (e.g., relevant page/table/figure numbers in the draft), or reference to a branch on GitHub.  {{% /tip %}}   - Priorities to work on issues are coordinated using the Project Boards on GitHub. - If you have time left in your sprint, you can work on other open issues. - In prioritizing your work, note that peer review takes priority over all open issues; open issues created earlier should take precedence over tasks created later. - In some cases, we may give explicit instructions that override these defaults (e.g., we tag an issue with \"good issue to start\", or \"critical\"). If you are ever unsure about prioritization, you should ask. "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/use-github/remove-files.md", "title": "Remove Sensitive or Large Files From Your Repository", "description": "Accidentally committed large data sets or sensitive information to your repository? Here is how to entirely remove files from your repository's history!", "keywords": "git, github, versioning, remove, sensitive, password", "code": [], "headers": ["Simply undo your last commit", "Entirely remove files"], "content": " Git will eventually become the long-term memory of your project, and you may decide to make the repository public so others can learn from or use your work.  Normally, versioning really is a good thing, unless... - you accidentally stored sensitive information in your source code that you really do not want to be out in the public (e.g., API credentials, passwords), - you accidentally stored files (e.g., large data sets, images) in your repository, that you cannot upload to GitHub (and hence can't synchronize your repository anymore)  Luckily, there are ways out!   If you have *just* committed the file that you shouldn't have committed, simply roll back to the last version of your repository.  `git reset --soft HEAD~1`   If you have committed sensitive data to your Git repository, use thef following resources to remove the file and all its previously issued commits.  1. Open Git bash, and type `git filter-branch --index-filter 'git rm --cached --ignore-unmatch file_to_remove' --prune-empty -- --all`. Replace `file_to_remove` by the path and file name of the file that you want to wipe out. Thanks to [StackOverflow](https://stackoverflow.com/questions/10676128/remove-deleted-files-from-git-history) for this solution!  2. Check out the GitHub manuals for a [step-by-step guide](https://docs.github.com/en/github/authenticating-to-github/removing-sensitive-data-from-a-repository)!.  Alternatively, use [BFG Repo Cleaner](https://rtyley.github.io/bfg-repo-cleaner/), a convenient tool to remove unwanted files from your project.  {{% tip %}} __Prevent committing specific files in the first place__  Had to deal with removing sensitive information or unwanted files, and want to avoid making the same mistake twice?  Learn how to [exclude files from versioning](../git-ignore).  {{% /tip %}} "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/use-github/most-important-git-commands.md", "title": "The Most Important Git Commands You Should Know", "description": "A quick recap of the essential Git commands you will be using everyday.", "keywords": "git, commands, important, essential, cheat", "code": [" ```bash git clone <URL> ``` ", " ```bash git status ``` ", " ```bash git add <file_name> ``` ", " ```bash git commit -m \"<your_message>\" ``` ", " ```bash git push origin <branch_name> ``` ", " ```bash git log ``` ", " ```bash git add . ``` ", " ```bash git reset --soft HEAD~1 ``` ", " ```bash git reset --hard <commit id> ``` "], "headers": ["Overview", "Code", "Advanced use cases", "Add all files to the staging area", "Ignore files from versioning", "Undo previous commits", "See also"], "content": "  This is a summary of the most important Git commands that you can use in Git Bash. If you're not so familiar with working in the command prompt/terminal, you could also try to check out Git Desktop or Git GUI, which provides a graphical user interface for performing the Git workflow.   Clone (\"download\") the repository to your computer.    {{% warning %}} Do not clone a repository into another repository! {{% /warning %}}  ---  Check which files/directories have changed since your last commit.    ---  Add specified file(s) to the staging area, so that any changes can eventually be committed.    ---  Commit staged changes to the version history of your repository. It's good practice to use a clear & concise commit message  (\"note to your future self and others\") which shows up prominently on your repository at GitHub.com.    ---  Push any changes you have done to the repository on your computer to the specified branch of the remote repository (e.g., at GitHub.com).    ---  View your commit history (i.e., a list of commit IDs and your commit messages.       To add all files that have been changed to the staging area (to eventually commit them), use    That way, you don't have to mention files individually. But beware to not accidentally version files that must not be versioned (e.g., datasets).   You can create a `.gitignore` file (e.g., in the root directory of your repository) to tell Git to stop paying attention to files you don't want to version (e.g., datasets, operation-specific files like Mac's `.DS_Store` or R's `.RHistory`.  For example, if you save the snippet below in a file called `.gitignore`, any content in the `my_passwords` folder and any `.csv` files will be ignored - even when using `git add .`!    ```   my_passwords/*   *.csv   ```  ---   It happens to all of us - we sometimes commit stuff that we didn't mean to commit. That can be quite problematic, for example if you've accidentally committed a password. But even for less drastic cases, reverting \"wrong\" commits is good practice because it keeps your repository clean.  The snippet below undoes the last commit.    Alternatively, you can view the commit history with `git log`, and revert to *any* commit by referring to its unique id (which looks a bit like this: `0hf1u7x2`).      * [Official GitHub Cheat Sheet](https://education.github.com/git-cheat-sheet-education.pdf) * [Oh Shit, Git!?! A collection of advanced and useful Git commands!](https://ohshitgit.com) * [Version control on The Turing Way](https://the-turing-way.netlify.app/reproducible-research/vcs.html) * [Version control at Software Carpentry](http://swcarpentry.github.io/git-novice/) * [Installation guide for Git and GitHub](/building-blocks/configure-your-computer/statistics-and-computation/git/) "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/use-github/git-ignore.md", "title": "Exclude Files From Versioning", "description": "Not all files should be tracked (e.g., datasets, or sensitive information). Learn how to exclude them from versioning!", "keywords": "git, github, versioning, gitignore, remove, sensitive", "code": [" ```txt **/rbin/ **/raw/ *RData *pdf **/audit **/input **/output **/temp **/zip *csv *xlsx *~* *log *.Rhistory **/exports **.ipynb_checkpoints **__pycache__ *.log slides/*.gz slides/*.snm slides/*.toc slides/*.nav slides/*.out slides/*.aux .RProfile ``` "], "headers": ["Let Git/GitHub know which files to exclude from versioning", "Example"], "content": "  By default, Git/GitHub track *any* files that you have created, for example: - large data files (that you wont be able to upload to GitHub), - files that are generated by code (and hence need not to be versioned), and - even sensitive passwords that you may have stored in your code accidentally.  Luckily, Git offers a convenient way to __exclude files and directories from versioning__.  1. Create a new file in your project's root directory, and call it `.gitignore` (remember to use the `.`!)  2. Edit the `.gitignore` file, and define which files or directories to exclude. For example, `**/DIRNAME` excludes any directory called `DIRNAME`, and `*pdf` excludes any PDF files.  3. Save the `.gitignore` file, and run `git status` in your repository - the excluded files and directories won't show up anymore!   Check out a few example `.gitignore` files for [inspiration](https://github.com/rgreminger/example-make-workflow/blob/master/.gitignore), or copy-paste the following to your own `.gitignore`:   "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/use-github/make-code-citable-with-doi.md", "title": "Make Your Code Citable with DOI", "description": "Learn how to obtain a DOI with Zenodo to make your GitHub repository citable in academic literature.", "keywords": "zenodo, doi, cite, github, literature", "code": [], "headers": [], "content": " DOI (Digital Object Identifiers) are a very important and widely used tool to uniquely identify objects in academic literature and beyond - for instance, journal articles, research reports, but also code and data sets.  If you are a researcher who writes code and deals with primary data, you may want to archive your code and make it citable by assigning a DOI to your GitHub repository.  Follow [this guide](https://guides.github.com/activities/citable-code/) and learn how to do it using GitHub and [Zenodo](https://zenodo.org/about). "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/use-github/versioning-using-git.md", "title": "Get Started with Using Git and GitHub", "description": "Git is an open-source version control system that allows you to keep track of your source files and the changes you make to them.", "keywords": "git, github, interactive tutorial, versioning, pull, push, fork, share, commit, add", "code": [" ```bash # set the author name for your commits git config --global user.name \"[name]\"  # set the author email for your commits git config --global user.email \"[email address]\" ``` ", " ```bash # initialize a new repository git init ```  ", " ```bash # clone an existing repository from Github to local PC git clone \"[url]\"  # clone an existing repository, and specify name of target directory on local PC git clone \"url\" my-project # will clone the repository to the folder `my-project`.  ``` ", " ```bash # show files or directories that were changed (or that still need to be added) # files that are already in the staging area are green; files that are not # in the staging area are red. git status  # add a directory or file to the staging area git add directory-name-or-file-name # for example, git add sourcecode.R, or dir-name/sourcecode.R.  # run git status again, to see that the files have \"turned green\" git status  # added a wrong file to the staging area, and want it to turn \"red\" again? git reset directory-name-or-file-name  # are you happy with what is green in your staging area? Then it's # time to finalize your save operation by initiating a so-called \"commit\". git commit -m \"message\" # (give it a clear message, so that you can easily roll back to this version # of your repository.  # optionally, use git log to see the versioning history git log ``` ", " ```bash # (1) DOWNLOAD AND MERGE CHANGES  # download and merge any changes from the remote/GitHub git pull  # alternatively, you can do the same in two steps: git fetch # fetches changes git merge # merges changes with your local files  # (2) UPLOAD CHANGES  # upload all local branch commits to GitHub git push ```  ", " ```bash # create a branch [branch-name] git branch [branch-name]  # switch to s specific branch [branch-name] git checkout [branch-name]  # merge a specific branch [branch] to the working branch; #, e.g., if you're in the master branch, you can integrate # any changes done in the feature [branch]. git merge [branch]  # delete a specific branch [branch-name] git branch -d [branch-name] ``` "], "headers": ["Overview", "What is Git?", "What is GitHub?", "Let's Use Git!", "Configuring Git on a new computer", "Create a new repository or join an existing one", "Create a new repository for a new project", "Create a new repository for an existing project", "Clone an existing repository to your local computer", "Work on your project in \"workcycles\"", "Track your changes", "Synchronize your changes with a server", "Advanced Use Cases", "Branching", "See also"], "content": "   **Git** is an [open-source version control system](/building-blocks/configure-your-computer/statistics-and-computation/git/), which allows you to keep track of your source files, and the changes you make to them. Using Git, you can roll back to any previous version of a file, and easily collaborate with team members.  Git runs on your computer, and allows you to configure so-called repositories that track files and directories.   You can optionally use Git in combination with an online hosting service such as [**GitHub**](https://www.github.com), which allows you to backup your code, synchronize your work across multiple computers, or collaborate with others on joint projects. There are several alternatives to GitHub available, and maybe your institution even offers its own \"Git\" services.  On top of the basic features of Git (like versioning), GitHub offers a range of tools that allow you to collaborate with each other more efficiently. A few examples: - *Issues* are sort of to-do lists, flavored with a discussion board - *Projects* are a sort of [*Scrum board*](https://www.visual-paradigm.com/scrum/how-to-use-scrum-board-for-agile-development/).   {{% tip %}}  **For starters...**  If you've never worked with Git or GitHub before, we recommend you following [this nteractive onboarding tutorial](https://lab.github.com/githubtraining/introduction-to-github).  **For more advanced learners..**  There is no better summary of using Git and GitHub than [this official cheatsheet](https://education.github.com/git-cheat-sheet-education.pdf). Download it, print it out or put it on your desktop. Consider it as your best friend for a while.  {{% /tip %}}   To use Git locally, tell Git your name and email address, so that any work you do (and potentially sync later) can be linked to your name.  Open `Git bash`, and type the following commands:     Git organizes your code in so-called repositories. You can either create a repository for new or existing projects, or join existing repositories. Keep on reading to find out how.  {{% tip %}} __Avoid complicated directory names, or Dropbox/Drive__  - Ideally, you use a main project folder on your computer, pretty high-up in your folder hierarchy (e.g., 'D:/projects/`). - Use simple directory names, so you can navigate quickly to it. - While technically feasible, you should avoid storing your Git repositories on Dropbox as this may lead to synchronization conflicts.  {{% /tip %}}   If you would like to start a new project, it's easiest to start your repository from one of our [example projects](/examples). Just navigate to a project's GitHub page, and select *Use this template* (a green button). Choose a name for your new repository, and proceed with the standard options.  Note down the URL for your repository (e.g., `https://github.com/tilburgsciencehub/test-repository`).   Chance is you are already working on a project, and you'd like to adopt Git to start versioning your files from now onwards.  Just navigate to your main project directory, open `Git bash`, and type      If you already have your own GitHub repository, or have become a member of the GitHub repository of a team member, you can just \"clone\" these repositories to your local computer.    {{% tip %}} __Start with example code__  Cloning is a fantastic way to use the work of others to start your own. For example, you can type `git clone \"https://github.com/rgreminger/example-make-workflow\"` to clone a copy of the example workflow using R and make (see also the [example projects](/examples)).  You can also take a sneak preview of one of the thousands of projects hosted on GitHub? Just note down the URL of the repository (e.g., `https://github.com/[username]/[project-name]`), and clone the repository to your local computer. {{% /tip %}}   When working on your project, you do the following two things: - track changes to your project (e.g., such as adding, removing, or changing source files), - synchronize your repository with GitHub, so that (a) you make a backup of your changes, and (b) you allow other team members to see your changes, and (c) you see changes that team members (may) have done.  ![Git workflow.](../git_workflow.png)   - Now it's time to start working on your project. \"Working\" means making changes to files or directories (such as adding, changing, or deleting files or directories).  - Typically, you execute this workflow multiple times a day when working on a project. Every time you execute this workflow, you \"save\" a snapshot of your project that you can roll back to later.  - Git separates a \"save\" operation in two stages: first, files (or directories) can be gathered on a so-called \"staging area\" using the command `git add`. You can use multiple of these commands after each other. Then, in a second step, the changes are saved - or, in Git terminology - \"committed\". The command for this is `git commit`. See below for an example, which also adds a few other useful commands.    {{% tip %}} **Exclude or remove files from tracking**  Want to avoid tracking specific files and folders from versioning? For example, there is no point in tracking *generated files* in `/gen`, as these files are purely created based on source code in `/src` (which, in turn, you *do* would like to track). Also, large data sets shouldn't be versioning, given the upload limit at GitHub. Learn how to exclude files and directories from tracking [here](../git-ignore).  Similarly, you should not store any sensitive information (such as passwords or API keys) in your code. Similarly, you may have accidently committed a large data set and would like to take it out from the repository entirely? [Here's how!](../remove-files.md).  {{% /tip %}}  Everyone can sync their local changes with the remote repository on GitHub. You can also \"download\" changes to your local repository from the remote repository.      Branches separate the main version of your project (the \"main\" branch), from any experimental code in which you develop new features.  One team member is in charge of the \"main\" branch, while everybody else implements new features in feature branches (give them any name you like, but names should be easily understood by anybody in your team).    After integrating changes from a particular branch, you ideally synchronize changes with the remote repository on GitHub.   Want to know more about how to use Git? Check out this material: - [Lessons at Software Carpentry](https://software-carpentry.org/lessons/) - [GitHub Cheatsheet](https://education.github.com/git-cheat-sheet-education.pdf) - [Tutorial on using Git at the University of Z\u00fcrich](https://github.com/pp4rs/2020-uzh-course-material/blob/master/11-version-control/slides/git-local.pdf).  <!-- ![Git workflows.](../git.png)  1. We distinguish between **local** and **remote** repositories.     - Each project can consist of multiple local repositories, which are stored on one or many computers (e.g., your desktop PC, your laptop, a computer in a cloud, or computers by team members).     - Each project typically has one remote repository (e.g., hosted on GitHub), which is used to backup and synchronize changes between multiple computers.  2. Each project has a \"working tree\" (your project's main directory) - by default, all files in that working directory can be tracked.  3. Workflow to version your files     - Run `git status` to see which files are staged (green), and which ones are not (red)     - Add files to the 'staging area', using the command `git add`     - Run `git status` to verify you have tracked everything you want.     - See files and/or directories that you never want to track? Add those to a `.gitignore` file (typically data files, or generated output files)     - Finalize your \"save\" by running `git commit -m \"give yourself a clear message \"`, which will commit any changes to your project's history. 4. Workflow to synchronize changes with a remote repository     - Run `git pull`; alternatively, first run `git fetch` and then `git merge`     - Push (`git push`) your own local changes to the repository (so that others see your changes) 5. Other useful commands     - Use `git checkout` to switch branches     - Use `git clone` to clone repositories from GitHub "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/use-github/pull-requests.md", "title": "Contribute to Open Source Projects", "description": "Learn how to contribute to open source projects published on GitHub.", "keywords": "github, open source, pull request, collaborate", "code": [], "headers": ["Step 1: Get to know the repository", "Step 2: Run the repository\u2019s code", "Step 3: Make changes", "Step 4: Ask Project Owner to Integrate your Changes via a Pull Request (PR)"], "content": " # Overview  Ever wondered how to contribute to open source projects on GitHub? Here's how!   Familiarize yourself with the repository to which you want to contribute.  - Typically, each repository has a readme with general instructions on what the repository is about (& how to run the code). - Also, new features and bugs are discussed at the repository\u2019s issue page. - Finally, many repositories contain a discussion forum and project board in which you can learn about the roadmap of the project.  {{% example %}} For example, visit the [repository of Tilburg Science Hub](https://github.com/tilburgsciencehub/tsh-website).  - Browse through the repository\u2019s readme (that\u2019s what you see when you click on the links above) - Head over to the issue page - View the project/discussion boards.  Can you identify ways in which to contribute to the project?  {{% /example %}}   After installing required software, you need to run the code to see whether you can actually test your changes to the project later.  - Open the repository on GitHub, and fork it (click on the fork button in the upper right corner on Git). This creates a copy of the code in your GitHub account.  - Clone your own fork to the disk, e.g., git clone https://github.com/your-user-name/tsh-website.  - Enter the directory of the cloned repository, and run the code.  {{% example %}} - In the case of Tilburg Science Hub, you can type `hugo server` to start up the webserver (Hugo is a content management system we use for running the website).  - You can now open the website locally in your browser. Check the terminal for the exact address, but likely you just have to enter https://127.0.0.1:1313 in your browser!  - Check out the code in \\docs - the websites are written in Markdown, and you can easily add/change. Observe how the site actually changes in your browser!  {{% /example %}}    Find stuff that you want to add to the project, or fix! Each *new feature* that you introduce usually needs to be developed in a separate branch, which allows the repository owner to carefully screen which parts of the project to add to the public version of the main project.  1. Create a new branch (`git branch name-of-a-new-feature`) 2. Work on your new feature. Throughout, apply the Git workflow (`git status`, `git add`, `git commit -m \"commit message\"`) 3. When you\u2019re done with all of your changes, push your changes to your GitHub repository `git push -u origin name-of-a-new-feature`  At this stage, your changes are visible at *your forked copy* of the repositoroy, but the repository owner of the main project doesn't know about these changes yet.   Fully done and happy with your changes? Then let the project owner know about your new, amazing feature!  1. Open your forked repository on GitHub, select the branch you've just edited, and click on \"pull request\".  2. Carefully describe your changes (e.g., the new feature you developed, why it's useful, how you've tested it, etc.), and then submit your pull request.  The owner of the main repository will now review your changes, maybe request changes, and potentially integrates your new feature with the main project. Congrats on your first open source contribution! "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/use-github/git-lfs.md", "title": "Working with Large Files on GitHub", "description": "GitHub limits the size of files allowed in repositories. Use Git-LFS, an open-source Git extension for storing large files.", "keywords": "github, git-lfs, large files, versioning, organizing", "code": [], "headers": ["Overview", "Install Git LFS <!-- Provide your code in all the relevant languages and/or operating systems. -->", "Usage", "Advanced use cases", "Store really large files"], "content": " <!-- This is a template. Please replace the content while keeping this structure. Make sure to read our contribution guide to learn how to submit your content to Tilburg Science Hub. --> <!-- Goal of the Building Block -->   GitHub limits the size of files allowed in repositories. It warns you if you're trying to push a 50MB file, and completely stops you if the push exceeds 100MB.  However, even if it didn't stop you, versioning large files would be very impractical. That's because **a repository contains every version of every file** \u2013 that's the point of versioning, right? Having multiple versions of large files cloned locally can become expensive in terms of disk space and fetch time.  The solution?  Well, first, you should ask yourself whether to store large files in the first place. Sometimes, large files are generated on the basis of existing data and code, and hence can always be \"reconstructed\" using existing files.  However, sometimes you wish to store raw data with moderate file sizes (let's say, between 5 and 50MB). For this, use **[Git LFS](https://git-lfs.github.com)**, an open-source Git extension for \"large file storage\". In short, Git LFS allows you to version large files while saving disk space and cloning time, using the same Git workflow that you're used to. It does **not** keep all your project's data locally. It only provides the version you actually **need** in your checked out revision.  When you mark a file as LFS file, the extension replaces the actual large file with a small *pointer* on your PC. The actual files - and all their versions - are located on the LFS remote server and *only the pulled* files are stored in a local cache. In other words, when you `pull` to your local repository, the pointer is replaced with the file and only the actual version you've requested gets stored locally.  Check out this video for a brief explanation on how Git LFS works.  {{< youtube 9gaTargV5BY iframe-video-margins >}}   Make sure that [Git is already installed](/building-blocks/configure-your-computer/statistics-and-computation/git/).  - Go to [git-lfs.github.com](https://git-lfs.github.com) and download directly. Or, if you use [`Brew`](/building-blocks/configure-your-computer/automation-and-workflows/commandline/#mac-users):  ``` bash brew install git-lfs ```  If you used Brew, go to the next step. If you downloaded directly from the website, open the terminal and change the current working directory to the downloaded and unzipped folder of Git LFS. Then, install:  ``` bash ./install.sh ```  - Once installed, set up LFS for your account:  ``` bash git lfs install ```  If it was successful, you should see ```Git LFS initialized```.   Git LFS doesn't do anything autonomously for you. You need to **explicitly tell it which files to track**.  ``` bash git lfs track \"largefile.png\" ```  Or, to track a file type (if you don't want to manually specify every single file you wish to track):  ``` bash git lfs track \"*.png\" ```  {{% warning %}} **Be careful!** Do not forget the quotes around your file name. {{% /warning %}}  Now you can resume your usual Git workflow. You just need to make sure to track the `.gitattributes` file too.  ``` bash git add .gitattributes ```  Simply add your file(s), commit and push as you'd normally do!  ``` bash git add largefile.png git commit -m \"Add large file\" git push origin master ```   You may be tempted to store your large files on a different server than the default LFS one.  - GitHub provides a [Git LFS server](https://github.com/git-lfs/lfs-test-server) that implements the Git LFS API which you can set up so that your binary files can be uploaded to a server that you administer. However, as of today, this is not in a \"production ready state\" and it is suggestedd to be used for testing only.  - In case you'd like to go serverless and back up these files on external services like Amazon S3, you can use one of the [Git LFS implementations](https://github.com/git-lfs/git-lfs/wiki/Implementations), like [this one](https://github.com/langep/git-lfs-serverless) and [this other one](https://github.com/meltingice/git-lfs-s3) for AWS S3. However, bear in mind that these are some external open-source implementations that have not been verified by GitHub.  You can follow [this tutorial by Atlassian](https://www.atlassian.com/git/tutorials/git-lfs) for more advanced use cases, like moving a LFS repository between hosts or deleting local LFS files.  <!-- For example: \u2018devising and organizing the project\u2019, \u2018data collection\u2019, \u2018data analysis\u2019 and \u2018article writing\u2019. -->   If you're hitting the limits of Git LFS, or only want to store *one* version of a file, object storage (e.g., such as the one on AWS S3) may be a better way to handling large files. "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/write-your-paper/slides-lyx.md", "title": "Create Slides in No Time with LyX", "description": "Learn how to quickly and efficiently create slides for your presentation using LyX.", "keywords": "slides, presentation, lyx, tex, template", "code": [], "headers": ["Overview", "Meet LyX", "Download our Presentation Template"], "content": "  [{{< katex >}}\\LaTeX{{< /katex >}}](https://www.latex-project.org) is a professional typesetting system widely used in academia. It produces great-looking journal articles, technical reports, books, and even slide presentations.  One of its main features is that **it separates the writing and the formatting stages**. The writer can focus solely on writing the content in plain text, and then use some markup tags and commands to stylize the text and define, for instance, how the slides will look like.  This is mainly what sets $\\LaTeX$ apart from WYSIWYG (an acronym for \"What You See Is What You Get\") editors, like Microsoft Word or PowerPoint. Experienced users can **produce professional-looking documents very quickly**, but **at the expense of losing a Graphical User Interface**.   **[LyX](https://www.lyx.org)** is a document preparation software that bridges the two worlds of \"What You See Is What You Mean\" and \"What You See Is What You Get\". It combines the power and flexibility of $\\LaTeX$ with a Graphical User Interface and the gentle learning curve of editors like Microsoft Word and PowerPoint.  LyX is cross-platform and free of charge. You can download it from the **[LyX Download page](https://www.lyx.org/Download)**.  {{% tip %}}  [Read more](/get/latex) about all the installing options for $\\LaTeX$ and LyX. In case you need further guidance, you can access [here](https://wiki.lyx.org/LyX/Tutorials) the full list of official LyX tutorials.  {{% /tip %}}   We provide a LyX template to get you started working on your slides very quickly.  {{% cta-primary \"Download the LyX Template\" \"../lyx-slides-template.zip\" %}}  ![Slides presentation in LaTeX](../img/slides-lyx-template.png) "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/write-your-paper/export-tables.md", "title": "Export Your Tables for Print-ready Publications", "description": "Learn how to quickly and efficiently export your tables for your paper.", "keywords": "stargazer, latex, paper, lyx, r", "code": [" ```R library(stargazer)  stargazer(mdl_1, mdl_2,           title = \"Figure 1\",           column.labels = c(\"Model 1\", \"Model 2\"),           type=\"html\",           out=\"output.html\"             ) ``` "], "headers": ["Overview", "An Example"], "content": "  **[Stargazer](https://www.rdocumentation.org/packages/stargazer/versions/5.2.2/topics/stargazer)** is an easy-to-use R package that creates nicely-formatted, high-quality tables from your R data in ASCII, {{< katex >}}\\LaTeX{{< /katex >}}, and HTML code.  It automatically recognizes the kind of data you provide. If you provide a set of regression model objects, it will produce a regression table. Instead, if you feed it with a data frame, it will produce a summary statistics table.   Convert regression coefficients of `mdl_1` and `mdl_2` into a HTML file that can be copied into a paper.   "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/write-your-paper/latex-templates.md", "title": "Get Started With Our LaTeX Templates", "description": "Get started with your new LaTeX documents using our templates for thesis projects, academic papers, and more.", "keywords": "template, latex, download, start, document", "code": [], "headers": ["Overview", "The Basics", "Download our Thesis Template"], "content": "  [{{< katex >}}\\LaTeX{{< /katex >}}](https://www.latex-project.org) is a professional typesetting system widely used in academia because it can produce complex documents - like journal articles, books, technical reports, and similar - with unparalleled output quality.  However, setting up new documents can be time consuming and a bit daunting, especially at the beginning. If you want to get started writing your $\\LaTeX$ documents quicker, you can use one of our custom-made templates below.   If you are new to $\\LaTeX$, before attempting to customize any of our templates, you should first learn about its basics and how it works. We've made **[a tutorial that can help you get started with $\\LaTeX$](/learn/latex)** and write your very first document within minutes.  In case you haven't installed $\\LaTeX$ yet, you can **[follow our installation guide here](/get/latex)**.   We provide a $\\LaTeX$ template to get you started working on your thesis project very quickly.  {{% cta-primary-center \"Download Thesis Template\" \"../latex-thesis-template.zip\" %}}  You can also download a PDF preview to see what the output file would look like.  {{% cta-secondary-center \"Download PDF Preview\" \"../latex-thesis-template.pdf\" %}}  ![Thesis Template for LaTeX](../img/latex-thesis-template.png) "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/write-your-paper/amsmath-latex-cheatsheet.md", "title": "Cheatsheet for LaTeX Math Commands", "description": "Explore all the most important AMS-Math LaTeX commands and download your cheatsheet.", "keywords": "ams, math, latex, tex, cheatsheet", "code": [], "headers": ["Overview", "Commands", "Packages", "Typeset", "Greek letters", "Lowercase", "Uppercase", "Mathematical font", "Superscript and subscript", "Root", "Dots", "Spaces", "Braces", "Accents", "Operators", "Modulo", "Fractions", "Symbol stacking", "Big operators", "Delimiter size", "Absolute value and norm", "Arrows", "Binary relations", "Binary operators", "Logic symbols", "Other symbols", "Multi line equation", "Vectors", "Arrays", "Cases", "Matrices", "Blackboard bold"], "content": "  This is a quick overview of the main {{< katex >}}\\LaTeX{{< /katex >}} commands to render mathematical expressions.  We also provide a compiled PDF version of this cheatsheet.  {{% cta-primary \"Download the PDF Cheatsheet\" \"https://github.com/manuelemacchia/math-latex/raw/master/amsmath.pdf\" %}}   The main package to load is `amsmath`. More symbols are included in `amssymb`.  {{% tip %}} You can load packages in the preamble: `\\usepackage{amsmath}` {{% /tip %}}  - For text style (inline) math, use: `$...$`. This is inline: $E=mc^2$ - For display style math, which breaks the paragraph: `\\begin{equation} ... \\end{equation}` (numbered equation) or `\\[ ... \\]` (non-numbered). This is a display equation: $$E=mc^2$$   |  |  |  |  |  |  |   |-------------|-------------|------------|------------|---------------|---------------| | $\\alpha$    | `\\alpha`    | $\\beta$    | `\\beta`    | $\\gamma$      | `\\gamma`      | | $\\delta$    | `\\delta`    | $\\epsilon$ | `\\epsilon` | $\\varepsilon$ | `\\varepsilon` | | $\\zeta$     | `\\zeta`     | $\\eta$     | `\\eta`     | $\\theta$       | `\\theta`      | | $\\vartheta$ | `\\vartheta` | $\\iota$    | `\\iota`    | $\\kappa$      | `\\kappa`      | | $\\lambda$   | `\\lambda`   | $\\mu$      | `\\mu`      | $\\nu$         | `\\nu`         | | $\\xi$       | `\\xi`       | $\\pi$      | `\\pi`      | $\\varpi$      | `\\varpi`      | | $\\rho$      | `\\rho`      | $\\varrho$  | `\\varrho`  | $\\sigma$      | `\\sigma`      | | $\\tau$      | `\\tau`      | $\\upsilon$ | `\\upsilon` | $\\phi$        | `\\phi`        | | $\\varphi$   | `\\varphi`   | $\\chi$     | `\\chi`     | $\\psi$        | `\\psi`        | | $\\omega$    | `\\omega`    |   |  |  |  |  |  |  | |-------------|-------------|------------|------------|---------------|---------------| | $\\Gamma$    | `\\Gamma`    | $\\Delta$   | `\\Delta`   | $\\Theta$      | `\\Theta`      | | $\\Lambda$   | `\\Lambda`   | $\\Xi$      | `\\Xi`      | $\\Pi$         | `\\Pi`         | | $\\Sigma$    | `\\Sigma`    | $\\Upsilon$ | `\\Upsilon` | $\\Phi$        | `\\Phi`        | | $\\Psi$      | `\\Psi`      | $\\Omega$   | `\\Omega`   |  {{% tip %}} To ensure a consistent style throughout the document, use: ```latex \\renewcommand{\\epsilon}{\\varepsilon}   \\renewcommand{\\theta}{\\vartheta}   \\renewcommand{\\rho}{\\varrho}   \\renewcommand{\\phi}{\\varphi} ``` {{% /tip %}}  $$\\mathcal{A} \\\\, \\mathcal{B} \\\\, \\mathcal{C} \\\\, \\mathcal{D} \\\\, \\mathcal{E} \\\\, \\mathcal{F} \\\\, \\mathcal{G} \\\\, \\mathcal{H} \\\\, \\mathcal{I} \\\\, \\mathcal{J} \\\\, \\mathcal{K} \\\\, \\mathcal{L} \\\\, \\mathcal{M} \\\\, \\mathcal{N} \\\\, \\mathcal{O} \\\\, \\mathcal{P} \\\\, \\mathcal{Q} \\\\, \\mathcal{R} \\\\, \\mathcal{S} \\\\, \\mathcal{T} \\\\, \\mathcal{U} \\\\, \\mathcal{V} \\\\, \\mathcal{W} \\\\, \\mathcal{X} \\\\, \\mathcal{Y} \\\\, \\mathcal{Z}$$  Use `\\mathcal{\\text{letter}}`.  | $\\LaTeX$ | Code | $\\LaTeX$ | Code | |---------|-------|-----------|-------------| | $x^y$ | `x^y` | $x^{a+b}$ | `x^{a+b}` | | $x_y$ | `x_y` | $x_{a+b}$ | `x_{a+b}` |  | Type | $\\LaTeX$    | Code    | |-------------|-----------------|---------------| | Square root | $\\sqrt{x}$    | `\\sqrt{x}`    | | N-th root   | $\\sqrt[N]{x}$ | `\\sqrt[N]{x}` |  | Type | $\\LaTeX$    | Code    | |---------------------|------------|----------| | Multiplication dot  | $\\cdot$  | `\\cdot`  | | Three centered dots | $\\cdots$ | `\\cdots` | | Three baseline dots | $\\ldots$ | `\\ldots` | | Three diagonal dots | $\\ddots$ | `\\ddots` | | Three vertical dots | $\\vdots$ | `\\vdots` |  | Type | Code | |----------------|----------| | Negative space | `\\!`     | | Thinnest space | `\\,`     | | Thin space     | `\\:`     | | Medium space   | `\\;`     | | 1em space      | `\\quad`  | | 2em space      | `\\qquad` |  | $\\LaTeX$ | Code | |----------------|----------| | $\\overbrace{ ... }^{ \\text{text over brace} }$ | `\\overbrace{ ... }^{ \\text{text over brace} }` | | $\\underbrace{ ... }_{ \\text{text under brace} }$ | `\\underbrace{ ... }_{ \\text{text under brace} }` |  | $\\LaTeX$ | Code | $\\LaTeX$ | Code | $\\LaTeX$ | Code | |---------------|-------------|--------------|------------|-------------------|-----------------| | $\\hat{a}$   | `\\hat{a}`   | $\\bar{a}$  | `\\bar{a}`  | $\\mathring{a}$  | `\\mathring{a}`  | | $\\check{a}$ | `\\check{a}` | $\\dot{a}$  | `\\dot{a}`  | $\\vec{a}$       | `\\vec{a}`       | | $\\tilde{a}$  | `\\tilde{a}` | $\\ddot{a}$ | `\\ddot{a}` | $\\widehat{AAA}$ | `\\widehat{AAA}` |   `\\sin    \\cos    \\arcsin    \\arccos    \\sinh`     `\\cosh    \\tan    \\arctan    \\log    \\ln`         `\\max    \\min    \\sup    \\inf    \\tanh`     `\\cot    \\sec    \\csc    \\det`      {{% tip %}} To define a custom operator: `\\DeclareMathOperator{\\argmax}{argmax}`. {{% /tip %}}  | $\\LaTeX$ | Code | |-----------------------|-----------------------| | $a \\bmod b$           | `a \\bmod b`           | | $a \\equiv b \\pmod{m}$ | `a \\equiv b \\pmod{m}` |   `\\frac{ ... }{ ... }`  For instance, 3/4 can be displayed as $\\frac{3}{4}$.   | $\\LaTeX$ | Code | |-----------------------|-----------------------| | $\\overset{ A }{ B }$ | `\\overset{ ... }{ ... }` | | $\\underset{ A }{ B }$ | `\\underset{ ... }{ ... }` |  First argument is the main symbol, second argument is the symbol to put over or under the main symbol.  | $\\LaTeX$ | Code | $\\LaTeX$ | Code | |---------------------------------|-------------------|--------------------------------|------------------| | $\\displaystyle \\int_{a}^{b}$    | `\\int_{a}^{b}`    | $\\displaystyle \\sum_{k=0}^{n}$ | `\\sum_{k=0}^{n}` | | $\\displaystyle \\prod_{k=0}^{n}$ | `\\prod_{k=0}^{n}` | $\\displaystyle \\lim_{x \\to 0}$  | `\\lim_{x \\to 0}` |  {{% tip %}} For multiple integrals, use: $\\iint$ `\\iint` $\\\\,\\\\, \\iiint$ `\\iiint` etc.  For a closed path integral, use: $\\oint$ `\\oint` {{% /tip %}}  Change the delimiter size by adding one of these modifiers immediately before the delimiter itself: `\\big  \\Big  \\bigg  \\Bigg`.  Let $\\LaTeX$ determine the correct size using `\\left` and `\\right` immediately before the opening and closing delimiters, respectively.  | $\\LaTeX$ | Code | |-------------------|-------------------| | $\\lvert x \\rvert$ | `\\lvert x \\rvert` | | $\\lVert x \\rVert$ | `\\lVert x \\rVert` |  The same can be achieved by defining: ```latex \\usepackage{mathtools} \\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert} \\DeclarePairedDelimiter{\\norm}{\\lVert}{\\rVert} ```  Use starred variants `\\abs*` and `\\norm*` to produce the correct delimiter height for any kind of equation.  | $\\LaTeX$ | Code | $\\LaTeX$ | Code | $\\LaTeX$ | Code | |----------|------|----------|------|----------|------| | $\\uparrow$ | `\\uparrow` | $\\downarrow$ | `\\downarrow` | $\\updownarrow$ | `\\updownarrow` | | $\\Uparrow$ | `\\Uparrow` | $\\Downarrow$ | `\\Downarrow` | $\\Updownarrow$ | `\\Updownarrow` | | $\\leftarrow$      | `\\leftarrow` or `\\gets` | $\\rightarrow$     | `\\rightarrow` or `\\to` | $\\leftrightarrow$ | `\\leftrightarrow`       | | $\\Leftarrow$      | `\\Leftarrow`          | $\\Rightarrow$     | `\\Rightarrow`           | $\\Leftrightarrow$ | `\\Leftrightarrow`     | | $\\mapsto$         | `\\mapsto`               | $\\longleftarrow$      | `\\longleftarrow`      | $\\longrightarrow$     | `\\longrightarrow`     | | $\\longleftrightarrow$ | `\\longleftrightarrow` | $\\Longleftarrow$      | `\\Longleftarrow`      | $\\Longrightarrow$     | `\\Longrightarrow`     | | $\\Longleftrightarrow$ | `\\Longleftrightarrow` | $\\longmapsto$         | `\\longmapsto`         |  | $\\LaTeX$ | Code | $\\LaTeX$ | Code | $\\LaTeX$ | Code | |-------------|-------------|-------------|-------------|-----------|-----------| | $\\ne$       | `\\ne`       | $\\le$       | `\\le`       | $\\ge$     | `\\ge`     | | $\\equiv$    | `\\equiv`    | $\\ll$       | `\\ll`       | $\\gg$     | `\\gg`     | | $\\doteq$    | `\\doteq`    | $\\sim$      | `\\sim`      | $\\simeq$  | `\\simeq`  | | $\\subset$   | `\\subset`   | $\\supset$   | `\\supset`   | $\\approx$ | `\\approx` | | $\\subseteq$ | `\\subseteq` | $\\supseteq$ | `\\supseteq` | $\\cong$   | `\\cong`   | | $\\in$       | `\\in`       | $\\ni$       | `\\ni`       | $\\propto$ | `\\propto` | | $\\mid$      | `\\mid`      | $\\parallel$ | `\\parallel` | $\\perp$   | `\\perp`   |  {{% tip %}} It's possible to negate these symbols by prefixing them with `\\not` (for example: $\\not\\equiv$ with `\\not\\equiv`). {{% /tip %}}  | $\\LaTeX$ | Code | $\\LaTeX$ | Code | $\\LaTeX$ | Code | |-------------|-------------|-------------|-------------|-----------|-----------| | $\\pm$     | `\\pm`     | $\\mp$     | `\\mp`     | $\\cdot$          | `\\cdot`          | | $\\div$    | `\\div`    | $\\times$   | `\\times`  | $\\setminus$      | `\\setminus`      | | $\\star$   | `\\star`   | $\\cup$    | `\\cup`    | $\\cap$           | `\\cap`           | | $\\ast$    | `\\ast`    | $\\circ$   | `\\circ`   | $\\bullet$        | `\\bullet`        | | $\\oplus$  | `\\oplus`  | $\\ominus$ | `\\ominus` | $\\odot$          | `\\odot`          | | $\\oslash$ | `\\oslash` | $\\otimes$ | `\\otimes` | $\\smallsetminus$ | `\\smallsetminus` |  | $\\LaTeX$ | Code | $\\LaTeX$ | Code | $\\LaTeX$ | Code | |-------------|-------------|-------------|-------------|-----------|-----------| | $\\lor$     | `\\lor`     | $\\land$    | `\\land`    | $\\neg$    | `\\neg`    | | $\\exists$  | `\\exists`  | $\\nexists$ | `\\nexists` | $\\forall$ | `\\forall` | | $\\implies$ | `\\implies` | $\\iff$     | `\\iff`     | $\\models$ | `\\models` |  | Symbol | $\\LaTeX$ | Code | |--------------------|---------------------|---------------------| | Infinity           | $\\infty$            | `\\infty`            | | Partial derivative | $\\partial$          | `\\partial`          | | Empty set          | $\\emptyset$         | `\\emptyset`         | | Nabla              | $\\nabla$            | `\\nabla`            | | Angle brackets     | $\\langle x \\rangle$ | `\\langle x \\rangle` |  Use the `multline` environment. ```latex \\begin{multline}     ... \\end{multline} ```  To align equations, use the `align` environment. Specify the alignment position with `&` and separate equations with `\\\\`. ```latex \\begin{align}     ... &= ...\\\\     ... &= ... \\end{align} ```  | $\\LaTeX$ | Code | |----------|------| | $\\vec{x}$ | `\\vec{x}` | | $\\bm{x}$ | `\\bm{x}` |  {{% warning %}} The `\\bm` command requires the `bm` package. {{% /warning %}}  {{% tip %}} Best practice to easily switch between types: ```latex \\usepackage{bm} \\renewcommand{\\vec}{\\bm} ``` {{% /tip %}}  Use the `array` environment. Use `\\\\` to separate rows, and `&` to separate elements of each row. To produce large delimiters around the array, use `\\left` and `\\right` followed by the desired delimiter.  {{< katex display >}} \\left( \\begin{array}{lcr}   a & b & c \\\\   d & e & f \\\\   g & h & i \\end{array} \\right) {{< /katex >}}  ```latex \\left( \\begin{array}{lcr}       a & b & c \\\\       d & e & f \\\\       g & h & i \\end{array} \\right) ```  Each letter in the argument of the array represents a column. - `l`: left aligned text - `c`: centered text - `r`: right aligned text  Use the `cases` environment. Use `\\\\` to separate different cases, and `&` for correct alignment.  {{< katex display >}} \\begin{cases}   x & \\text{if } x > 0 \\\\   0 & \\text{if } x \\le 0 \\end{cases} {{< /katex >}}  ```latex \\begin{cases}   x & \\text{if } x > 0 \\\\   0 & \\text{if } x \\le 0 \\end{cases} ```  Use one of the following environments. - `matrix`: No delimiter - `pmatrix`: $($ delimiter - `bmatrix`: $[$ delimiter - `Bmatrix`: $\\\\{$ delimiter - `vmatrix`: $\\lvert$ delimiter - `Vmatrix`: $\\lVert$ delimiter  Use `\\\\` to separate different rows, and `&` to separate elements of each row.  {{< katex display >}} \\begin{bmatrix}       1 & 2 & 3 \\\\       4 & 5 & 6 \\\\ \\end{bmatrix} {{< /katex >}}  ```latex \\begin{bmatrix}       1 & 2 & 3 \\\\       4 & 5 & 6 \\\\ \\end{bmatrix} ```  {{% tip %}} To produce a small matrix, useful for inline math, use the `smallmatrix` environment: $\\left[\\begin{smallmatrix} a & b \\\\\\\\ c & d \\end{smallmatrix}\\right]$. ```latex \\left[\\begin{smallmatrix} a & b \\\\\\\\ c & d \\end{smallmatrix}\\right] ``` {{% /tip %}}  $\\mathbb{A}$ `\\mathbb{A}` ... $\\mathbb{R}$ `\\mathbb{R}`  Include the package `bbm` for these symbols. All letters are supported.  {{% cta-secondary \"Visit this project's repository\" \"https://github.com/manuelemacchia/math-latex\" %}} "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/write-your-paper/bookdown-theses.md", "title": "Get Started With Our Bookdown Thesis Template", "description": "Get started with your thesis using Rmarkdown, bookdown and the `tisemdown` template.", "keywords": "template, rmarkdown, bookdown, download, start, document, thesis", "code": [], "headers": ["Overview", "Getting Started", "Download our Templates", "Tilburg School of Economics and Management (TiSEM)"], "content": "  [`Rmarkdown`](https://rmarkdown.rstudio.com/) and [`bookdown`](https://bookdown.org/yihui/bookdown/) are a professional typesetting system widely used in academia because it can produce complex documents - like journal articles, books, technical reports, and similar - with unparalleled output quality.   If you are new to `Rmarkdown` and `bookdown`, before attempting to get writing, you should first learn the basics behind how these packages work.  - Check out the documentation for [`Rmarkdown`](https://bookdown.org/yihui/rmarkdown/), and  - carefully review the documentation of [`bookdown`](https://bookdown.org/yihui/bookdown/) to get up and running.    The `tisemdown` thesis template provides a working template using `Rmarkdown` and `bookdown`, to get up and running with the writing of your thesis quickly. All the formatting requirements for your study programs are configured for you. You only need to modify some meta data, such as your thesis title, name and supervisor, and then get straight to writing!  To get started with the `tisemdown` thesis template, you can install it as an `R` package from GitHub by clicking [here](https://github.com/deer-marketing-lab/tisemdown) and following any further installation instructions in the README.  You can also download a PDF preview to see what the output file would look like.  {{% cta-secondary-center \"Download PDF Preview\" \"../tisemdown_thesis_template.pdf\" %}} "}, {"objectID": "./building-blocks/collaborate-and-share-your-work/write-your-paper/research-paper-lyx.md", "title": "Write a Professional Research Paper with LyX", "description": "Learn how to write your research paper using LyX.", "keywords": "paper, template, lyx, tex", "code": [], "headers": ["Overview", "Meet LyX", "Download our Research Paper Template"], "content": "  [{{< katex >}}\\LaTeX{{< /katex >}}](https://www.latex-project.org) is a professional typesetting system widely used in academia. It produces great-looking journal articles, technical reports, books, and even slide presentations.  One of its main features is that **it separates the writing and the formatting stages**. The writer can focus solely on writing the content in plain text, and then use some markup tags and commands to stylize the text and define the document structure.  This is mainly what sets $\\LaTeX$ apart from WYSIWYG (an acronym for \"What You See Is What You Get\") editors, like Microsoft Word. Experienced users can **produce professional-looking documents very quickly**, but **at the expense of losing a Graphical User Interface**.   **[LyX](https://www.lyx.org)** is a document preparation software that bridges the two worlds of \"What You See Is What You Mean\" and \"What You See Is What You Get\". It combines the power and flexibility of $\\LaTeX$ with a Graphical User Interface and the gentle learning curve of editors like Microsoft Word.  LyX is cross-platform and free of charge. You can download it from the **[LyX Download page](https://www.lyx.org/Download)**.  {{% tip %}}  [Read more](/get/latex) about all the installing options for $\\LaTeX$ and LyX. In case you need further guidance, you can access [here](https://wiki.lyx.org/LyX/Tutorials) the full list of official LyX tutorials.  {{% /tip %}}   We provide a LyX template to get you started writing your own research paper.  {{% cta-primary \"Download the LyX Template\" \"../lyx-research-paper-template.zip\" %}}  ![Academic paper in LaTeX](../img/research-paper-lyx-template.png) "}]